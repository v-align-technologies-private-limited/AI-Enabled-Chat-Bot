{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4855bb8c-ee04-46f9-906d-a61e6ba37cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\556443880.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\556443880.py:73: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\556443880.py:73: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\556443880.py:73: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema with string and date/time data types fetched successfully.\n",
      "Processing table: tasks\n",
      "Unique values for column task_id in table tasks between 2024-10-07 and 2024-10-08: []\n",
      "Processing table: milestones\n",
      "Unique values for column project_name in table milestones between 2024-10-07 and 2024-10-08: []\n",
      "Processing table: projects\n",
      "Unique values for column project_id in table projects between 2024-10-07 and 2024-10-08: []\n",
      "Processing table: contacts\n",
      "No date or timestamp column found for table contacts.\n",
      "No 'created date' or timestamp column found for table contacts, skipping.\n",
      "Processing table: users\n",
      "No date or timestamp column found for table users.\n",
      "No 'created date' or timestamp column found for table users, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model loaded successfully.\n",
      "No unique values found for task_id in tasks. Skipping embeddings.\n",
      "No unique values found for project_name in milestones. Skipping embeddings.\n",
      "No unique values found for project_id in projects. Skipping embeddings.\n",
      "Embeddings for string columns generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc\"\n",
    "PORT = 5432\n",
    "\n",
    "# Pinecone details\n",
    "pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "index_name = \"smart-desk\"\n",
    "BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# Function to connect to PostgreSQL database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch schema with column names and data types, dynamically including string and date/timestamp types\n",
    "def fetch_schema_with_data_types(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        AND data_type IN ('character varying', 'text', 'varchar', 'date', 'timestamp without time zone', 'timestamp with time zone')\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema with data types: {e}\")\n",
    "        raise\n",
    "\n",
    "# Find the 'created date' column dynamically based on its data type\n",
    "def find_created_date_column(schema_df, table_name):\n",
    "    try:\n",
    "        date_columns = schema_df[\n",
    "            (schema_df['table_name'] == table_name) & \n",
    "            (schema_df['data_type'].isin(['date', 'timestamp without time zone', 'timestamp with time zone']))\n",
    "        ]\n",
    "        if not date_columns.empty:\n",
    "            return date_columns.iloc[0]['column_name']\n",
    "        else:\n",
    "            print(f\"No date or timestamp column found for table {table_name}.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding created date column for {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch unique values based on created date or time column dynamically\n",
    "def fetch_unique_values(conn, table_name, column_name, created_date_column, from_date, to_date):\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT {column_name}\n",
    "        FROM {table_name}\n",
    "        WHERE {created_date_column} BETWEEN '{from_date}' AND '{to_date}'\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df[column_name].dropna().astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching unique values for {column_name} in {table_name} based on {created_date_column}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "def load_huggingface_model():\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    return embedding_model\n",
    "\n",
    "# Initialize Pinecone\n",
    "def initialize_pinecone():\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "# Check if the unique value already exists in Pinecone\n",
    "def check_existing_in_pinecone(index, table_name, column_name, unique_value):\n",
    "    try:\n",
    "        vector_id = f\"{table_name}_{column_name}_{unique_value}\"\n",
    "        result = index.fetch(ids=[vector_id], namespace=table_name)\n",
    "        if result and 'vectors' in result and result['vectors']:\n",
    "            print(f\"Record {vector_id} already exists in Pinecone.\")\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existence in Pinecone for {vector_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate embeddings for each unique value and store them\n",
    "def generate_and_store_embeddings(embedding_model, unique_values_dict, index):\n",
    "    embeddings_dict = {}\n",
    "    for table_name, columns in unique_values_dict.items():\n",
    "        embeddings_dict[table_name] = {}\n",
    "        for column_name, unique_values in columns.items():\n",
    "            if unique_values:  # Check if there are any unique values\n",
    "                embeddings_list = []\n",
    "                unique_values_filtered = []\n",
    "                for value in unique_values:\n",
    "                    if not check_existing_in_pinecone(index, table_name, column_name, value):\n",
    "                        try:\n",
    "                            embedding = embedding_model.encode(value)\n",
    "                            embeddings_list.append(embedding)\n",
    "                            unique_values_filtered.append(value)  # Only store values that don't exist\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error generating embeddings for {column_name} in {table_name}: {e}\")\n",
    "                            continue\n",
    "                embeddings_dict[table_name][column_name] = {\n",
    "                    \"unique_values\": unique_values_filtered,\n",
    "                    \"embeddings\": embeddings_list\n",
    "                }\n",
    "            else:\n",
    "                print(f\"No unique values found for {column_name} in {table_name}. Skipping embeddings.\")\n",
    "                embeddings_dict[table_name][column_name] = {\n",
    "                    \"unique_values\": [],\n",
    "                    \"embeddings\": []\n",
    "                }\n",
    "    return embeddings_dict\n",
    "\n",
    "# Batch the embeddings for upserts\n",
    "def batch_embeddings(upsert_data, batch_size):\n",
    "    for i in range(0, len(upsert_data), batch_size):\n",
    "        yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "    for table_name, columns in embeddings_dict.items():\n",
    "        for column_name, data in columns.items():\n",
    "            upsert_data = []\n",
    "            for i, embedding in enumerate(data['embeddings']):\n",
    "                unique_value = data['unique_values'][i]\n",
    "                vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "                metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "                upsert_data.append({\n",
    "                    \"id\": vector_id,\n",
    "                    \"values\": embedding.tolist(),\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "\n",
    "            # Batch the upsert to avoid exceeding size limits\n",
    "            for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "                index.upsert(vectors=batch, namespace=table_name)\n",
    "                print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# Main function to execute the process\n",
    "def main():\n",
    "    # Step 1: Connect to the database\n",
    "    conn = connect_to_db()\n",
    "\n",
    "    # Step 2: Fetch the schema with metadata and data types, including string and date/time columns\n",
    "    schema_df = fetch_schema_with_data_types(conn)\n",
    "    print(\"Schema with string and date/time data types fetched successfully.\")\n",
    "\n",
    "    # Step 3: Define the date range for today (for daily scheduler)\n",
    "    today = datetime.now()\n",
    "    from_date = today.strftime('%Y-%m-%d')  # Start of the day\n",
    "    to_date = (today + timedelta(days=1)).strftime('%Y-%m-%d')  # End of the day\n",
    "\n",
    "    # Example: Iterate through tables dynamically, finding the created date column and fetching unique values\n",
    "    unique_values_dict = {}\n",
    "    for table_name in schema_df['table_name'].unique():\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "\n",
    "        # Step 4: Find the created date column dynamically for each table\n",
    "        created_date_column = find_created_date_column(schema_df, table_name)\n",
    "\n",
    "        # If a created date column is found, proceed to fetch unique values\n",
    "        if created_date_column:\n",
    "            # For simplicity, using the first string-based column for fetching unique values\n",
    "            string_columns = schema_df[\n",
    "                (schema_df['table_name'] == table_name) & \n",
    "                (schema_df['data_type'].isin(['character varying', 'text', 'varchar']))\n",
    "            ]\n",
    "\n",
    "            # Fetch unique values only if there are string columns\n",
    "            if not string_columns.empty:\n",
    "                column_name = string_columns.iloc[0]['column_name']  # Use the first string-based column\n",
    "                unique_values = fetch_unique_values(conn, table_name, column_name, created_date_column, from_date, to_date)\n",
    "                unique_values_dict.setdefault(table_name, {})[column_name] = unique_values\n",
    "                print(f\"Unique values for column {column_name} in table {table_name} between {from_date} and {to_date}: {unique_values}\")\n",
    "            else:\n",
    "                print(f\"No string columns found in table {table_name} to fetch unique values.\")\n",
    "        else:\n",
    "            print(f\"No 'created date' or timestamp column found for table {table_name}, skipping.\")\n",
    "\n",
    "    # Step 5: Load the Hugging Face model for embeddings\n",
    "    embedding_model = load_huggingface_model()\n",
    "    print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "    # Step 6: Initialize Pinecone and check for existing records before generating embeddings\n",
    "    pinecone_index = initialize_pinecone()\n",
    "\n",
    "    # Step 7: Generate embeddings for all unique values, avoiding duplicates\n",
    "    embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict, pinecone_index)\n",
    "    print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "    # Step 8: Upsert embeddings under each table's namespace, skipping existing records\n",
    "    upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c627541-aac7-4f06-9763-f554afd7683e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a189dc2-63cc-43df-a678-65641d586642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"python_test_poc\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Function to connect to PostgreSQL database\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch schema with column names and data types, dynamically including 'date' or 'timestamp' types for created dates\n",
    "# def fetch_schema_with_data_types(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         AND data_type IN ('character varying', 'text', 'varchar', 'date', 'timestamp without time zone', 'timestamp with time zone')\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema with data types: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch unique values based on created date or time column dynamically\n",
    "# def fetch_unique_values(conn, table_name, column_name, created_date_column, from_date, to_date):\n",
    "#     try:\n",
    "#         query = f\"\"\"\n",
    "#         SELECT DISTINCT {column_name}\n",
    "#         FROM {table_name}\n",
    "#         WHERE {created_date_column} BETWEEN '{from_date}' AND '{to_date}'\n",
    "#         \"\"\"\n",
    "#         df = pd.read_sql(query, conn)\n",
    "#         return df[column_name].dropna().astype(str).tolist()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching unique values for {column_name} in {table_name} based on {created_date_column}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Find the 'created date' column dynamically based on its data type\n",
    "# def find_created_date_column(schema_df, table_name):\n",
    "#     try:\n",
    "#         # Filter the schema for date or timestamp columns in the specified table\n",
    "#         date_columns = schema_df[\n",
    "#             (schema_df['table_name'] == table_name) & \n",
    "#             (schema_df['data_type'].isin(['date', 'timestamp without time zone', 'timestamp with time zone']))\n",
    "#         ]\n",
    "        \n",
    "#         # Return the first 'date' or 'timestamp' column found (assuming it's the created date)\n",
    "#         if not date_columns.empty:\n",
    "#             return date_columns.iloc[0]['column_name']\n",
    "#         else:\n",
    "#             print(f\"No date or timestamp column found for table {table_name}.\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error finding created date column for {table_name}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Main function to execute the process\n",
    "# def main():\n",
    "#     # Step 1: Connect to the database\n",
    "#     conn = connect_to_db()\n",
    "\n",
    "#     # Step 2: Fetch the schema with metadata and data types, including string and date/time columns\n",
    "#     schema_df = fetch_schema_with_data_types(conn)\n",
    "#     print(\"Schema with string and date/time data types fetched successfully.\")\n",
    "\n",
    "#     # Step 3: Define the date range for today (for daily scheduler)\n",
    "#     today = datetime.now()\n",
    "#     from_date = today.strftime('%Y-%m-%d')  # Start of the day\n",
    "#     to_date = (today + timedelta(days=1)).strftime('%Y-%m-%d')  # End of the day\n",
    "\n",
    "#     # Example: Iterate through tables dynamically, finding the created date column and fetching unique values\n",
    "#     for table_name in schema_df['table_name'].unique():\n",
    "#         print(f\"Processing table: {table_name}\")\n",
    "\n",
    "#         # Step 4: Find the created date column dynamically for each table\n",
    "#         created_date_column = find_created_date_column(schema_df, table_name)\n",
    "\n",
    "#         # If a created date column is found, proceed to fetch unique values\n",
    "#         if created_date_column:\n",
    "#             # For simplicity, using the first string-based column for fetching unique values\n",
    "#             string_columns = schema_df[\n",
    "#                 (schema_df['table_name'] == table_name) & \n",
    "#                 (schema_df['data_type'].isin(['character varying', 'text', 'varchar']))\n",
    "#             ]\n",
    "\n",
    "#             # Fetch unique values only if there are string columns\n",
    "#             if not string_columns.empty:\n",
    "#                 column_name = string_columns.iloc[0]['column_name']  # Use the first string-based column\n",
    "#                 unique_values = fetch_unique_values(conn, table_name, column_name, created_date_column, from_date, to_date)\n",
    "#                 print(f\"Unique values for column {column_name} in table {table_name} between {from_date} and {to_date}: {unique_values}\")\n",
    "#             else:\n",
    "#                 print(f\"No string columns found in table {table_name} to fetch unique values.\")\n",
    "#         else:\n",
    "#             print(f\"No 'created date' or timestamp column found for table {table_name}, skipping.\")\n",
    "\n",
    "#     # Close the connection after all operations\n",
    "#     conn.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb7edbc-8e0c-4d39-a906-6b8bfd233f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\3083385900.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\3083385900.py:73: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\3083385900.py:73: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6408\\3083385900.py:73: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema with string and date/time data types fetched successfully.\n",
      "Processing table: tasks\n",
      "Unique values for column task_id in table tasks between 2024-10-07 and 2024-10-08: []\n",
      "Processing table: milestones\n",
      "Unique values for column project_name in table milestones between 2024-10-07 and 2024-10-08: []\n",
      "Processing table: projects\n",
      "Unique values for column project_id in table projects between 2024-10-07 and 2024-10-08: []\n",
      "Processing table: contacts\n",
      "No date or timestamp column found for table contacts.\n",
      "No 'created date' or timestamp column found for table contacts, skipping.\n",
      "Processing table: users\n",
      "No date or timestamp column found for table users.\n",
      "No 'created date' or timestamp column found for table users, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model loaded successfully.\n",
      "No unique values found for task_id in tasks. Skipping embeddings.\n",
      "No unique values found for project_name in milestones. Skipping embeddings.\n",
      "No unique values found for project_id in projects. Skipping embeddings.\n",
      "Embeddings for string columns generated successfully.\n",
      "Embeddings upserted into Pinecone successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc\"\n",
    "PORT = 5432\n",
    "\n",
    "# Pinecone details\n",
    "pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "index_name = \"smart-desk\"\n",
    "BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# Function to connect to PostgreSQL database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch schema with column names and data types, dynamically including string and date/timestamp types\n",
    "def fetch_schema_with_data_types(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        AND data_type IN ('character varying', 'text', 'varchar', 'date', 'timestamp without time zone', 'timestamp with time zone')\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema with data types: {e}\")\n",
    "        raise\n",
    "\n",
    "# Find the 'created date' column dynamically based on its data type\n",
    "def find_created_date_column(schema_df, table_name):\n",
    "    try:\n",
    "        date_columns = schema_df[\n",
    "            (schema_df['table_name'] == table_name) & \n",
    "            (schema_df['data_type'].isin(['date', 'timestamp without time zone', 'timestamp with time zone']))\n",
    "        ]\n",
    "        if not date_columns.empty:\n",
    "            return date_columns.iloc[0]['column_name']\n",
    "        else:\n",
    "            print(f\"No date or timestamp column found for table {table_name}.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding created date column for {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch unique values based on created date or time column dynamically\n",
    "def fetch_unique_values(conn, table_name, column_name, created_date_column, from_date, to_date):\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT {column_name}\n",
    "        FROM {table_name}\n",
    "        WHERE {created_date_column} BETWEEN '{from_date}' AND '{to_date}'\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df[column_name].dropna().astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching unique values for {column_name} in {table_name} based on {created_date_column}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "def load_huggingface_model():\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    return embedding_model\n",
    "\n",
    "# Generate embeddings for each unique value and store them\n",
    "def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "    embeddings_dict = {}\n",
    "    for table_name, columns in unique_values_dict.items():\n",
    "        embeddings_dict[table_name] = {}\n",
    "        for column_name, unique_values in columns.items():\n",
    "            if unique_values:  # Check if there are any unique values\n",
    "                try:\n",
    "                    embeddings = embedding_model.encode(unique_values)\n",
    "                    embeddings_dict[table_name][column_name] = {\n",
    "                        \"unique_values\": unique_values,\n",
    "                        \"embeddings\": embeddings\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating embeddings for {column_name} in {table_name}: {e}\")\n",
    "                    embeddings_dict[table_name][column_name] = {\n",
    "                        \"unique_values\": unique_values,\n",
    "                        \"embeddings\": []  # Store an empty list if encoding fails\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"No unique values found for {column_name} in {table_name}. Skipping embeddings.\")\n",
    "                embeddings_dict[table_name][column_name] = {\n",
    "                    \"unique_values\": [],\n",
    "                    \"embeddings\": []\n",
    "                }\n",
    "    return embeddings_dict\n",
    "\n",
    "# Initialize Pinecone\n",
    "def initialize_pinecone():\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "# Batch the embeddings for upserts\n",
    "def batch_embeddings(upsert_data, batch_size):\n",
    "    for i in range(0, len(upsert_data), batch_size):\n",
    "        yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "    for table_name, columns in embeddings_dict.items():\n",
    "        for column_name, data in columns.items():\n",
    "            upsert_data = []\n",
    "            for i, embedding in enumerate(data['embeddings']):\n",
    "                unique_value = data['unique_values'][i]\n",
    "                vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "                metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "                upsert_data.append({\n",
    "                    \"id\": vector_id,\n",
    "                    \"values\": embedding.tolist(),\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "\n",
    "            # Batch the upsert to avoid exceeding size limits\n",
    "            for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "                index.upsert(vectors=batch, namespace=table_name)\n",
    "                print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# Main function to execute the process\n",
    "def main():\n",
    "    # Step 1: Connect to the database\n",
    "    conn = connect_to_db()\n",
    "\n",
    "    # Step 2: Fetch the schema with metadata and data types, including string and date/time columns\n",
    "    schema_df = fetch_schema_with_data_types(conn)\n",
    "    print(\"Schema with string and date/time data types fetched successfully.\")\n",
    "\n",
    "    # Step 3: Define the date range for today (for daily scheduler)\n",
    "    today = datetime.now()\n",
    "    from_date = today.strftime('%Y-%m-%d')  # Start of the day\n",
    "    to_date = (today + timedelta(days=1)).strftime('%Y-%m-%d')  # End of the day\n",
    "\n",
    "    # Example: Iterate through tables dynamically, finding the created date column and fetching unique values\n",
    "    unique_values_dict = {}\n",
    "    for table_name in schema_df['table_name'].unique():\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "\n",
    "        # Step 4: Find the created date column dynamically for each table\n",
    "        created_date_column = find_created_date_column(schema_df, table_name)\n",
    "\n",
    "        # If a created date column is found, proceed to fetch unique values\n",
    "        if created_date_column:\n",
    "            # For simplicity, using the first string-based column for fetching unique values\n",
    "            string_columns = schema_df[\n",
    "                (schema_df['table_name'] == table_name) & \n",
    "                (schema_df['data_type'].isin(['character varying', 'text', 'varchar']))\n",
    "            ]\n",
    "\n",
    "            # Fetch unique values only if there are string columns\n",
    "            if not string_columns.empty:\n",
    "                column_name = string_columns.iloc[0]['column_name']  # Use the first string-based column\n",
    "                unique_values = fetch_unique_values(conn, table_name, column_name, created_date_column, from_date, to_date)\n",
    "                unique_values_dict.setdefault(table_name, {})[column_name] = unique_values\n",
    "                print(f\"Unique values for column {column_name} in table {table_name} between {from_date} and {to_date}: {unique_values}\")\n",
    "            else:\n",
    "                print(f\"No string columns found in table {table_name} to fetch unique values.\")\n",
    "        else:\n",
    "            print(f\"No 'created date' or timestamp column found for table {table_name}, skipping.\")\n",
    "\n",
    "    # Step 5: Load the Hugging Face model for embeddings\n",
    "    embedding_model = load_huggingface_model()\n",
    "    print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "    # Step 6: Generate embeddings for all unique values\n",
    "    embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "    print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "    # Step 7: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "    print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "    # Close the connection after all operations\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e96895-eb9b-483a-a532-b87f933feba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vrsion One Vector DB Set Up\n",
    "\n",
    "# import psycopg2\n",
    "# import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"postgres\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Pinecone details\n",
    "# pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "# index_name = \"smart-desk\"\n",
    "# BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# # Function to connect to PostgreSQL database\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch schema with column names and data types, only including string types\n",
    "# def fetch_schema_with_data_types(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         AND data_type IN ('character varying', 'text', 'varchar')\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         print(schema_df)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema with data types: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch unique values from each column along with table details\n",
    "# def fetch_unique_values(conn, table_name, column_name):\n",
    "#     try:\n",
    "#         query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "#         df = pd.read_sql(query, conn)\n",
    "#         return df[column_name].dropna().astype(str).tolist()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching unique values for {column_name} in {table_name}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Fetch all unique values for each column and map them to table details\n",
    "# def fetch_all_unique_values_with_table(conn, schema_df):\n",
    "#     unique_values_dict = {}\n",
    "#     for table_name in schema_df['table_name'].unique():\n",
    "#         unique_values_dict[table_name] = {}\n",
    "#         table_columns = schema_df[schema_df['table_name'] == table_name]\n",
    "#         for column_name in table_columns['column_name']:\n",
    "#             unique_values = fetch_unique_values(conn, table_name, column_name)\n",
    "#             unique_values_dict[table_name][column_name] = unique_values\n",
    "#     return unique_values_dict\n",
    "\n",
    "# # Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "# def load_huggingface_model():\n",
    "#     model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "#     embedding_model = SentenceTransformer(model_name)\n",
    "#     return embedding_model\n",
    "\n",
    "# # Generate embeddings for each unique value and store them\n",
    "# def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "#     embeddings_dict = {}\n",
    "#     for table_name, columns in unique_values_dict.items():\n",
    "#         embeddings_dict[table_name] = {}\n",
    "#         for column_name, unique_values in columns.items():\n",
    "#             embeddings = embedding_model.encode(unique_values)\n",
    "#             embeddings_dict[table_name][column_name] = {\n",
    "#                 \"unique_values\": unique_values,\n",
    "#                 \"embeddings\": embeddings\n",
    "#             }\n",
    "#     return embeddings_dict\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# def initialize_pinecone():\n",
    "#     pc = Pinecone(api_key=pinecone_api_key)\n",
    "#     index = pc.Index(index_name)\n",
    "#     return index\n",
    "\n",
    "# # Batch the embeddings for upserts\n",
    "# def batch_embeddings(upsert_data, batch_size):\n",
    "#     for i in range(0, len(upsert_data), batch_size):\n",
    "#         yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# # Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "# def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "#     for table_name, columns in embeddings_dict.items():\n",
    "#         for column_name, data in columns.items():\n",
    "#             upsert_data = []\n",
    "#             for i, embedding in enumerate(data['embeddings']):\n",
    "#                 unique_value = data['unique_values'][i]\n",
    "#                 vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "#                 metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "#                 upsert_data.append({\n",
    "#                     \"id\": vector_id,\n",
    "#                     \"values\": embedding.tolist(),\n",
    "#                     \"metadata\": metadata\n",
    "#                 })\n",
    "\n",
    "#             # Batch the upsert to avoid exceeding size limits\n",
    "#             for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "#                 index.upsert(vectors=batch, namespace=table_name)\n",
    "#                 print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# # Main function to execute the process\n",
    "# def main():\n",
    "#     # Step 1: Connect to the database\n",
    "#     conn = connect_to_db()\n",
    "\n",
    "#     # Step 2: Fetch the schema with metadata and data types, only for string columns\n",
    "#     schema_df = fetch_schema_with_data_types(conn)\n",
    "#     print(\"Schema with string data types fetched successfully.\")\n",
    "\n",
    "#     # Step 3: Fetch all unique values along with table details\n",
    "#     unique_values_dict = fetch_all_unique_values_with_table(conn, schema_df)\n",
    "#     print(\"Unique values for string columns fetched successfully.\")\n",
    "\n",
    "#     # Step 4: Load the Hugging Face model for embeddings\n",
    "#     embedding_model = load_huggingface_model()\n",
    "#     print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "#     # Step 5: Generate embeddings for all unique values\n",
    "#     embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "#     print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "#     # Step 6: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "#     pinecone_index = initialize_pinecone()\n",
    "#     upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "#     print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006074a-79a7-4544-a0e6-944e709946b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Updated Pinecone Code to Create and Upsert Embeddings\n",
    "\n",
    "# import psycopg2\n",
    "# import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"postgres\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Pinecone details\n",
    "# pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "# index_name = \"smart-desk\"\n",
    "# BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# # Function to connect to PostgreSQL database\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch schema with column names and data types, only including string types\n",
    "# def fetch_schema_with_data_types(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         AND data_type IN ('character varying', 'text', 'varchar') AND table_name = 'contacts'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         print(schema_df)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema with data types: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch unique values from each column along with table details\n",
    "# def fetch_unique_values(conn, table_name, column_name):\n",
    "#     try:\n",
    "#         query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "#         df = pd.read_sql(query, conn)\n",
    "#         return df[column_name].dropna().astype(str).tolist()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching unique values for {column_name} in {table_name}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Fetch all unique values for each column and map them to table details\n",
    "# def fetch_all_unique_values_with_table(conn, schema_df):\n",
    "#     unique_values_dict = {}\n",
    "#     for table_name in schema_df['table_name'].unique():\n",
    "#         unique_values_dict[table_name] = {}\n",
    "#         table_columns = schema_df[schema_df['table_name'] == table_name]\n",
    "#         for column_name in table_columns['column_name']:\n",
    "#             unique_values = fetch_unique_values(conn, table_name, column_name)\n",
    "#             unique_values_dict[table_name][column_name] = unique_values\n",
    "#     return unique_values_dict\n",
    "\n",
    "# # Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "# def load_huggingface_model():\n",
    "#     model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "#     embedding_model = SentenceTransformer(model_name)\n",
    "#     return embedding_model\n",
    "\n",
    "# # Generate embeddings for each unique value and store them\n",
    "# # Generate embeddings for each unique value and store them\n",
    "# def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "#     embeddings_dict = {}\n",
    "#     for table_name, columns in unique_values_dict.items():\n",
    "#         embeddings_dict[table_name] = {}\n",
    "#         for column_name, unique_values in columns.items():\n",
    "#             if unique_values:  # Check if there are any unique values\n",
    "#                 try:\n",
    "#                     embeddings = embedding_model.encode(unique_values)\n",
    "#                     embeddings_dict[table_name][column_name] = {\n",
    "#                         \"unique_values\": unique_values,\n",
    "#                         \"embeddings\": embeddings\n",
    "#                     }\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error generating embeddings for {column_name} in {table_name}: {e}\")\n",
    "#                     embeddings_dict[table_name][column_name] = {\n",
    "#                         \"unique_values\": unique_values,\n",
    "#                         \"embeddings\": []  # Store an empty list if encoding fails\n",
    "#                     }\n",
    "#             else:\n",
    "#                 print(f\"No unique values found for {column_name} in {table_name}. Skipping embeddings.\")\n",
    "#                 embeddings_dict[table_name][column_name] = {\n",
    "#                     \"unique_values\": [],\n",
    "#                     \"embeddings\": []\n",
    "#                 }\n",
    "#     return embeddings_dict\n",
    "\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# def initialize_pinecone():\n",
    "#     pc = Pinecone(api_key=pinecone_api_key)\n",
    "#     index = pc.Index(index_name)\n",
    "#     return index\n",
    "\n",
    "# # Batch the embeddings for upserts\n",
    "# def batch_embeddings(upsert_data, batch_size):\n",
    "#     for i in range(0, len(upsert_data), batch_size):\n",
    "#         yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# # Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "# def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "#     for table_name, columns in embeddings_dict.items():\n",
    "#         for column_name, data in columns.items():\n",
    "#             upsert_data = []\n",
    "#             for i, embedding in enumerate(data['embeddings']):\n",
    "#                 unique_value = data['unique_values'][i]\n",
    "#                 vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "#                 metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "#                 upsert_data.append({\n",
    "#                     \"id\": vector_id,\n",
    "#                     \"values\": embedding.tolist(),\n",
    "#                     \"metadata\": metadata\n",
    "#                 })\n",
    "\n",
    "#             # Batch the upsert to avoid exceeding size limits\n",
    "#             for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "#                 index.upsert(vectors=batch, namespace=table_name)\n",
    "#                 print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# # Main function to execute the process\n",
    "# def main():\n",
    "#     # Step 1: Connect to the database\n",
    "#     conn = connect_to_db()\n",
    "\n",
    "#     # Step 2: Fetch the schema with metadata and data types, only for string columns\n",
    "#     schema_df = fetch_schema_with_data_types(conn)\n",
    "#     print(\"Schema with string data types fetched successfully.\")\n",
    "\n",
    "#     # Step 3: Fetch all unique values along with table details\n",
    "#     unique_values_dict = fetch_all_unique_values_with_table(conn, schema_df)\n",
    "#     print(\"Unique values for string columns fetched successfully.\")\n",
    "\n",
    "#     # Step 4: Load the Hugging Face model for embeddings\n",
    "#     embedding_model = load_huggingface_model()\n",
    "#     print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "#     # Step 5: Generate embeddings for all unique values\n",
    "#     embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "#     print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "#     # Step 6: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "#     pinecone_index = initialize_pinecone()\n",
    "#     upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "#     print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ade82-491d-4b2c-a025-b00129224961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "# import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"python_test_poc\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Pinecone details\n",
    "# pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "# index_name = \"smart-desk\"\n",
    "# BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# # Function to connect to PostgreSQL database\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch schema with column names and data types\n",
    "# def fetch_schema_with_data_types(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema with data types: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch unique values from each column along with table details\n",
    "# def fetch_unique_values(conn, table_name, column_name):\n",
    "#     try:\n",
    "#         query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "#         df = pd.read_sql(query, conn)\n",
    "#         return df[column_name].dropna().astype(str).tolist()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching unique values for {column_name} in {table_name}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Fetch all unique values for each column and map them to table details\n",
    "# def fetch_all_unique_values_with_table(conn, schema_df):\n",
    "#     unique_values_dict = {}\n",
    "#     for table_name in schema_df['table_name'].unique():\n",
    "#         unique_values_dict[table_name] = {}\n",
    "#         table_columns = schema_df[schema_df['table_name'] == table_name]\n",
    "#         for column_name in table_columns['column_name']:\n",
    "#             unique_values = fetch_unique_values(conn, table_name, column_name)\n",
    "#             unique_values_dict[table_name][column_name] = unique_values\n",
    "#     return unique_values_dict\n",
    "\n",
    "# # Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "# def load_huggingface_model():\n",
    "#     model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "#     embedding_model = SentenceTransformer(model_name)\n",
    "#     return embedding_model\n",
    "\n",
    "# # Generate embeddings for each unique value and store them\n",
    "# def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "#     embeddings_dict = {}\n",
    "#     for table_name, columns in unique_values_dict.items():\n",
    "#         embeddings_dict[table_name] = {}\n",
    "#         for column_name, unique_values in columns.items():\n",
    "#             embeddings = embedding_model.encode(unique_values)\n",
    "#             embeddings_dict[table_name][column_name] = {\n",
    "#                 \"unique_values\": unique_values,\n",
    "#                 \"embeddings\": embeddings\n",
    "#             }\n",
    "#     return embeddings_dict\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# def initialize_pinecone():\n",
    "#     pc = Pinecone(api_key=pinecone_api_key)\n",
    "#     index = pc.Index(index_name)\n",
    "#     return index\n",
    "\n",
    "# # Batch the embeddings for upserts\n",
    "# def batch_embeddings(upsert_data, batch_size):\n",
    "#     for i in range(0, len(upsert_data), batch_size):\n",
    "#         yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# # Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "# def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "#     for table_name, columns in embeddings_dict.items():\n",
    "#         for column_name, data in columns.items():\n",
    "#             upsert_data = []\n",
    "#             for i, embedding in enumerate(data['embeddings']):\n",
    "#                 unique_value = data['unique_values'][i]\n",
    "#                 vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "#                 metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "#                 upsert_data.append({\n",
    "#                     \"id\": vector_id,\n",
    "#                     \"values\": embedding.tolist(),\n",
    "#                     \"metadata\": metadata\n",
    "#                 })\n",
    "\n",
    "#             # Batch the upsert to avoid exceeding size limits\n",
    "#             for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "#                 index.upsert(vectors=batch, namespace=table_name)\n",
    "#                 print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# # Main function to execute the process\n",
    "# def main():\n",
    "#     # Step 1: Connect to the database\n",
    "#     conn = connect_to_db()\n",
    "\n",
    "#     # Step 2: Fetch the schema with metadata and data types\n",
    "#     schema_df = fetch_schema_with_data_types(conn)\n",
    "#     print(\"Schema with data types fetched successfully.\")\n",
    "\n",
    "#     # Step 3: Fetch all unique values along with table details\n",
    "#     unique_values_dict = fetch_all_unique_values_with_table(conn, schema_df)\n",
    "#     print(\"Unique values fetched successfully.\")\n",
    "\n",
    "#     # Step 4: Load the Hugging Face model for embeddings\n",
    "#     embedding_model = load_huggingface_model()\n",
    "#     print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "#     # Step 5: Generate embeddings for all unique values\n",
    "#     embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "#     print(\"Embeddings generated successfully.\")\n",
    "\n",
    "#     # Step 6: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "#     pinecone_index = initialize_pinecone()\n",
    "#     upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "#     print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8e0b8f-31cc-4415-89a3-4fcc4b650cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:42: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table_name                 column_name          data_type\n",
      "0    contacts                contact_name  character varying\n",
      "1    contacts               customer_name  character varying\n",
      "2    contacts                 vendor_name  character varying\n",
      "3    contacts                company_name  character varying\n",
      "4    contacts                contact_type  character varying\n",
      "5    contacts               currency_code  character varying\n",
      "6    contacts                  first_name  character varying\n",
      "7    contacts                   last_name  character varying\n",
      "8    contacts                       email  character varying\n",
      "9    contacts                       phone  character varying\n",
      "10   contacts                      mobile  character varying\n",
      "11   contacts                      gst_no  character varying\n",
      "12   contacts               gst_treatment  character varying\n",
      "13   contacts  place_of_contact_formatted  character varying\n",
      "14   contacts            place_of_contact  character varying\n",
      "15   contacts                      pan_no  character varying\n",
      "Schema with string data types fetched successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12480\\291255020.py:53: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for string columns fetched successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model loaded successfully.\n",
      "No unique values found for vendor_name in contacts. Skipping embeddings.\n",
      "No unique values found for currency_code in contacts. Skipping embeddings.\n",
      "No unique values found for first_name in contacts. Skipping embeddings.\n",
      "No unique values found for last_name in contacts. Skipping embeddings.\n",
      "No unique values found for email in contacts. Skipping embeddings.\n",
      "No unique values found for phone in contacts. Skipping embeddings.\n",
      "No unique values found for mobile in contacts. Skipping embeddings.\n",
      "No unique values found for gst_no in contacts. Skipping embeddings.\n",
      "No unique values found for gst_treatment in contacts. Skipping embeddings.\n",
      "No unique values found for place_of_contact_formatted in contacts. Skipping embeddings.\n",
      "No unique values found for place_of_contact in contacts. Skipping embeddings.\n",
      "No unique values found for pan_no in contacts. Skipping embeddings.\n",
      "Embeddings for string columns generated successfully.\n",
      "Upserted batch for contact_name in contacts\n",
      "Upserted batch for customer_name in contacts\n",
      "Upserted batch for company_name in contacts\n",
      "Upserted batch for contact_type in contacts\n",
      "Embeddings upserted into Pinecone successfully.\n"
     ]
    }
   ],
   "source": [
    "# #Updated Pinecone Code to Create and Upsert Embeddings\n",
    "\n",
    "# import psycopg2\n",
    "# import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"postgres\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Pinecone details\n",
    "# pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "# index_name = \"smart-desk\"\n",
    "# BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# # Function to connect to PostgreSQL database\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch schema with column names and data types, only including string types\n",
    "# def fetch_schema_with_data_types(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         AND data_type IN ('character varying', 'text', 'varchar') AND table_name = 'contacts'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         print(schema_df)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema with data types: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Fetch unique values from each column along with table details\n",
    "# def fetch_unique_values(conn, table_name, column_name):\n",
    "#     try:\n",
    "#         query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "#         df = pd.read_sql(query, conn)\n",
    "#         return df[column_name].dropna().astype(str).tolist()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching unique values for {column_name} in {table_name}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Fetch all unique values for each column and map them to table details\n",
    "# def fetch_all_unique_values_with_table(conn, schema_df):\n",
    "#     unique_values_dict = {}\n",
    "#     for table_name in schema_df['table_name'].unique():\n",
    "#         unique_values_dict[table_name] = {}\n",
    "#         table_columns = schema_df[schema_df['table_name'] == table_name]\n",
    "#         for column_name in table_columns['column_name']:\n",
    "#             unique_values = fetch_unique_values(conn, table_name, column_name)\n",
    "#             unique_values_dict[table_name][column_name] = unique_values\n",
    "#     return unique_values_dict\n",
    "\n",
    "# # Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "# def load_huggingface_model():\n",
    "#     model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "#     embedding_model = SentenceTransformer(model_name)\n",
    "#     return embedding_model\n",
    "\n",
    "# # Generate embeddings for each unique value and store them\n",
    "# # Generate embeddings for each unique value and store them\n",
    "# def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "#     embeddings_dict = {}\n",
    "#     for table_name, columns in unique_values_dict.items():\n",
    "#         embeddings_dict[table_name] = {}\n",
    "#         for column_name, unique_values in columns.items():\n",
    "#             if unique_values:  # Check if there are any unique values\n",
    "#                 try:\n",
    "#                     embeddings = embedding_model.encode(unique_values)\n",
    "#                     embeddings_dict[table_name][column_name] = {\n",
    "#                         \"unique_values\": unique_values,\n",
    "#                         \"embeddings\": embeddings\n",
    "#                     }\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error generating embeddings for {column_name} in {table_name}: {e}\")\n",
    "#                     embeddings_dict[table_name][column_name] = {\n",
    "#                         \"unique_values\": unique_values,\n",
    "#                         \"embeddings\": []  # Store an empty list if encoding fails\n",
    "#                     }\n",
    "#             else:\n",
    "#                 print(f\"No unique values found for {column_name} in {table_name}. Skipping embeddings.\")\n",
    "#                 embeddings_dict[table_name][column_name] = {\n",
    "#                     \"unique_values\": [],\n",
    "#                     \"embeddings\": []\n",
    "#                 }\n",
    "#     return embeddings_dict\n",
    "\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# def initialize_pinecone():\n",
    "#     pc = Pinecone(api_key=pinecone_api_key)\n",
    "#     index = pc.Index(index_name)\n",
    "#     return index\n",
    "\n",
    "# # Batch the embeddings for upserts\n",
    "# def batch_embeddings(upsert_data, batch_size):\n",
    "#     for i in range(0, len(upsert_data), batch_size):\n",
    "#         yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# # Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "# def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "#     for table_name, columns in embeddings_dict.items():\n",
    "#         for column_name, data in columns.items():\n",
    "#             upsert_data = []\n",
    "#             for i, embedding in enumerate(data['embeddings']):\n",
    "#                 unique_value = data['unique_values'][i]\n",
    "#                 vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "#                 metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "#                 upsert_data.append({\n",
    "#                     \"id\": vector_id,\n",
    "#                     \"values\": embedding.tolist(),\n",
    "#                     \"metadata\": metadata\n",
    "#                 })\n",
    "\n",
    "#             # Batch the upsert to avoid exceeding size limits\n",
    "#             for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "#                 index.upsert(vectors=batch, namespace=table_name)\n",
    "#                 print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# # Main function to execute the process\n",
    "# def main():\n",
    "#     # Step 1: Connect to the database\n",
    "#     conn = connect_to_db()\n",
    "\n",
    "#     # Step 2: Fetch the schema with metadata and data types, only for string columns\n",
    "#     schema_df = fetch_schema_with_data_types(conn)\n",
    "#     print(\"Schema with string data types fetched successfully.\")\n",
    "\n",
    "#     # Step 3: Fetch all unique values along with table details\n",
    "#     unique_values_dict = fetch_all_unique_values_with_table(conn, schema_df)\n",
    "#     print(\"Unique values for string columns fetched successfully.\")\n",
    "\n",
    "#     # Step 4: Load the Hugging Face model for embeddings\n",
    "#     embedding_model = load_huggingface_model()\n",
    "#     print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "#     # Step 5: Generate embeddings for all unique values\n",
    "#     embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "#     print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "#     # Step 6: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "#     pinecone_index = initialize_pinecone()\n",
    "#     upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "#     print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662c7c1-cdbe-47ec-828e-0f9bb261e48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6414278-8d68-4d9f-89e4-72faaea4344e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
