{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fcb4a0-476b-4a63-8c78-3c6012280b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from openai_manager import *\n",
    "from initial_config import *\n",
    "from db_manager import *\n",
    "\n",
    "class Pinecone_manager:\n",
    "    def __init__(self, schema_df):\n",
    "        self.NAMESPACE = []  # Replace with your namespace\n",
    "        self.columnnames = {}\n",
    "        self.searched_cols = []\n",
    "        self.searched_tables = []\n",
    "        self.augmented_input = ''\n",
    "        self.intermediate_input=''\n",
    "        self.schema_df = schema_df\n",
    "        self.extracted_Features = None\n",
    "        self.cleaned_feature_dict = None\n",
    "        self.tokenizer= None\n",
    "        self.pinecone_index = None\n",
    "        self.selection={}\n",
    "        self.selection_required=False\n",
    "        self.ic=Initialize_config()\n",
    "\n",
    "    def clear_all(self):\n",
    "        self.NAMESPACE = []  # Replace with your namespace\n",
    "        self.columnnames = {}\n",
    "        self.searched_cols = []\n",
    "        self.searched_tables = []\n",
    "        self.augmented_input = ''\n",
    "        self.intermediate_input=''\n",
    "        self.selection={}\n",
    "        self.selection_required=False\n",
    "\n",
    "    def process_user_input(self, user_input):\n",
    "        self.extracted_Features = OpenAI_manager.extract_features_with_openai(OpenAI_manager, user_input, self.schema_df)\n",
    "        #print(self.extracted_Features)\n",
    "\n",
    "    def process_extracted_features(self):\n",
    "        def clean_extracted_features(feature_dict):\n",
    "            print(feature_dict)\n",
    "            # Remove any keys with None or empty values\n",
    "            cleaned_feature_dict = {k: v for k, v in feature_dict.items() if v not in [None, '', [], {}, 'none', 'null', 'n/a', 'not specified']}\n",
    "            # Extract the non-null values into a list\n",
    "            feature_list = list(cleaned_feature_dict.values())\n",
    "            return cleaned_feature_dict, feature_list\n",
    "\n",
    "        try:\n",
    "            # Remove the \"## Solution:\" part and any other non-JSON text\n",
    "            json_match = re.search(r'\\{.*\\}', self.extracted_Features, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                # Extract the JSON part from the matched result\n",
    "                cleaned_features = json_match.group(0)\n",
    "\n",
    "                # Convert JSON string to a Python dictionary\n",
    "                feature_dict = json.loads(cleaned_features)\n",
    "\n",
    "                # Clean feature dictionary and feature list to remove nulls and empty values\n",
    "                self.cleaned_feature_dict, feature_list = clean_extracted_features(feature_dict)\n",
    "\n",
    "                # Return cleaned JSON and feature list\n",
    "                return json.dumps(self.cleaned_feature_dict, indent=4), feature_list\n",
    "            else:\n",
    "                return None, []\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"Error parsing features: {e}\")\n",
    "            return None, []\n",
    "\n",
    "    def extract_namespace(self):\n",
    "        for key in self.extracted_dict.keys():\n",
    "            self.NAMESPACE.append(key)\n",
    "            self.columnnames[key] = extracted_dict[key]\n",
    "    #reframe the input with selected values\n",
    "    def call_query_pinecone1(self, user_input, p_i, data):\n",
    "        print(\"pineconedata\", data)\n",
    "        for x in data.keys():\n",
    "            selected=str(data[x])\n",
    "            user_input=user_input.replace(x,selected)\n",
    "        self.augmented_input=user_input\n",
    "        print(\"augumented_input\", self.augmented_input)\n",
    "        self.selection_required=False\n",
    "        \n",
    "    #check if any multiple values for each entity found in vectorDB\n",
    "    def call_query_pinecone(self, user_input, p_i):\n",
    "        res=''\n",
    "        self.pinecone_index = p_i\n",
    "        for key, val in self.cleaned_feature_dict.items():\n",
    "            columns = list(val.keys())\n",
    "            tables=list(val.values())\n",
    "            print(columns)\n",
    "            if self.augmented_input == '':\n",
    "                res = self.query_pinecone_and_augment_input(user_input, key, columns,tables)\n",
    "            else:\n",
    "                res = self.query_pinecone_and_augment_input(self.augmented_input, key, columns,tables)\n",
    "        print(\"augumentedinput\",res)\n",
    "        self.clear_all()\n",
    "        return res\n",
    "    def create_prompt(self,query):\n",
    "        \"\"\"\n",
    "        Creates a structured prompt to handle negation and context.\n",
    "        \"\"\"\n",
    "        if \"not\" in query.lower():\n",
    "            return f\"Input query: {query}. Focus on the meaning and negation.\"\n",
    "        return f\"Input query: {query}. Interpret the query accurately.\"\n",
    "    \n",
    "\n",
    "    def query_pinecone_and_augment_input(self, user_input, namespace, columns,tables):\n",
    "        openai.api_key=self.ic.return_key()\n",
    "        self.augmented_input = user_input\n",
    "\n",
    "        def flatten_dict(d, parent_key=''):\n",
    "            items = []\n",
    "            for k, v in d.items():\n",
    "                new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "                if isinstance(v, dict):\n",
    "                    items.extend(flatten_dict(v, new_key).items())\n",
    "                else:\n",
    "                    items.append((new_key, v))\n",
    "            return dict(items)\n",
    "\n",
    "        flat_entities = flatten_dict(self.cleaned_feature_dict)\n",
    "        #print(flat_entities)\n",
    "        for column_name,table_name in zip(columns,tables):\n",
    "            print(column_name)\n",
    "            \n",
    "            if column_name not in self.searched_cols or table_name not in self.searched_tables:\n",
    "                self.searched_cols.append(column_name)\n",
    "                self.searched_tables.append(table_name)\n",
    "\n",
    "                # Obtain the entity value corresponding to the current column\n",
    "                entity_value = self.cleaned_feature_dict[namespace].get(column_name, None)\n",
    "                #print(entity_value)\n",
    "                if not entity_value:\n",
    "                    print(True)\n",
    "                    continue  # Skip to the next column if no value is found\n",
    "                #entity_value=self.create_prompt(entity_value)\n",
    "\n",
    "                # Generate the query embedding for the entity value\n",
    "                split_entities = entity_value.split()\n",
    "                if len(split_entities)>1:\n",
    "                    entity_value = split_entities[1]\n",
    "                response = openai.embeddings.create(\n",
    "                    model=\"text-embedding-3-large\",  # Correct embedding model\n",
    "                    input=entity_value # Input must be a list\n",
    "                )\n",
    "                embedding = response.data[0].embedding\n",
    "\n",
    "                try:\n",
    "                    result = self.pinecone_index.query(\n",
    "                        namespace=namespace,\n",
    "                        vector=embedding,\n",
    "                        filter={\"column_name\": {\"$eq\": column_name}},\n",
    "                        top_k=3,\n",
    "                        include_values=True,\n",
    "                        include_metadata=True\n",
    "                    )\n",
    "\n",
    "                    matches = result.get('matches', [])\n",
    "                    #print(matches)\n",
    "                    if matches:\n",
    "                        get_match=[]\n",
    "                        # Sort matches by score in descending order\n",
    "                        matches.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "                        # Check if multiple matches have a significant score difference\n",
    "                        best_match = matches[0]\n",
    "                        print(\"match1:\",matches[0]['metadata'].get('unique_value', entity_value))\n",
    "                        print(\"match2:\",matches[1]['metadata'].get('unique_value', entity_value))\n",
    "                        print(\"match3:\",matches[2]['metadata'].get('unique_value', entity_value))\n",
    "                        print(\"Best match:\",matches[0]['metadata'].get('unique_value', entity_value))\n",
    "                        best_score = best_match['score']\n",
    "                        print(\"Best Score:\",best_score)\n",
    "                        selection_required = False\n",
    "                        selected_match = best_match['metadata'].get('unique_value', entity_value)\n",
    "\n",
    "                        # Check if any other match has a score difference < 0.1\n",
    "                        for match in matches[1:]:\n",
    "                            print(\"MAtch score:\",match['score'])\n",
    "                            score_diff = best_score - match['score']\n",
    "                            if score_diff < 0.02:\n",
    "                                selection_required = True\n",
    "                                break\n",
    "                            else:\n",
    "                                continue\n",
    "                                \n",
    "                        if selection_required:\n",
    "                            # Record the values for multiple values to select among the matches\n",
    "                            print(f\"Multiple matches found with significant score difference for '{entity_value}'. Please select:\")\n",
    "                            for i, match in enumerate(matches):\n",
    "                                get_match.append(match['metadata'].get('unique_value', entity_value))\n",
    "                            self.selection[entity_value]=get_match\n",
    "                            self.selection_required=True\n",
    "                        else:\n",
    "                            best_match_for_1_entity = matches[0]['metadata'].get('unique_value', entity_value)\n",
    "                            #print('best_match_for_1_entity', best_match_for_1_entity)\n",
    "                            self.augmented_input = self.augmented_input.replace(entity_value, best_match_for_1_entity)\n",
    "\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error querying Pinecone: {str(e)}\")\n",
    "            else:\n",
    "                print(\"Column already searched\")\n",
    "        if self.selection_required==True:\n",
    "            \n",
    "            print(\"Selection dict:\",self.selection)\n",
    "            print(\"Recent:\",self.intermediate_input)\n",
    "            return {\"selection\": self.selection}\n",
    "        else:\n",
    "            return self.augmented_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2acbc0e-0d5d-48b0-a417-045105d9a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AI-Enabled-Chat-Bot-staging\\schema_manager.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  self.schema_df=pd.read_sql(self.query, self.conn)\n"
     ]
    }
   ],
   "source": [
    "from schema_manager import *\n",
    "DB=DB_Manager()\n",
    "openai_manager=OpenAI_manager()\n",
    "p=Initialize_config()\n",
    "p.assign_pinecone_index()\n",
    "p.process_openAI_model()\n",
    "p.set_prompt_template()\n",
    "db_name=\"zoho_projects_data_v2_backup\"\n",
    "conn = DB.connect(DATABASE_DB = f\"{db_name}\")\n",
    "schema='public'\n",
    "query = f\"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{schema}'\n",
    "        \"\"\"\n",
    "schema_manager=Schema_manager(conn,query,schema)\n",
    "schema_manager.fetch_schema_with_data_types()\n",
    "schema_manager.format_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be103d60-3fee-4f4a-99d5-9674ad1e26f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'projects_zoho_projects_': {'status': 'not completed'}, 'milestones_zoho_projects_': {'status': 'delayed'}}\n",
      "['status']\n",
      "status\n",
      "match1: Completed\n",
      "match2: Cancelled\n",
      "match3: To Do\n",
      "Best match: Completed\n",
      "Best Score: 0.872679412\n",
      "MAtch score: 0.51363337\n",
      "MAtch score: 0.362889528\n",
      "['status']\n",
      "status\n",
      "match1: Overdue\n",
      "match2: Archived\n",
      "match3: Upcoming\n",
      "Best match: Overdue\n",
      "Best Score: 0.479148567\n",
      "MAtch score: 0.395186186\n",
      "MAtch score: 0.372353315\n",
      "augumentedinput give me the count all not Completed projects which have Overdue milestone\n",
      "give me the count all not Completed projects which have Overdue milestone\n"
     ]
    }
   ],
   "source": [
    "user_input=\"give me the count all not completed projects which have delayed milestone\"\n",
    "pine_cone=Pinecone_manager(schema_manager.schema_df)\n",
    "pine_cone.process_user_input(user_input)\n",
    "_, feature_list=pine_cone.process_extracted_features()\n",
    "res1=pine_cone.call_query_pinecone(user_input,p.pinecone_index)\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae43e93d-73d3-4e75-b56f-09475c419f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "from openai_manager import OpenAI_manager\n",
    "from initial_config import Initialize_config\n",
    "from db_manager import DB_Manager\n",
    "from schema_manager import Schema_manager\n",
    "\n",
    "class PineconeManager:\n",
    "    def __init__(self, schema_df):\n",
    "        self.NAMESPACE = []\n",
    "        self.columnnames = {}\n",
    "        self.searched_cols = []\n",
    "        self.searched_tables = []\n",
    "        self.augmented_input = ''\n",
    "        self.intermediate_input = ''\n",
    "        self.schema_df = schema_df\n",
    "        self.extracted_features = None\n",
    "        self.cleaned_feature_dict = None\n",
    "        self.tokenizer = None\n",
    "        self.pinecone_index = None\n",
    "        self.selection = {}\n",
    "        self.selection_required = False\n",
    "        self.ic = Initialize_config()\n",
    "\n",
    "    def clear_all(self):\n",
    "        \"\"\"Reset all class variables to their initial state.\"\"\"\n",
    "        self.NAMESPACE = []\n",
    "        self.columnnames = {}\n",
    "        self.searched_cols = []\n",
    "        self.searched_tables = []\n",
    "        self.augmented_input = ''\n",
    "        self.intermediate_input = ''\n",
    "        self.selection = {}\n",
    "        self.selection_required = False\n",
    "\n",
    "    def process_user_input(self, user_input):\n",
    "        \"\"\"Extract features using OpenAI.\"\"\"\n",
    "        self.extracted_features = OpenAI_manager.extract_features_with_openai(\n",
    "            OpenAI_manager, user_input, self.schema_df\n",
    "        )\n",
    "\n",
    "    def process_extracted_features(self):\n",
    "        \"\"\"Clean and parse extracted features into a structured format.\"\"\"\n",
    "        def clean_extracted_features(feature_dict):\n",
    "            cleaned_feature_dict = {\n",
    "                k: v for k, v in feature_dict.items()\n",
    "                if v not in [None, '', [], {}, 'none', 'null', 'n/a', 'not specified']\n",
    "            }\n",
    "            feature_list = list(cleaned_feature_dict.values())\n",
    "            return cleaned_feature_dict, feature_list\n",
    "\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', self.extracted_features, re.DOTALL)\n",
    "            if json_match:\n",
    "                cleaned_features = json_match.group(0)\n",
    "                feature_dict = json.loads(cleaned_features)\n",
    "                self.cleaned_feature_dict, feature_list = clean_extracted_features(feature_dict)\n",
    "                return json.dumps(self.cleaned_feature_dict, indent=4), feature_list\n",
    "            return None, []\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"Error parsing features: {e}\")\n",
    "            return None, []\n",
    "\n",
    "    def call_query_pinecone(self, user_input, pinecone_index):\n",
    "        \"\"\"Process input with Pinecone to refine the query.\"\"\"\n",
    "        self.pinecone_index = pinecone_index\n",
    "        for key, val in self.cleaned_feature_dict.items():\n",
    "            columns = list(val.keys())\n",
    "            tables = list(val.values())\n",
    "            if not self.augmented_input:\n",
    "                res = self.query_pinecone_and_augment_input(user_input, key, columns, tables)\n",
    "            else:\n",
    "                res = self.query_pinecone_and_augment_input(self.augmented_input, key, columns, tables)\n",
    "        self.clear_all()\n",
    "        return res\n",
    "\n",
    "    def query_pinecone_and_augment_input(self, user_input, namespace, columns, tables):\n",
    "        \"\"\"Query Pinecone and augment user input with refined values.\"\"\"\n",
    "        openai.api_key = self.ic.return_key()\n",
    "        self.augmented_input = user_input\n",
    "\n",
    "        for column_name, table_name in zip(columns, tables):\n",
    "            if column_name not in self.searched_cols or table_name not in self.searched_tables:\n",
    "                self.searched_cols.append(column_name)\n",
    "                self.searched_tables.append(table_name)\n",
    "\n",
    "                entity_value = self.cleaned_feature_dict[namespace].get(column_name, None)\n",
    "                if not entity_value:\n",
    "                    continue\n",
    "\n",
    "                response = openai.embeddings.create(\n",
    "                    model=\"text-embedding-3-large\",  # Correct embedding model\n",
    "                    input=entity_value # Input must be a list\n",
    "                )\n",
    "                embedding = response.data[0].embedding\n",
    "\n",
    "                try:\n",
    "                    result = self.pinecone_index.query(\n",
    "                        namespace=namespace,\n",
    "                        vector=embedding,\n",
    "                        filter={\"column_name\": {\"$eq\": column_name}},\n",
    "                        top_k=3,\n",
    "                        include_values=True,\n",
    "                        include_metadata=True\n",
    "                    )\n",
    "                    matches = result.get('matches', [])\n",
    "                    if matches:\n",
    "                        matches.sort(key=lambda x: x['score'], reverse=True)\n",
    "                        best_match = matches[0]\n",
    "                        best_score = best_match['score']\n",
    "                        selection_required = False\n",
    "\n",
    "                        for match in matches[1:]:\n",
    "                            score_diff = best_score - match['score']\n",
    "                            if score_diff < 0.02:\n",
    "                                selection_required = True\n",
    "                                break\n",
    "\n",
    "                        if selection_required:\n",
    "                            self.selection[entity_value] = [\n",
    "                                match['metadata'].get('unique_value', entity_value) for match in matches\n",
    "                            ]\n",
    "                            self.selection_required = True\n",
    "                        else:\n",
    "                            best_match_value = matches[0]['metadata'].get('unique_value', entity_value)\n",
    "                            self.augmented_input = self.augmented_input.replace(entity_value, best_match_value)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error querying Pinecone: {str(e)}\")\n",
    "            else:\n",
    "                print(\"Column already searched\")\n",
    "\n",
    "        return {\"selection\": self.selection} if self.selection_required else self.augmented_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e182cbe6-0d00-4ab6-a4e1-c25d0d4a9cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AI-Enabled-Chat-Bot-staging\\schema_manager.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  self.schema_df=pd.read_sql(self.query, self.conn)\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me the count of all Completed projects which have Overdue milestone\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DB = DB_Manager()\n",
    "    openai_manager = OpenAI_manager()\n",
    "    config = Initialize_config()\n",
    "    config.assign_pinecone_index()\n",
    "    config.process_openAI_model()\n",
    "    config.set_prompt_template()\n",
    "\n",
    "    db_name = \"zoho_projects_data_v2_backup\"\n",
    "    conn = DB.connect(DATABASE_DB=db_name)\n",
    "    schema = 'public'\n",
    "    query = f\"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{schema}'\n",
    "    \"\"\"\n",
    "    schema_manager = Schema_manager(conn, query, schema)\n",
    "    schema_manager.fetch_schema_with_data_types()\n",
    "    schema_manager.format_schema()\n",
    "\n",
    "    user_input = \"give me the count of all not completed projects which have delayed milestone\"\n",
    "    pinecone_manager = PineconeManager(schema_manager.schema_df)\n",
    "    pinecone_manager.process_user_input(user_input)\n",
    "    _, feature_list = pinecone_manager.process_extracted_features()\n",
    "    result = pinecone_manager.call_query_pinecone(user_input, config.pinecone_index)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65fa72cb-a2d3-4509-9d30-b3adc0aa8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_synonyms: {'not', 'completed'}\n",
      "not completed -> not\n",
      "common_synonyms: {'heavy', 'most'}\n",
      "most heavy -> heavy\n",
      "common_synonyms: {'show', 'premier', 'time'}\n",
      "premier show time -> show\n",
      "common_synonyms: {'vision', 'computer'}\n",
      "computer vision -> vision\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_unigram_from_ngram(ngram: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds a unigram with a similar meaning to a given bigram or trigram using WordNet.\n",
    "\n",
    "    Args:\n",
    "        ngram (str): Input bigram or trigram (e.g., 'machine learning').\n",
    "\n",
    "    Returns:\n",
    "        str: A unigram with a similar meaning, if found; otherwise, an empty string.\n",
    "    \"\"\"\n",
    "    # Split the ngram into words\n",
    "    words = ngram.split()\n",
    "\n",
    "    # Generate synonyms using WordNet for each word\n",
    "    synonyms = set()\n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "\n",
    "    # Find the intersection of synonyms for all words\n",
    "    # This is a naive heuristic; a more advanced method may use semantic similarity.\n",
    "    common_synonyms = synonyms.intersection(words)\n",
    "    print(\"common_synonyms:\",common_synonyms)\n",
    "\n",
    "    # Return a suitable unigram if found, otherwise a fallback\n",
    "    return list(common_synonyms)[0] if common_synonyms else \"No match found\"\n",
    "\n",
    "# Example usage\n",
    "bigrams = [\"not completed\", \"most heavy\"]\n",
    "trigrams = [\"premier show time\", \"computer vision\"]\n",
    "\n",
    "for phrase in bigrams + trigrams:\n",
    "    unigram = get_unigram_from_ngram(phrase)\n",
    "    print(f\"{phrase} -> {unigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4744c899-9410-4dc6-9db0-3af0e46c513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning -> AI\n",
      "deep learning -> AI\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_closest_unigram(ngram: str, candidate_unigrams: list) -> str:\n",
    "    \"\"\"\n",
    "    Finds the closest unigram to a given n-gram using embeddings.\n",
    "\n",
    "    Args:\n",
    "        ngram (str): Input bigram or trigram (e.g., 'machine learning').\n",
    "        candidate_unigrams (list): List of unigrams to compare against.\n",
    "\n",
    "    Returns:\n",
    "        str: The closest unigram by semantic similarity.\n",
    "    \"\"\"\n",
    "    # Compute embedding for the n-gram\n",
    "    ngram_embedding = model.encode(ngram, convert_to_tensor=True)\n",
    "\n",
    "    # Compute embeddings for unigrams\n",
    "    unigram_embeddings = model.encode(candidate_unigrams, convert_to_tensor=True)\n",
    "\n",
    "    # Find the most similar unigram\n",
    "    similarities = util.pytorch_cos_sim(ngram_embedding, unigram_embeddings)\n",
    "    closest_index = similarities.argmax().item()\n",
    "    return candidate_unigrams[closest_index]\n",
    "\n",
    "# Example usage\n",
    "bigrams = [\"machine learning\", \"deep learning\"]\n",
    "candidate_unigrams = [\"AI\", \"automation\", \"technology\", \"intelligence\"]\n",
    "\n",
    "for phrase in bigrams:\n",
    "    closest_unigram = find_closest_unigram(phrase, candidate_unigrams)\n",
    "    print(f\"{phrase} -> {closest_unigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f66d1d4-934d-4dea-8b3b-cc1ae0d2bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not completed -> incomplete\n",
      "not started -> uninitiated\n",
      "machine learning -> AI\n",
      "natural language processing -> NLP\n"
     ]
    }
   ],
   "source": [
    "def get_specific_unigram(phrase: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a specific unigram equivalent for a given bigram/trigram.\n",
    "\n",
    "    Args:\n",
    "        phrase (str): Input bigram or trigram (e.g., 'not completed').\n",
    "\n",
    "    Returns:\n",
    "        str: The specific unigram if found; otherwise, the original phrase.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"not completed\": \"incomplete\",\n",
    "        \"not started\": \"uninitiated\",\n",
    "        \"not working\": \"broken\",\n",
    "        \"not allowed\": \"prohibited\",\n",
    "        \"natural language processing\": \"NLP\",\n",
    "        \"machine learning\": \"AI\"\n",
    "    }\n",
    "    return mapping.get(phrase.lower(), phrase)\n",
    "\n",
    "phrases = [\"not completed\", \"not started\", \"machine learning\", \"natural language processing\"]\n",
    "for phrase in phrases:\n",
    "    unigram = get_specific_unigram(phrase)\n",
    "    print(f\"{phrase} -> {unigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628e567-4fb3-4c20-93ab-5df174a197ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
