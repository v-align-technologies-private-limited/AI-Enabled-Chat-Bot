{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7kJBL5ecUtT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import openai\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Set environment variables securely for sensitive information\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD =\"valign#123\"  # Make sure this is set in your environment variables\n",
    "# DATABASE_DB = \"zoho_projects_data_v2_backup\"\n",
    "# PORT = 5432\n",
    "\n",
    "# Set OpenAI API key\n",
    "OPENAI_API_KEY = \"\"  # Replace with your OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Function to connect to the database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        connection_url = f\"postgresql+psycopg2://{DATABASE_USERNAME}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{PORT}/{DATABASE_DB}\"\n",
    "        engine = create_engine(connection_url)\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to get schema information from the database\n",
    "def get_schema_info():\n",
    "    engine = connect_to_db()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public';\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema information: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Function to filter schema based on extracted entities\n",
    "def get_schema_filter_info(extracted_entities):\n",
    "    # Extract the table names from the `extracted_entities` dictionary\n",
    "    tables_of_interest = list(extracted_entities.keys())\n",
    "    print(\"tables_of_interest\",tables_of_interest)\n",
    "\n",
    "    engine = connect_to_db()\n",
    "    try:\n",
    "        # Query to get schema information for the specified tables\n",
    "        query = f\"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "          AND table_name IN ({', '.join([f\"'{table}'\" for table in tables_of_interest])});\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema information: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Function to extract features with OpenAI based on user input and schema\n",
    "def extract_features_with_openai(user_input, processed_schema_df):\n",
    "    schema_json = processed_schema_df.to_json(orient='records')\n",
    "\n",
    "    # Define dynamic extraction keywords for aggregation and entity-specific queries\n",
    "    dynamic_keywords = {\n",
    "        \"aggregation\": [\"highest\", \"most\", \"maximum\", \"max\", \"largest\", \"lowest\", \"least\", \"minimum\", \"min\", \"count\", \"sum\", \"average\"],\n",
    "        \"entity_specific\": [\"who\", \"where\", \"which\", \"what\", \"how many\", \"how much\"]\n",
    "    }\n",
    "\n",
    "    # Prompt preparation: Guide OpenAI to understand schema and extract relevant features\n",
    "    prompt = f\"\"\"\n",
    "    ### Database Schema Overview\n",
    "    The following schema provides the columns and tables available in the database. Use this schema to interpret user input accurately:\n",
    "    {schema_json}\n",
    "\n",
    "    ### User Query\n",
    "    The user has input: \"{user_input}\"\n",
    "\n",
    "    ### Instructions\n",
    "    1. **Understand User Intent**:\n",
    "       - Carefully read the user query both left-to-right and right-to-left to identify potential entities, relationships, and operations.\n",
    "       - Use a \"chain of thoughts\" approach to break down the query into parts and analyze the intent for each segment.\n",
    "       - Identify if the query indicates aggregation (e.g., maximum, minimum, count) or specific entities (e.g., user names, project names, milestone names, status etc.).\n",
    "       - Ensure that synonyms and closely related terms are mapped accurately to schema components.\n",
    "\n",
    "    2. **Extract Relevant Entities**:\n",
    "       - Match user input to text-based columns in the schema (e.g., `varchar`, `char`, `text`).\n",
    "       - Handle partial matches, synonyms, and spelling variations by considering contextual clues from the schema.\n",
    "       - Resolve foreign keys to their corresponding descriptive columns (e.g., `assigneeid` -> `username`,`projectid` -> `projectname`,`milestoneid` -> `milestonename`).\n",
    "\n",
    "    3. **Classify Query Type**:\n",
    "       - Determine if the query is:\n",
    "         a. An **aggregation query**, requiring calculations like maximum, minimum, or count.\n",
    "         b. An **entity-specific query**, focusing on particular items or details.\n",
    "       - Provide reasoning for the classification in the output.\n",
    "\n",
    "    4. **Expected Output Format**:\n",
    "       - A JSON object containing:\n",
    "         a. `query_type`: Either \"aggregation\" or \"entity_extraction\".\n",
    "         b. `intent_analysis`: A breakdown of user intent and reasoning.\n",
    "         c. `extracted_entities`: Relevant tables, columns, and matched values from the schema.\n",
    "\n",
    "    5. **Handle Foreign Keys Dynamically**:\n",
    "       - If a foreign key column exists (e.g., `assigneeid`,`projectid`,`milestoneid`) but points to a non-text field, exclude it.\n",
    "       - If the foreign key references a string-based column (e.g., `username`,`projectname`,`milestonename`), include only the string-based column from the related table.\n",
    "\n",
    "    ### Example Outputs\n",
    "    For the query \"Who logged the highest hours last month?\":\n",
    "    {{\n",
    "        \"query_type\": \"aggregation\",\n",
    "        \"intent_analysis\": \"The query asks for the user who logged the most hours, requiring aggregation on the hours column.\",\n",
    "        \"extracted_entities\": {{\n",
    "            \"users_zoho_projects_\": {{\n",
    "                \"username\": \"resolved dynamically\"\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    For the query \"How many hours did Rishi log last month?\":\n",
    "    {{\n",
    "        \"query_type\": \"entity_extraction\",\n",
    "        \"intent_analysis\": \"The query asks for specific details about a user (Rishi) and the hours logged.\",\n",
    "        \"extracted_entities\": {{\n",
    "            \"users_zoho_projects_\": {{\n",
    "                \"username\": \"Rishi\"\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    For the query \"What is the status of project Voice enabled which is assigned to Basavaraj?\":\n",
    "    {{\n",
    "        \"query_type\": \"entity_extraction\",\n",
    "        \"intent_analysis\": \"The query asks for the status of a project assigned to a specific user.\",\n",
    "        \"extracted_entities\": {{\n",
    "            \"projects_zoho_projects_\": {{\n",
    "                \"projectname\": \"Voice enabled\",\n",
    "                \"status\": \"resolved dynamically\"\n",
    "            }},\n",
    "            \"users_zoho_projects_\": {{\n",
    "                \"username\": \"Basavaraj\"\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    ### User Query Analysis and Output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use OpenAI to analyze the query dynamically\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert assistant skilled in understanding and extracting entities from user queries based on a database schema.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=7000,\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        # Extract the response content\n",
    "        response_content = response.choices[0].message.content.strip()\n",
    "        print(\"response_content:\", response_content)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        extracted_features = json.loads(response_content)\n",
    "        print(\"extracted_features:\", extracted_features)\n",
    "\n",
    "        return extracted_features\n",
    "\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error with OpenAI API: {e}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing OpenAI response: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to generate SQL query using schema and data context\n",
    "def generate_sql_query(processed_schema_str, response_content):\n",
    "    # Extract intent and entities from response_content\n",
    "    intent_analysis = response_content.get(\"intent_analysis\", \"\")\n",
    "    extracted_entities = response_content.get(\"extracted_entities\", {})\n",
    "\n",
    "    # Create a prompt with the context provided by response_content\n",
    "    prompt = f\"\"\"\n",
    "    You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query and return ONLY the generated query. Use the following guidelines for SQL generation:\n",
    "    ### Database Schema Overview\n",
    "    The following schema provides the columns and tables available in the database:\n",
    "    {processed_schema_str}\n",
    "\n",
    "\n",
    "    ### User Intent Analysis\n",
    "    {intent_analysis}\n",
    "\n",
    "    ### Extracted Entities\n",
    "    The following entities have been extracted from the input:\n",
    "    {extracted_entities}\n",
    "\n",
    "    ### Instructions\n",
    "    - The input may contain partial or similar names for entities, such as project names, user names, or status descriptions. Handle these using `ILIKE` operators with `%` on both sides of the string to allow flexible matching and accurate querying.\n",
    "    - **Do not use column names directly from user input** without validation. Cross-check user terms against the provided database schema and select the most relevant and existing columns.\n",
    "    - If the user input mentions a column that doesn't exist (e.g., `due_date`), **substitute it with a semantically similar, valid column** from the schema (e.g., use `end_date` if `due_date` is requested but not found).\n",
    "    - Ensure you match project names like 'IIFL Samasta', 'IIfl Samasta', 'IIFL', 'iifl smasta', 'Iiflsmsota' (or any spelling variations) to the correct project name from the database.\n",
    "    - Use the column `instnm` whenever the question involves institute names, and ensure it is correctly associated with `unitid` in the query.\n",
    "    - When context involves multiple tables, use JOIN operations, ensuring only appropriate columns (e.g., `unitid`) are used for these joins while maintaining referential integrity.\n",
    "    - **Whenever using foreign key columns**, always **select the corresponding string column from the related table** for clarity. For example, if `owner_id` from the `projects` table is a foreign key referencing `user_id` in the `users` table, select the `user_name` (or relevant name column) from the `users` table, not just the ID.\n",
    "    - For calculations such as averages or ratios, ensure proper aggregation using `AVG()`, `SUM()`, or other relevant functions, maintaining clarity and correctness in grouped results.\n",
    "    - Apply detailed filtering criteria from the input in the `WHERE` clause, using logical operators like `AND` and `OR` to combine conditions effectively.\n",
    "    - Use date and timestamp functions like `TO_TIMESTAMP()` and `EXTRACT` to handle date parsing and extraction of date components as needed.\n",
    "    - Employ the `GROUP BY` clause when aggregating by categories or attributes for clear grouping.\n",
    "    - Maintain readability with table and column aliases, especially for complex joins or subqueries.\n",
    "    - Utilize subqueries or Common Table Expressions (CTEs) for breaking down complex logic into simpler, modular parts for better performance and clarity.\n",
    "    - Limit the number of rows in the result set to a maximum of 100 rows unless specified otherwise to optimize performance and avoid excessive data output.\n",
    "\n",
    "    - Ensure comprehensive understanding of user input and intent by interpreting queries from both left to right and right to left to capture full context.\n",
    "    - Use a range of SQL operators (`=`, `!=`, `LIKE`, `ILIKE`, `IN`) and functions (`IFNULL`, `ISNULL`, `COALESCE`) as needed to handle specific conditions, null handling, and flexible matching.\n",
    "    - Integrate advanced SQL clauses like `CASE` and `HAVING` when addressing conditional logic or grouped results.\n",
    "    - Safeguard the handling of NULLs to avoid logical errors, ensuring expressions that involve NULLs behave as expected.\n",
    "    - Address complex filtering by using nested conditions and logical operators, aligning the SQL with the nuanced requirements of the user query.\n",
    "    - Confirm all table relationships in JOINs are clear and maintain referential integrity to support correct data linkage.\n",
    "    - Prioritize performance by avoiding unnecessary complexity or inefficient constructs, focusing on optimized query design.\n",
    "    - Handle edge cases where user input may imply similar or interchangeable column names, ensuring the selected columns are contextually appropriate (e.g., using `end_date` instead of `due_date` if the former is present in the schema).\n",
    "    - **For foreign key relationships, always ensure that the related tableâ€™s string column (such as user names, project names, etc.) is also selected when referring to the foreign key.**\n",
    "\n",
    "    Ensure the final SQL query is well-structured, efficient, and accurately reflects the user's request based on the provided input. The response should not include explanations, comments, or any other content beyond the generated SQL query.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI to generate the SQL query\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in PostgreSQL query generation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=7000,\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        # Extract generated query from response\n",
    "        generated_query = response.choices[0].message.content.strip()\n",
    "        print(\"Generated SQL Query:\", generated_query)\n",
    "\n",
    "        return generated_query\n",
    "\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error with OpenAI API: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jpvfaJTcUxe",
    "outputId": "3e1c7b4e-2530-4504-9c1c-f21353e97f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_content: {\n",
      "    \"query_type\": \"entity_extraction\",\n",
      "    \"intent_analysis\": \"The query seeks to identify issues related to projects specifically named 'iDatalytics'. This involves extracting issues associated with a particular project name.\",\n",
      "    \"extracted_entities\": {\n",
      "        \"projects_zoho_projects_\": {\n",
      "            \"projectname\": \"iDatalytics\"\n",
      "        },\n",
      "        \"issues_zoho_projects_\": {\n",
      "            \"projectid\": \"resolved dynamically\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "extracted_features: {'query_type': 'entity_extraction', 'intent_analysis': \"The query seeks to identify issues related to projects specifically named 'iDatalytics'. This involves extracting issues associated with a particular project name.\", 'extracted_entities': {'projects_zoho_projects_': {'projectname': 'iDatalytics'}, 'issues_zoho_projects_': {'projectid': 'resolved dynamically'}}}\n",
      "Query Type <class 'dict'>\n",
      "Extractewd Featues Clean {'projects_zoho_projects_': {'projectname': 'iDatalytics'}, 'issues_zoho_projects_': {'projectid': 'resolved dynamically'}}\n",
      "tables_of_interest ['projects_zoho_projects_', 'issues_zoho_projects_']\n",
      "filtered_schema_df:                  table_name            column_name  \\\n",
      "0     issues_zoho_projects_               bugtitle   \n",
      "1     issues_zoho_projects_            createddate   \n",
      "2     issues_zoho_projects_                 status   \n",
      "3     issues_zoho_projects_               severity   \n",
      "4     issues_zoho_projects_                duedate   \n",
      "5     issues_zoho_projects_            description   \n",
      "6     issues_zoho_projects_                 module   \n",
      "7     issues_zoho_projects_         classification   \n",
      "8     issues_zoho_projects_           reproducible   \n",
      "9     issues_zoho_projects_                   flag   \n",
      "10    issues_zoho_projects_       lastmodifieddate   \n",
      "11    issues_zoho_projects_            completedon   \n",
      "12    issues_zoho_projects_                  bugid   \n",
      "13    issues_zoho_projects_             reporterid   \n",
      "14    issues_zoho_projects_             assigneeid   \n",
      "15    issues_zoho_projects_              projectid   \n",
      "16    issues_zoho_projects_            milestoneid   \n",
      "17    issues_zoho_projects_            planneddays   \n",
      "18    issues_zoho_projects_        timetakenindays   \n",
      "19    issues_zoho_projects_            delayindays   \n",
      "20    issues_zoho_projects_         toplevelstatus   \n",
      "21    issues_zoho_projects_               timeleft   \n",
      "22    issues_zoho_projects_      bugcompletionmode   \n",
      "23    issues_zoho_projects_    timesincelastchange   \n",
      "24    issues_zoho_projects_               isclosed   \n",
      "25    issues_zoho_projects_                 bugkey   \n",
      "26    issues_zoho_projects_        escalationlevel   \n",
      "27    issues_zoho_projects_              bugstatus   \n",
      "28    issues_zoho_projects_    affectedmilestoneid   \n",
      "29    issues_zoho_projects_              issuetype   \n",
      "30    issues_zoho_projects_        associatedteams   \n",
      "31  projects_zoho_projects_            projectname   \n",
      "32  projects_zoho_projects_              startdate   \n",
      "33  projects_zoho_projects_                enddate   \n",
      "34  projects_zoho_projects_                 status   \n",
      "35  projects_zoho_projects_           deliveryteam   \n",
      "36  projects_zoho_projects_            completedon   \n",
      "37  projects_zoho_projects_  projecteffortsmandays   \n",
      "38  projects_zoho_projects_              projectid   \n",
      "39  projects_zoho_projects_           budgetamount   \n",
      "40  projects_zoho_projects_            budgethours   \n",
      "41  projects_zoho_projects_             taskprefix   \n",
      "42  projects_zoho_projects_            createdtime   \n",
      "43  projects_zoho_projects_       lastmodifiedtime   \n",
      "44  projects_zoho_projects_                ownerid   \n",
      "45  projects_zoho_projects_            description   \n",
      "46  projects_zoho_projects_                    key   \n",
      "47  projects_zoho_projects_            projecttype   \n",
      "48  projects_zoho_projects_          projectendlag   \n",
      "49  projects_zoho_projects_                country   \n",
      "50  projects_zoho_projects_           requiredapps   \n",
      "51  projects_zoho_projects_   planneddurationhours   \n",
      "52  projects_zoho_projects_               dealname   \n",
      "53  projects_zoho_projects_                  istnm   \n",
      "54  projects_zoho_projects_         timelinestatus   \n",
      "55  projects_zoho_projects_           healthstatus   \n",
      "56  projects_zoho_projects_      delaydaysasondate   \n",
      "57  projects_zoho_projects_             projecturl   \n",
      "58  projects_zoho_projects_              createdby   \n",
      "59  projects_zoho_projects_             modifiedby   \n",
      "\n",
      "                      data_type  \n",
      "0                          text  \n",
      "1   timestamp without time zone  \n",
      "2                          text  \n",
      "3                          text  \n",
      "4   timestamp without time zone  \n",
      "5                          text  \n",
      "6                          text  \n",
      "7                          text  \n",
      "8                          text  \n",
      "9                          text  \n",
      "10  timestamp without time zone  \n",
      "11  timestamp without time zone  \n",
      "12                       bigint  \n",
      "13                       bigint  \n",
      "14                       bigint  \n",
      "15                       bigint  \n",
      "16                       bigint  \n",
      "17             double precision  \n",
      "18                       bigint  \n",
      "19             double precision  \n",
      "20                         text  \n",
      "21             double precision  \n",
      "22                         text  \n",
      "23                       bigint  \n",
      "24                         text  \n",
      "25                       bigint  \n",
      "26                       bigint  \n",
      "27                         text  \n",
      "28                       bigint  \n",
      "29                         text  \n",
      "30                         text  \n",
      "31                         text  \n",
      "32  timestamp without time zone  \n",
      "33  timestamp without time zone  \n",
      "34                         text  \n",
      "35                         text  \n",
      "36  timestamp without time zone  \n",
      "37                         text  \n",
      "38                       bigint  \n",
      "39             double precision  \n",
      "40             double precision  \n",
      "41                         text  \n",
      "42  timestamp without time zone  \n",
      "43  timestamp without time zone  \n",
      "44                       bigint  \n",
      "45                         text  \n",
      "46                         text  \n",
      "47                         text  \n",
      "48                         text  \n",
      "49                         text  \n",
      "50                         text  \n",
      "51                         text  \n",
      "52                         text  \n",
      "53                         text  \n",
      "54                         text  \n",
      "55                         text  \n",
      "56                         text  \n",
      "57                         text  \n",
      "58                       bigint  \n",
      "59                       bigint  \n",
      "Generated SQL Query: ```sql\n",
      "SELECT i.*\n",
      "FROM issues_zoho_projects_ i\n",
      "JOIN projects_zoho_projects_ p ON i.projectid = p.projectid\n",
      "WHERE p.projectname ILIKE '%iDatalytics%';\n",
      "```\n",
      "Generated SQL Query: ```sql\n",
      "SELECT i.*\n",
      "FROM issues_zoho_projects_ i\n",
      "JOIN projects_zoho_projects_ p ON i.projectid = p.projectid\n",
      "WHERE p.projectname ILIKE '%iDatalytics%';\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Run the main script\n",
    "if __name__ == \"__main__\":\n",
    "    processed_schema_df = get_schema_info()\n",
    "\n",
    "    # Sample user input\n",
    "    user_input = \"What are the issues associated with all 'iDatalytics' projects?\"\n",
    "\n",
    "    # Extract features using OpenAI\n",
    "    response_content = extract_features_with_openai(user_input, processed_schema_df)\n",
    "    print(\"Query Type\", type(response_content.get(\"extracted_entities\")))\n",
    "    print(\"Extractewd Featues Clean\", response_content.get(\"extracted_entities\"))\n",
    "\n",
    "    # Filter schema based on extracted entities\n",
    "    filtered_schema_df = get_schema_filter_info(response_content.get(\"extracted_entities\", {}))\n",
    "    #print(f\"filtered_schema_df: {filtered_schema_df}\")\n",
    "\n",
    "    # Generate SQL query based on user input and extracted features\n",
    "    generated_sql_query = generate_sql_query(filtered_schema_df.to_json(orient='records'), response_content)\n",
    "    print(f\"Generated SQL Query: {generated_sql_query}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
