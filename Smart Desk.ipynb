{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db20bcc-bfdc-46c6-9716-8c388bb3ddbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the status of a project IIFL Samasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"IIFL Samasta\", \"status\": \"status\"},\n",
      "        \"tasks\": {\"task_name\": null, \"owner\": null}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT status FROM projects WHERE project_name LIKE 'IIFL Samasta'\"\n",
      "    }\n",
      "Extracted Entities: {\n",
      "  \"projects\": {\n",
      "    \"project_name\": \"IIFL Samasta\",\n",
      "    \"status\": \"status\"\n",
      "  },\n",
      "  \"tasks\": {\n",
      "    \"task_name\": null,\n",
      "    \"owner\": null\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE 'IIFL Samasta'\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE 'IIFL Samasta'\n",
      "Query Results:\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE 'IIFL Samasta'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the status of a project IIFL Samasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"IIFL Samasta\", \"status\": \"status\"},\n",
      "        \"tasks\": {\"task_name\": None, \"owner\": None}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 4 column 32 (char 127)\n",
      "Response text: {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"IIFL Samasta\", \"status\": \"status\"},\n",
      "        \"tasks\": {\"task_name\": None, \"owner\": None}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the status of a project IIFL Samasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"IIFL Samasta\", \"status\": \"status\"},\n",
      "        \"tasks\": {\"task_name\": \"None\", \"owner\": \"None\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\"\n",
      "    }\n",
      "Extracted Entities: {\n",
      "  \"projects\": {\n",
      "    \"project_name\": \"IIFL Samasta\",\n",
      "    \"status\": \"status\"\n",
      "  },\n",
      "  \"tasks\": {\n",
      "    \"task_name\": \"None\",\n",
      "    \"owner\": \"None\"\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\n",
      "Query Results:\n",
      "('To Do',)\n",
      "('Requirement Gathering',)\n",
      "('Development',)\n",
      "('Development',)\n",
      "('To Do',)\n",
      "('To Do',)\n",
      "('To Do',)\n",
      "('To Do',)\n",
      "('Development',)\n",
      "('To Do',)\n",
      "('Development',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Development',)\n",
      "('Development',)\n",
      "('To Do',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the status of a project IIFL Samasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"IIFL Samasta\", \"status\": \"status\"},\n",
      "        \"tasks\": {\"task_name\": \"\", \"owner\": \"\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\"\n",
      "    }\n",
      "Extracted Entities: {\n",
      "  \"projects\": {\n",
      "    \"project_name\": \"IIFL Samasta\",\n",
      "    \"status\": \"status\"\n",
      "  },\n",
      "  \"tasks\": {\n",
      "    \"task_name\": \"\",\n",
      "    \"owner\": \"\"\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\n",
      "Query Results:\n",
      "('To Do',)\n",
      "('Requirement Gathering',)\n",
      "('Development',)\n",
      "('Development',)\n",
      "('To Do',)\n",
      "('To Do',)\n",
      "('To Do',)\n",
      "('To Do',)\n",
      "('Development',)\n",
      "('To Do',)\n",
      "('Development',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Development',)\n",
      "('Development',)\n",
      "('To Do',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL Samasta%'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"Extracted Project Name\", \"status\": \"Extracted Status\"},\n",
      "        \"tasks\": {\"task_name\": \"Extracted Task Name\", \"owner\": \"Extracted Owner\"},\n",
      "        \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE project_name LIKE '%Extracted Project Name%' AND owner LIKE '%Extracted Owner%' AND status = 'Extracted Status' ORDER BY milestone_end_lag DESC LIMIT 1;\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "Response text: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"Extracted Project Name\", \"status\": \"Extracted Status\"},\n",
      "        \"tasks\": {\"task_name\": \"Extracted Task Name\", \"owner\": \"Extracted Owner\"},\n",
      "        \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE project_name LIKE '%Extracted Project Name%' AND owner LIKE '%Extracted Owner%' AND status = 'Extracted Status' ORDER BY milestone_end_lag DESC LIMIT 1;\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: ## Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"N/A\", \"status\": \"N/A\"},\n",
      "        \"tasks\": {\"task_name\": \"N/A\", \"owner\": \"N/A\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestones.milestone_name FROM milestones ORDER BY milestones.duration DESC LIMIT 1\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "Response text: ## Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": \"N/A\", \"status\": \"N/A\"},\n",
      "        \"tasks\": {\"task_name\": \"N/A\", \"owner\": \"N/A\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestones.milestone_name FROM milestones ORDER BY milestones.duration DESC LIMIT 1\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_name\": \"longest delayed\", \"status\": \"delayed\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE status LIKE '%delayed%' ORDER BY duration DESC LIMIT 1\"\n",
      "    }\n",
      "Extracted Entities: {\n",
      "  \"milestones\": {\n",
      "    \"milestone_name\": \"longest delayed\",\n",
      "    \"status\": \"delayed\"\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE status LIKE '%delayed%' ORDER BY duration DESC LIMIT 1\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE status LIKE '%delayed%' ORDER BY duration DESC LIMIT 1\n",
      "Query Results:\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE status LIKE '%delayed%' ORDER BY duration DESC LIMIT 1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "  \"entities\": {\n",
      "    \"milestones\": {\"milestone_name\": \"Longest Delayed Milestone Name\"}\n",
      "  },\n",
      "  \"sql_query\": \"SELECT milestone_name FROM milestones ORDER BY milestone_end_lag DESC LIMIT 1\"\n",
      "}\n",
      "Extracted Entities: {\n",
      "  \"milestones\": {\n",
      "    \"milestone_name\": \"Longest Delayed Milestone Name\"\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones ORDER BY milestone_end_lag DESC LIMIT 1\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones ORDER BY milestone_end_lag DESC LIMIT 1\n",
      "Query Results:\n",
      "('Milestone Twelve-Post Go-Live Application Support - Zoho ONE',)\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones ORDER BY milestone_end_lag DESC LIMIT 1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": null, \"status\": null},\n",
      "        \"tasks\": {\"task_name\": null, \"owner\": null}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones ORDER BY duration DESC LIMIT 1\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "Response text: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"projects\": {\"project_name\": null, \"status\": null},\n",
      "        \"tasks\": {\"task_name\": null, \"owner\": null}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones ORDER BY duration DESC LIMIT 1\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_end_lag\": \"Extracted Milestone End Lag\", \"status\": \"Extracted Status\"},\n",
      "        \"projects\": {\"project_name\": \"Extracted Project Name\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE project_name LIKE 'Extracted Project Name' AND status = 'Extracted Status' ORDER BY milestone_end_lag DESC LIMIT 1;\"\n",
      "    }\n",
      "Extracted Entities: {\n",
      "  \"milestones\": {\n",
      "    \"milestone_end_lag\": \"Extracted Milestone End Lag\",\n",
      "    \"status\": \"Extracted Status\"\n",
      "  },\n",
      "  \"projects\": {\n",
      "    \"project_name\": \"Extracted Project Name\"\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE project_name LIKE 'Extracted Project Name' AND status = 'Extracted Status' ORDER BY milestone_end_lag DESC LIMIT 1;\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE project_name LIKE 'Extracted Project Name' AND status = 'Extracted Status' ORDER BY milestone_end_lag DESC LIMIT 1;\n",
      "Query Results:\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE project_name LIKE 'Extracted Project Name' AND status = 'Extracted Status' ORDER BY milestone_end_lag DESC LIMIT 1;\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the longest delayed milestone name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: ## Example Output:\n",
      "    {\n",
      "        \"entities\": {\n",
      "            \"projects\": {\"project_name\": \"Extracted Project Name\"},\n",
      "            \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"}\n",
      "        },\n",
      "        \"sql_query\": \"SELECT milestone_name FROM milestones WHERE project_name = 'Extracted Project Name' AND milestone_status = 'Delayed' ORDER BY duration DESC LIMIT 1\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "Response text: ## Example Output:\n",
      "    {\n",
      "        \"entities\": {\n",
      "            \"projects\": {\"project_name\": \"Extracted Project Name\"},\n",
      "            \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"}\n",
      "        },\n",
      "        \"sql_query\": \"SELECT milestone_name FROM milestones WHERE project_name = 'Extracted Project Name' AND milestone_status = 'Delayed' ORDER BY duration DESC LIMIT 1\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  give me the name of milestone which has delayed more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"},\n",
      "        \"tasks\": {\"status\": \"Delayed\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE status LIKE '%Delayed%' ORDER BY milestone_end_date DESC\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "Response text: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"},\n",
      "        \"tasks\": {\"status\": \"Delayed\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE status LIKE '%Delayed%' ORDER BY milestone_end_date DESC\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  give me the name of milestone which has delayed more \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE milestone_status = 'delayed' ORDER BY milestone_end_lag DESC\"\n",
      "    }\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "Response text: ## Example Output:\n",
      "    {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_name\": \"Extracted Milestone Name\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE milestone_status = 'delayed' ORDER BY milestone_end_lag DESC\"\n",
      "    }\n",
      "Generated SQL Query: None\n",
      "Error executing SQL query: can't execute an empty query\n",
      "No results returned or error occurred during query execution.\n",
      "Generated SQL Query: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  give me the name of milestone which has delayed in days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2376\\2937412725.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API Response: {\n",
      "      \"entities\": {\n",
      "        \"milestones\": {\"milestone_name\": \"Delayed Milestone\"}\n",
      "      },\n",
      "      \"sql_query\": \"SELECT milestone_name FROM milestones WHERE milestone_status = 'delayed' AND duration > 0\"\n",
      "    }\n",
      "Extracted Entities: {\n",
      "  \"milestones\": {\n",
      "    \"milestone_name\": \"Delayed Milestone\"\n",
      "  }\n",
      "}\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE milestone_status = 'delayed' AND duration > 0\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE milestone_status = 'delayed' AND duration > 0\n",
      "Query Results:\n",
      "Generated SQL Query: SELECT milestone_name FROM milestones WHERE milestone_status = 'delayed' AND duration > 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pinecone\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# OpenAI API key\n",
    "OPENAI_API_KEY = ''\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc\"\n",
    "PORT = 5432\n",
    "\n",
    "# Constants\n",
    "PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "NAMESPACE = \"projects\"  # Replace with your namespace\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Initialize Pinecone client\n",
    "def initialize_pinecone():\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    if INDEX_NAME not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=768,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "        )\n",
    "    return pc.Index(INDEX_NAME)\n",
    "\n",
    "# Load Hugging Face model for embeddings\n",
    "def load_huggingface_model():\n",
    "    return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "        \n",
    "# Function to fetch schema from PostgreSQL database\n",
    "def fetch_schema(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Function to process schema: remove special characters and convert to lowercase\n",
    "def process_schema(schema_df):\n",
    "    def clean_column_name(name):\n",
    "        return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "    schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "    return schema_df\n",
    "\n",
    "# Function to generate SQL Query\n",
    "def generate_sql_from_input(user_input, processed_schema_df):\n",
    "    \"\"\"\n",
    "    Extracts entities from the user input, understands user intent, and generates a corresponding SQL query.\n",
    "    \"\"\"\n",
    "    # Summarize the schema to only include table and column names\n",
    "    schema_summary = processed_schema_df[['table_name', 'column_name']].to_dict(orient='records')\n",
    "    schema_summary_str = json.dumps(schema_summary, indent=2)\n",
    "\n",
    "    # Chain of Thought reasoning prompt with summarized schema\n",
    "    cot_prompt = f\"\"\"\n",
    "    ## Database Schema Summary:\n",
    "    {schema_summary_str}\n",
    "\n",
    "    ## User Input:\n",
    "    \"{user_input}\"\n",
    "\n",
    "    ## Steps:\n",
    "    1. Extract relevant entities (project name, task name, milestone, etc.).\n",
    "    2. Map the entities to the schema columns.\n",
    "    3. Generate an SQL query using appropriate operators like `LIKE` for partial matches.\n",
    "\n",
    "    ## Example Output:\n",
    "    {{\n",
    "      \"entities\": {{\n",
    "        \"projects\": {{\"project_name\": \"Extracted Project Name\", \"status\": \"Extracted Status\"}},\n",
    "        \"tasks\": {{\"task_name\": \"Extracted Task Name\", \"owner\": \"Extracted Owner\"}}\n",
    "      }},\n",
    "      \"sql_query\": \"Generated SQL Query\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=cot_prompt,\n",
    "            max_tokens=500,  # Reduced token limit for completion\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        # Check if response is valid and has choices\n",
    "        if not response or not response.choices or len(response.choices) == 0:\n",
    "            print(f\"Empty response received from OpenAI: {response}\")\n",
    "            return None, None\n",
    "\n",
    "        result = response.choices[0].text.strip()\n",
    "        print(f\"Raw API Response: {result}\")  # Debugging output\n",
    "\n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            parsed_result = json.loads(result)\n",
    "        except json.JSONDecodeError as json_err:\n",
    "            print(f\"Error decoding JSON response: {json_err}\")\n",
    "            print(f\"Response text: {result}\")  # Print raw response for debugging\n",
    "            return None, None  # Handle the error gracefully\n",
    "\n",
    "        # Extract entities and SQL query\n",
    "        extracted_entities = parsed_result.get('entities')\n",
    "        sql_query = parsed_result.get('sql_query')\n",
    "\n",
    "        print(\"Extracted Entities:\", json.dumps(extracted_entities, indent=2))\n",
    "        print(\"Generated SQL Query:\", sql_query)\n",
    "\n",
    "        return extracted_entities, sql_query\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error processing request: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Initialize OpenAI Chat model\n",
    "openai_model = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate with the knowledge base included\n",
    "template = \"\"\"\n",
    "## Knowledge Base:\n",
    "{knowledge_base}\n",
    "\n",
    "## Database Schema:\n",
    "{database_schema}\n",
    "\n",
    "## Question:\n",
    "{question}\n",
    "\n",
    "## Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def get_answer_from_chatbot(question, database_schema):\n",
    "    try:\n",
    "        prompt = prompt_template.format(\n",
    "            knowledge_base=\"\",\n",
    "            database_schema=database_schema,\n",
    "            question=question\n",
    "        )\n",
    "        response = openai_model.invoke(input=prompt)\n",
    "        parsed_response = response.content.strip() if hasattr(response, 'content') else \"No response content found.\"\n",
    "        return parsed_response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response from OpenAI: {str(e)}\"\n",
    "\n",
    "# Function to execute the SQL query and print the results\n",
    "def execute_sql_query(conn, sql_query):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql_query)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Determine if user query is related to database or general knowledge\n",
    "def determine_query_type(user_query, schema_df, threshold = 75):\n",
    "    user_query_lower = user_query.lower()\n",
    "    \n",
    "    # Extract unique table and column names from the schema and convert to lowercase\n",
    "    table_names = schema_df['table_name'].str.lower().unique()\n",
    "    column_names = schema_df['column_name'].str.lower().unique()\n",
    "    \n",
    "    # Function to check fuzzy match\n",
    "    def is_fuzzy_match(query, options, threshold):\n",
    "        for option in options:\n",
    "            if fuzz.partial_ratio(query, option) >= threshold:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Check if user query matches any table or column name\n",
    "    if is_fuzzy_match(user_query_lower, table_names, threshold) or \\\n",
    "       is_fuzzy_match(user_query_lower, column_names, threshold):\n",
    "        return \"database\"\n",
    "    \n",
    "    return \"knowledge\"\n",
    "\n",
    "# Main function to handle user queries\n",
    "def process_user_query(user_query):\n",
    "    # Connect to the database and fetch the schema\n",
    "    conn = connect_to_db()\n",
    "    \n",
    "    schema_df = fetch_schema(conn)\n",
    "    processed_schema_df = process_schema(schema_df)\n",
    "    query_type = determine_query_type(user_query, schema_df)\n",
    "\n",
    "    if query_type == \"database\":\n",
    "        extracted_entities, sql_query = generate_sql_from_input(user_query, processed_schema_df)\n",
    "        \n",
    "        print(\"Generated SQL Query:\", sql_query)\n",
    "        \n",
    "        # Execute the generated SQL query\n",
    "        results = execute_sql_query(conn, sql_query)\n",
    "        conn.close()\n",
    "\n",
    "        if results is not None:\n",
    "            print(\"Query Results:\")\n",
    "            for row in results:\n",
    "                print(row)\n",
    "        else:\n",
    "            print(\"No results returned or error occurred during query execution.\")\n",
    "        \n",
    "        return f\"Generated SQL Query: {sql_query}\"\n",
    "    \n",
    "    else:\n",
    "        # For non-database related queries, respond using the chatbot\n",
    "        database_schema = fetch_schema(conn)  # Fetching schema again if needed\n",
    "        database_schema_df = process_schema(database_schema)\n",
    "        return get_answer_from_chatbot(user_query, database_schema_df.to_dict(orient='records'))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        response = process_user_query(user_input)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9698ecc-4b9f-4702-a658-234d899e89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the status of a project IIFL Samasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1800\\209691670.py:71: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 411\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m response \u001b[38;5;241m=\u001b[39m process_user_query(user_input)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[1], line 384\u001b[0m, in \u001b[0;36mprocess_user_query\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# # Extract features from user input using OpenAI's LLM\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# extracted_features = extract_entities(user_input, schema_df)\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# # Process the extracted features and clean them\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# cleaned_json = process_extracted_features(extracted_features)\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# # Extract entities from the user query\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# entities = extract_entities(user_query, processed_schema_df)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# Execute the SQL query and print the results\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     sql_query \u001b[38;5;241m=\u001b[39m generate_sql_from_input(user_query, processed_schema_df)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sql_query)\n\u001b[0;32m    386\u001b[0m     results \u001b[38;5;241m=\u001b[39m execute_sql_query(conn, sql_query)\n",
      "Cell \u001b[1;32mIn[1], line 260\u001b[0m, in \u001b[0;36mgenerate_sql_from_input\u001b[1;34m(user_input, processed_schema_df)\u001b[0m\n\u001b[0;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Parse the response\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m parsed_result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result)\n\u001b[0;32m    262\u001b[0m extracted_entities \u001b[38;5;241m=\u001b[39m parsed_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    263\u001b[0m sql_query \u001b[38;5;241m=\u001b[39m parsed_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql_query\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import pandas as pd\n",
    "# import psycopg2\n",
    "# import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# import pinecone\n",
    "# import openai\n",
    "# import os\n",
    "# import json\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "# # OpenAI API key\n",
    "# OPENAI_API_KEY = 'sk-proj-UnzdWuWBs7ZQRbRPiRCoT3BlbkFJhPM1p7DdZUMklcpnWK1S'\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"python_test_poc\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Constants\n",
    "# PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "# INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "# NAMESPACE = \"projects\"  # Replace with your namespace\n",
    "# MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# # Initialize Pinecone client\n",
    "# def initialize_pinecone():\n",
    "#     from pinecone import Pinecone, ServerlessSpec\n",
    "#     pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "#     if INDEX_NAME not in pc.list_indexes().names():\n",
    "#         pc.create_index(\n",
    "#             name=INDEX_NAME,\n",
    "#             dimension=768,\n",
    "#             metric='cosine',\n",
    "#             spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "#         )\n",
    "#     return pc.Index(INDEX_NAME)\n",
    "\n",
    "# # Load Hugging Face model for embeddings\n",
    "# def load_huggingface_model():\n",
    "#     return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "        \n",
    "# # Function to fetch schema from PostgreSQL database\n",
    "# def fetch_schema(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # Function to process schema: remove special characters and convert to lowercase\n",
    "# def process_schema(schema_df):\n",
    "#     def clean_column_name(name):\n",
    "#         return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "#     schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "#     return schema_df\n",
    "\n",
    "# # # Function to apply Chain of Thought reasoning to extract features (like project name, owner, etc.) using OpenAI's LLM\n",
    "# # def extract_entities(user_input, processed_schema_df):\n",
    "# #     schema_json = processed_schema_df.to_json(orient='records')\n",
    "    \n",
    "# #     prompt = f\"\"\"\n",
    "# #     ## Database Schema Context:\n",
    "# #     The following represents the columns and their respective tables available in the database:\n",
    "# #     {schema_json}\n",
    "\n",
    "# #     ## Chain of Thought Reasoning:\n",
    "# #     Step 1: Analyze the user input to understand the intent. The user might ask about specific tasks, projects, owners, dates, etc.\n",
    "# #     Step 2: Match these intent-specific words to the schema's column names and table names.\n",
    "# #     Step 3: Extract the relevant features (like project names, owners, dates, statuses, etc.) based on the schema.\n",
    "    \n",
    "# #     ## User Input:\n",
    "# #     The user has provided the following input: \"{user_input}\"\n",
    "\n",
    "# #     ## Task:\n",
    "# #     Extract the relevant features, values, and table names from the user input based on the schema using the above reasoning steps. \n",
    "# #     Output a JSON object where table names are keys, and within each table, include the fields and their values.\n",
    "\n",
    "# #     ## Instructions:\n",
    "# #     - Return a JSON dictionary that includes the table names as keys, and within each table, include the fields and their values extracted from the user input.\n",
    "# #     - Omit any fields or tables where the value is empty or null.\n",
    "# #     - Format the output as a JSON object with keys only for tables and fields that have values.\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     try:\n",
    "# #         response = openai.completions.create(\n",
    "# #             model=\"gpt-3.5-turbo-instruct\",\n",
    "# #             prompt=prompt,\n",
    "# #             max_tokens=500,\n",
    "# #             temperature=0.5\n",
    "# #         )\n",
    "# #         extracted_features = response.choices[0].text.strip()\n",
    "# #         print(\"OpenAI API Response:\", extracted_features)  # Debugging line\n",
    "# #         return json.loads(extracted_features)  # Parse JSON string into a dictionary\n",
    "# #     except openai.OpenAIError as e:\n",
    "# #         print(f\"Error with OpenAI: {e}\")\n",
    "# #         raise\n",
    "# #     except json.JSONDecodeError as e:\n",
    "# #         print(f\"Error decoding JSON response: {e}\")\n",
    "# #         raise\n",
    "\n",
    "# # # Function to remove null, None, empty values from JSON and list\n",
    "# # def clean_extracted_features(feature_dict):\n",
    "# #     # Remove any keys with None or empty values\n",
    "# #     cleaned_feature_dict = {k: v for k, v in feature_dict.items() if v}\n",
    "# #     return cleaned_feature_dict\n",
    "\n",
    "# # # Function to parse and process extracted features\n",
    "# # def process_extracted_features(extracted_features):\n",
    "# #     try:\n",
    "# #         # Remove the \"## Solution:\" part and any other non-JSON text\n",
    "# #         json_match = re.search(r'\\{.*\\}', extracted_features, re.DOTALL)\n",
    "\n",
    "# #         if json_match:\n",
    "# #             # Extract the JSON part from the matched result\n",
    "# #             cleaned_features = json_match.group(0).strip()\n",
    "\n",
    "# #             # Handle potential trailing commas or extra data issues\n",
    "# #             # Replace any trailing commas before closing braces\n",
    "# #             cleaned_features = re.sub(r',\\s*}', '}', cleaned_features)\n",
    "\n",
    "# #             # Convert JSON string to a Python dictionary\n",
    "# #             feature_dict = json.loads(cleaned_features)\n",
    "\n",
    "# #             # Clean feature dictionary\n",
    "# #             cleaned_feature_dict = clean_extracted_features(feature_dict)\n",
    "            \n",
    "\n",
    "# #             # Format the dictionary as per the requested format\n",
    "# #             formatted_dict = []\n",
    "# #             for table, fields in cleaned_feature_dict.items():\n",
    "# #                 # Convert the field values to a string, joining with commas\n",
    "# #                 field_values = ', '.join(fields.values())\n",
    "# #                 formatted_dict.append(f'{{ \"{table}\": \"{field_values}\" }}')\n",
    "\n",
    "# #             # Return cleaned and formatted dictionary in the requested output format\n",
    "# #             return ', '.join(formatted_dict)\n",
    "            \n",
    "# #         else:\n",
    "# #             return \"Error: No valid JSON found in the extracted features\"\n",
    "# #     except (json.JSONDecodeError, ValueError) as e:\n",
    "# #         print(f\"Error parsing features: {e}\")\n",
    "# #         return \"Error: Could not parse the features as JSON\"\n",
    "\n",
    "\n",
    "# # # Query Pinecone for relevant context and augment the input\n",
    "# # def query_pinecone_and_augment_input(user_input, entities, namespace):\n",
    "# #     embedding_model = load_huggingface_model()\n",
    "# #     pinecone_index = initialize_pinecone()\n",
    "# #     augmented_input = user_input\n",
    "# #     pinecone_data = {}\n",
    "# #     for entity_name, entity_value in entities.items():\n",
    "# #         if entity_value:\n",
    "# #             query_embedding = embedding_model.encode([entity_value])[0]\n",
    "# #             query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "# #             try:\n",
    "# #                 result = pinecone_index.query(\n",
    "# #                     namespace=namespace,\n",
    "# #                     vector=query_embedding.tolist(),\n",
    "# #                     top_k=3,\n",
    "# #                     include_values=True,\n",
    "# #                     include_metadata=True\n",
    "# #                 )\n",
    "# #                 matches = result.get('matches', [])\n",
    "# #                 if matches:\n",
    "# #                     unique_values = [match['metadata'].get('unique_value') for match in matches if 'metadata' in match]\n",
    "# #                     if unique_values:\n",
    "# #                         pinecone_data[entity_name] = unique_values\n",
    "# #                         if len(unique_values) > 1:\n",
    "# #                             print(f\"Multiple matches found for '{entity_value}':\")\n",
    "# #                             for idx, unique_value in enumerate(unique_values):\n",
    "# #                                 print(f\"{idx + 1}: {unique_value}\")\n",
    "# #                             while True:\n",
    "# #                                 selection = input(f\"Please select the most relevant option for '{entity_value}' (1-{len(unique_values)}): \")\n",
    "# #                                 try:\n",
    "# #                                     selected_value = unique_values[int(selection) - 1]\n",
    "# #                                     augmented_input = augmented_input.replace(entity_value, selected_value)\n",
    "# #                                     break\n",
    "# #                                 except (IndexError, ValueError):\n",
    "# #                                     print(\"Invalid selection. Please choose a valid option.\")\n",
    "# #                         else:\n",
    "# #                             augmented_input = augmented_input.replace(entity_value, unique_values[0])\n",
    "# #                 else:\n",
    "# #                     print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "# #             except Exception as e:\n",
    "# #                 print(f\"Error querying Pinecone: {str(e)}\")\n",
    "# #                 return f\"Error querying Pinecone: {str(e)}\", {}\n",
    "# #     return augmented_input, pinecone_data\n",
    "\n",
    "# # Function to generate SQL query using OpenAI API\n",
    "# def generate_sql_from_input(user_input, processed_schema_df):\n",
    "#     \"\"\"\n",
    "#     Extracts entities from the user input, understands user intent, and generates a corresponding SQL query.\n",
    "#     \"\"\"\n",
    "#     # Summarize the schema to only include table and column names\n",
    "#     schema_summary = processed_schema_df[['table_name', 'column_name']].to_dict(orient='records')\n",
    "#     schema_summary_str = json.dumps(schema_summary, indent=2)\n",
    "\n",
    "#     # Chain of Thought reasoning prompt with summarized schema\n",
    "#     cot_prompt = f\"\"\"\n",
    "#     ## Database Schema Summary:\n",
    "#     {schema_summary_str}\n",
    "\n",
    "#     ## User Input:\n",
    "#     \"{user_input}\"\n",
    "\n",
    "#     ## Steps:\n",
    "#     1. Extract relevant entities (project name, task name, milestone, etc.).\n",
    "#     2. Map the entities to the schema columns.\n",
    "#     3. Generate an SQL query using appropriate operators like `LIKE` for partial matches.\n",
    "\n",
    "#     ## Example Output:\n",
    "#     {{\n",
    "#       \"entities\": {{\n",
    "#         \"projects\": {{\"project_name\": \"Extracted Project Name\", \"status\": \"Extracted Status\"}},\n",
    "#         \"tasks\": {{\"task_name\": \"Extracted Task Name\", \"owner\": \"Extracted Owner\"}}\n",
    "#       }},\n",
    "#       \"sql_query\": \"Generated SQL Query\"\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         response = openai.completions.create(\n",
    "#             model=\"gpt-3.5-turbo-instruct\",\n",
    "#             prompt=cot_prompt,\n",
    "#             max_tokens=500,  # Reduced token limit for completion\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "#         result = response.choices[0].text.strip()\n",
    "\n",
    "#         # Parse the response\n",
    "#         parsed_result = json.loads(result)\n",
    "\n",
    "#         extracted_entities = parsed_result.get('entities')\n",
    "#         sql_query = parsed_result.get('sql_query')\n",
    "\n",
    "#         print(\"Extracted Entities:\", json.dumps(extracted_entities, indent=2))\n",
    "#         print(\"Generated SQL Query:\", sql_query)\n",
    "\n",
    "#         return extracted_entities, sql_query\n",
    "#     except openai.OpenAIError as e:\n",
    "#         print(f\"Error processing request: {e}\")\n",
    "#         raise\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"Error decoding JSON response: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Initialize OpenAI Chat model\n",
    "# openai_model = ChatOpenAI(\n",
    "#     openai_api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     temperature=0.7,\n",
    "#     max_tokens=150\n",
    "# )\n",
    "\n",
    "# # Create a ChatPromptTemplate with the knowledge base included\n",
    "# template = \"\"\"\n",
    "# ## Knowledge Base:\n",
    "# {knowledge_base}\n",
    "\n",
    "# ## Database Schema:\n",
    "# {database_schema}\n",
    "\n",
    "# ## Question:\n",
    "# {question}\n",
    "\n",
    "# ## Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# def get_answer_from_chatbot(question, database_schema):\n",
    "#     try:\n",
    "#         prompt = prompt_template.format(\n",
    "#             knowledge_base=\"\",\n",
    "#             database_schema=database_schema,\n",
    "#             question=question\n",
    "#         )\n",
    "#         response = openai_model.invoke(input=prompt)\n",
    "#         parsed_response = response.content.strip() if hasattr(response, 'content') else \"No response content found.\"\n",
    "#         return parsed_response\n",
    "#     except Exception as e:\n",
    "#         return f\"Error generating response from OpenAI: {str(e)}\"\n",
    "\n",
    "# # Function to execute the SQL query and print the results\n",
    "# def execute_sql_query(conn, sql_query):\n",
    "#     try:\n",
    "#         with conn.cursor() as cursor:\n",
    "#             cursor.execute(sql_query)\n",
    "#             results = cursor.fetchall()\n",
    "#             return results\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SQL query: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Determine if user query is related to database or general knowledge\n",
    "\n",
    "# def determine_query_type(user_query, schema_df, threshold = 75):\n",
    "#     user_query_lower = user_query.lower()\n",
    "    \n",
    "#     # Extract unique table and column names from the schema and convert to lowercase\n",
    "#     table_names = schema_df['table_name'].str.lower().unique()\n",
    "#     column_names = schema_df['column_name'].str.lower().unique()\n",
    "    \n",
    "#     # Function to check fuzzy match\n",
    "#     def is_fuzzy_match(query, options, threshold):\n",
    "#         for option in options:\n",
    "#             if fuzz.partial_ratio(query, option) >= threshold:\n",
    "#                 return True\n",
    "#         return False\n",
    "    \n",
    "#     # Check if user query matches any table or column name\n",
    "#     if is_fuzzy_match(user_query_lower, table_names, threshold) or \\\n",
    "#        is_fuzzy_match(user_query_lower, column_names, threshold):\n",
    "#         return \"database\"\n",
    "    \n",
    "#     return \"knowledge\"\n",
    "\n",
    "\n",
    "\n",
    "# # Main function to handle user queries\n",
    "# def process_user_query(user_query):\n",
    "#     # Connect to the database and fetch the schema\n",
    "#     conn = connect_to_db()\n",
    "    \n",
    "#     schema_df = fetch_schema(conn)\n",
    "#     processed_schema_df = process_schema(schema_df)\n",
    "#     query_type = determine_query_type(user_query, schema_df)\n",
    "\n",
    "#     # # Extract features from user input using OpenAI's LLM\n",
    "#     # extracted_features = extract_entities(user_input, schema_df)\n",
    "\n",
    "#     # # Process the extracted features and clean them\n",
    "#     # cleaned_json = process_extracted_features(extracted_features)\n",
    "\n",
    "#     if query_type == \"database\":\n",
    "#         # # Extract entities from the user query\n",
    "#         # entities = extract_entities(user_query, processed_schema_df)\n",
    "\n",
    "#         # if isinstance(cleaned_features_dict, dict):\n",
    "#         #     for table, fields in cleaned_features_dict.items():\n",
    "#         #         for field, entity_value in fields.items():\n",
    "#         #             if entity_value:\n",
    "#         #                 print(\"Entities found. Querying Pinecone for context.\")\n",
    "#         #                 augmented_query, pinecone_data = query_pinecone_and_augment_input(user_query, entities, NAMESPACE)\n",
    "#         #                 # Generate SQL query using the augmented query\n",
    "#         #                 processed_schema_df = process_schema(fetch_schema(conn))  # Process schema for better matching\n",
    "#         #                 sql_query = generate_sql_query(augmented_query, processed_schema_df)\n",
    "                        \n",
    "#         #             else: \n",
    "#         #                 print(\"No relevant entities found. Directly generating SQL query.\")\n",
    "#         #                 sql_query = generate_sql_query(user_query, processed_schema_df)\n",
    "#         #                 print(sql_query)      \n",
    "                \n",
    "#         # Execute the SQL query and print the results\n",
    "#         sql_query = generate_sql_from_input(user_query, processed_schema_df)\n",
    "#         print(sql_query)\n",
    "#         results = execute_sql_query(conn, sql_query)\n",
    "#         print(results)\n",
    "#         conn.close()\n",
    "\n",
    "#         if results is not None:\n",
    "#             print(\"Query Results:\")\n",
    "#             for row in results:\n",
    "#                 print(row)\n",
    "#         else:\n",
    "#             print(\"No results returned or error occurred during query execution.\")\n",
    "        \n",
    "#         return f\"Generated SQL Query: {sql_query}\"\n",
    "    \n",
    "#     else:\n",
    "#         # For non-database related queries, respond using the chatbot\n",
    "#         database_schema = fetch_schema(conn)  # Fetching schema again if needed\n",
    "#         database_schema_df = process_schema(database_schema)\n",
    "#         return get_answer_from_chatbot(user_query, database_schema_df.to_dict(orient='records'))\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         user_input = input(\"Enter your query: \")\n",
    "#         if user_input.lower() in ['exit', 'quit']:\n",
    "#             break\n",
    "#         response = process_user_query(user_input)\n",
    "#         print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba670d05-be00-4c58-a4bc-c06cc6b77428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd\n",
    "# import psycopg2\n",
    "# import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# import pinecone\n",
    "# import openai\n",
    "# import os\n",
    "# import json\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "# # OpenAI API key\n",
    "# OPENAI_API_KEY = 'sk-proj-UnzdWuWBs7ZQRbRPiRCoT3BlbkFJhPM1p7DdZUMklcpnWK1S'\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"python_test_poc\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Constants\n",
    "# PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "# INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "# NAMESPACE = \"projects\"  # Replace with your namespace\n",
    "# MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# # Initialize Pinecone client\n",
    "# def initialize_pinecone():\n",
    "#     from pinecone import Pinecone, ServerlessSpec\n",
    "#     pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "#     if INDEX_NAME not in pc.list_indexes().names():\n",
    "#         pc.create_index(\n",
    "#             name=INDEX_NAME,\n",
    "#             dimension=768,\n",
    "#             metric='cosine',\n",
    "#             spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "#         )\n",
    "#     return pc.Index(INDEX_NAME)\n",
    "\n",
    "# # Load Hugging Face model for embeddings\n",
    "# def load_huggingface_model():\n",
    "#     return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "        \n",
    "# # Function to fetch schema from PostgreSQL database\n",
    "# def fetch_schema(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # Function to process schema: remove special characters and convert to lowercase\n",
    "# def process_schema(schema_df):\n",
    "#     def clean_column_name(name):\n",
    "#         return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "#     schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "#     return schema_df\n",
    "\n",
    "# # # Function to apply Chain of Thought reasoning to extract features (like project name, owner, etc.) using OpenAI's LLM\n",
    "# # def extract_entities(user_input, processed_schema_df):\n",
    "# #     schema_json = processed_schema_df.to_json(orient='records')\n",
    "    \n",
    "# #     prompt = f\"\"\"\n",
    "# #     ## Database Schema Context:\n",
    "# #     The following represents the columns and their respective tables available in the database:\n",
    "# #     {schema_json}\n",
    "\n",
    "# #     ## Chain of Thought Reasoning:\n",
    "# #     Step 1: Analyze the user input to understand the intent. The user might ask about specific tasks, projects, owners, dates, etc.\n",
    "# #     Step 2: Match these intent-specific words to the schema's column names and table names.\n",
    "# #     Step 3: Extract the relevant features (like project names, owners, dates, statuses, etc.) based on the schema.\n",
    "    \n",
    "# #     ## User Input:\n",
    "# #     The user has provided the following input: \"{user_input}\"\n",
    "\n",
    "# #     ## Task:\n",
    "# #     Extract the relevant features, values, and table names from the user input based on the schema using the above reasoning steps. \n",
    "# #     Output a JSON object where table names are keys, and within each table, include the fields and their values.\n",
    "\n",
    "# #     ## Instructions:\n",
    "# #     - Return a JSON dictionary that includes the table names as keys, and within each table, include the fields and their values extracted from the user input.\n",
    "# #     - Omit any fields or tables where the value is empty or null.\n",
    "# #     - Format the output as a JSON object with keys only for tables and fields that have values.\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     try:\n",
    "# #         response = openai.completions.create(\n",
    "# #             model=\"gpt-3.5-turbo-instruct\",\n",
    "# #             prompt=prompt,\n",
    "# #             max_tokens=500,\n",
    "# #             temperature=0.5\n",
    "# #         )\n",
    "# #         extracted_features = response.choices[0].text.strip()\n",
    "# #         print(\"OpenAI API Response:\", extracted_features)  # Debugging line\n",
    "# #         return json.loads(extracted_features)  # Parse JSON string into a dictionary\n",
    "# #     except openai.OpenAIError as e:\n",
    "# #         print(f\"Error with OpenAI: {e}\")\n",
    "# #         raise\n",
    "# #     except json.JSONDecodeError as e:\n",
    "# #         print(f\"Error decoding JSON response: {e}\")\n",
    "# #         raise\n",
    "\n",
    "# # # Function to remove null, None, empty values from JSON and list\n",
    "# # def clean_extracted_features(feature_dict):\n",
    "# #     # Remove any keys with None or empty values\n",
    "# #     cleaned_feature_dict = {k: v for k, v in feature_dict.items() if v}\n",
    "# #     return cleaned_feature_dict\n",
    "\n",
    "# # # Function to parse and process extracted features\n",
    "# # def process_extracted_features(extracted_features):\n",
    "# #     try:\n",
    "# #         # Remove the \"## Solution:\" part and any other non-JSON text\n",
    "# #         json_match = re.search(r'\\{.*\\}', extracted_features, re.DOTALL)\n",
    "\n",
    "# #         if json_match:\n",
    "# #             # Extract the JSON part from the matched result\n",
    "# #             cleaned_features = json_match.group(0).strip()\n",
    "\n",
    "# #             # Handle potential trailing commas or extra data issues\n",
    "# #             # Replace any trailing commas before closing braces\n",
    "# #             cleaned_features = re.sub(r',\\s*}', '}', cleaned_features)\n",
    "\n",
    "# #             # Convert JSON string to a Python dictionary\n",
    "# #             feature_dict = json.loads(cleaned_features)\n",
    "\n",
    "# #             # Clean feature dictionary\n",
    "# #             cleaned_feature_dict = clean_extracted_features(feature_dict)\n",
    "            \n",
    "\n",
    "# #             # Format the dictionary as per the requested format\n",
    "# #             formatted_dict = []\n",
    "# #             for table, fields in cleaned_feature_dict.items():\n",
    "# #                 # Convert the field values to a string, joining with commas\n",
    "# #                 field_values = ', '.join(fields.values())\n",
    "# #                 formatted_dict.append(f'{{ \"{table}\": \"{field_values}\" }}')\n",
    "\n",
    "# #             # Return cleaned and formatted dictionary in the requested output format\n",
    "# #             return ', '.join(formatted_dict)\n",
    "            \n",
    "# #         else:\n",
    "# #             return \"Error: No valid JSON found in the extracted features\"\n",
    "# #     except (json.JSONDecodeError, ValueError) as e:\n",
    "# #         print(f\"Error parsing features: {e}\")\n",
    "# #         return \"Error: Could not parse the features as JSON\"\n",
    "\n",
    "\n",
    "# # # Query Pinecone for relevant context and augment the input\n",
    "# # def query_pinecone_and_augment_input(user_input, entities, namespace):\n",
    "# #     embedding_model = load_huggingface_model()\n",
    "# #     pinecone_index = initialize_pinecone()\n",
    "# #     augmented_input = user_input\n",
    "# #     pinecone_data = {}\n",
    "# #     for entity_name, entity_value in entities.items():\n",
    "# #         if entity_value:\n",
    "# #             query_embedding = embedding_model.encode([entity_value])[0]\n",
    "# #             query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "# #             try:\n",
    "# #                 result = pinecone_index.query(\n",
    "# #                     namespace=namespace,\n",
    "# #                     vector=query_embedding.tolist(),\n",
    "# #                     top_k=3,\n",
    "# #                     include_values=True,\n",
    "# #                     include_metadata=True\n",
    "# #                 )\n",
    "# #                 matches = result.get('matches', [])\n",
    "# #                 if matches:\n",
    "# #                     unique_values = [match['metadata'].get('unique_value') for match in matches if 'metadata' in match]\n",
    "# #                     if unique_values:\n",
    "# #                         pinecone_data[entity_name] = unique_values\n",
    "# #                         if len(unique_values) > 1:\n",
    "# #                             print(f\"Multiple matches found for '{entity_value}':\")\n",
    "# #                             for idx, unique_value in enumerate(unique_values):\n",
    "# #                                 print(f\"{idx + 1}: {unique_value}\")\n",
    "# #                             while True:\n",
    "# #                                 selection = input(f\"Please select the most relevant option for '{entity_value}' (1-{len(unique_values)}): \")\n",
    "# #                                 try:\n",
    "# #                                     selected_value = unique_values[int(selection) - 1]\n",
    "# #                                     augmented_input = augmented_input.replace(entity_value, selected_value)\n",
    "# #                                     break\n",
    "# #                                 except (IndexError, ValueError):\n",
    "# #                                     print(\"Invalid selection. Please choose a valid option.\")\n",
    "# #                         else:\n",
    "# #                             augmented_input = augmented_input.replace(entity_value, unique_values[0])\n",
    "# #                 else:\n",
    "# #                     print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "# #             except Exception as e:\n",
    "# #                 print(f\"Error querying Pinecone: {str(e)}\")\n",
    "# #                 return f\"Error querying Pinecone: {str(e)}\", {}\n",
    "# #     return augmented_input, pinecone_data\n",
    "\n",
    "# # Function to generate SQL query using OpenAI API\n",
    "# def generate_sql_query(user_input, processed_schema_df):\n",
    "#     schema_json = processed_schema_df.to_json(orient='records')\n",
    "#     schema_with_types = processed_schema_df[['table_name', 'column_name', 'data_type']].to_dict(orient='records')\n",
    "#     context = f\"\"\"\n",
    "#     ## Database Schema Context\n",
    "#     Schema JSON: {schema_json}\n",
    "#     Detailed Schema: {schema_with_types}\n",
    "\n",
    "#     ## User Input\n",
    "#     Given the following user input: '{user_input}', generate an SQL query.\n",
    "#     Use the LIKE operator for partial matches where appropriate. Handle data type mismatches explicitly.\n",
    "\n",
    "#     ## Instructions\n",
    "#     Based on the user input and the provided schema, generate an accurate SQL query.\n",
    "#     Ensure the query maps correctly to the tables and columns in the database.\n",
    "#     Handle data type casting if necessary to match columns with different types.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response = openai.completions.create(\n",
    "#             model=\"gpt-3.5-turbo-instruct\",\n",
    "#             prompt=context,\n",
    "#             max_tokens=500,\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "#         generated_query = response.choices[0].text.strip()\n",
    "#         if generated_query.lower().startswith(\"the generated sql query is:\"):\n",
    "#             generated_query = generated_query[len(\"The generated SQL query is:\"):].strip()\n",
    "#         return generated_query\n",
    "#     except openai.OpenAIError as e:\n",
    "#         print(f\"Error generating SQL query: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # Initialize OpenAI Chat model\n",
    "# openai_model = ChatOpenAI(\n",
    "#     openai_api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     temperature=0.7,\n",
    "#     max_tokens=150\n",
    "# )\n",
    "\n",
    "# # Create a ChatPromptTemplate with the knowledge base included\n",
    "# template = \"\"\"\n",
    "# ## Knowledge Base:\n",
    "# {knowledge_base}\n",
    "\n",
    "# ## Database Schema:\n",
    "# {database_schema}\n",
    "\n",
    "# ## Question:\n",
    "# {question}\n",
    "\n",
    "# ## Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# def get_answer_from_chatbot(question, database_schema):\n",
    "#     try:\n",
    "#         prompt = prompt_template.format(\n",
    "#             knowledge_base=\"\",\n",
    "#             database_schema=database_schema,\n",
    "#             question=question\n",
    "#         )\n",
    "#         response = openai_model.invoke(input=prompt)\n",
    "#         parsed_response = response.content.strip() if hasattr(response, 'content') else \"No response content found.\"\n",
    "#         return parsed_response\n",
    "#     except Exception as e:\n",
    "#         return f\"Error generating response from OpenAI: {str(e)}\"\n",
    "\n",
    "# # Function to execute the SQL query and print the results\n",
    "# def execute_sql_query(conn, sql_query):\n",
    "#     try:\n",
    "#         with conn.cursor() as cursor:\n",
    "#             cursor.execute(sql_query)\n",
    "#             results = cursor.fetchall()\n",
    "#             return results\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SQL query: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Determine if user query is related to database or general knowledge\n",
    "\n",
    "# def determine_query_type(user_query, schema_df, threshold = 75):\n",
    "#     user_query_lower = user_query.lower()\n",
    "    \n",
    "#     # Extract unique table and column names from the schema and convert to lowercase\n",
    "#     table_names = schema_df['table_name'].str.lower().unique()\n",
    "#     column_names = schema_df['column_name'].str.lower().unique()\n",
    "    \n",
    "#     # Function to check fuzzy match\n",
    "#     def is_fuzzy_match(query, options, threshold):\n",
    "#         for option in options:\n",
    "#             if fuzz.partial_ratio(query, option) >= threshold:\n",
    "#                 return True\n",
    "#         return False\n",
    "    \n",
    "#     # Check if user query matches any table or column name\n",
    "#     if is_fuzzy_match(user_query_lower, table_names, threshold) or \\\n",
    "#        is_fuzzy_match(user_query_lower, column_names, threshold):\n",
    "#         return \"database\"\n",
    "    \n",
    "#     return \"knowledge\"\n",
    "\n",
    "\n",
    "\n",
    "# # Main function to handle user queries\n",
    "# def process_user_query(user_query):\n",
    "#     # Connect to the database and fetch the schema\n",
    "#     conn = connect_to_db()\n",
    "    \n",
    "#     schema_df = fetch_schema(conn)\n",
    "#     processed_schema_df = process_schema(schema_df)\n",
    "#     query_type = determine_query_type(user_query, schema_df)\n",
    "\n",
    "#     # # Extract features from user input using OpenAI's LLM\n",
    "#     # extracted_features = extract_entities(user_input, schema_df)\n",
    "\n",
    "#     # # Process the extracted features and clean them\n",
    "#     # cleaned_json = process_extracted_features(extracted_features)\n",
    "\n",
    "#     if query_type == \"database\":\n",
    "#         # # Extract entities from the user query\n",
    "#         # entities = extract_entities(user_query, processed_schema_df)\n",
    "\n",
    "#         # if isinstance(cleaned_features_dict, dict):\n",
    "#         #     for table, fields in cleaned_features_dict.items():\n",
    "#         #         for field, entity_value in fields.items():\n",
    "#         #             if entity_value:\n",
    "#         #                 print(\"Entities found. Querying Pinecone for context.\")\n",
    "#         #                 augmented_query, pinecone_data = query_pinecone_and_augment_input(user_query, entities, NAMESPACE)\n",
    "#         #                 # Generate SQL query using the augmented query\n",
    "#         #                 processed_schema_df = process_schema(fetch_schema(conn))  # Process schema for better matching\n",
    "#         #                 sql_query = generate_sql_query(augmented_query, processed_schema_df)\n",
    "                        \n",
    "#         #             else: \n",
    "#         #                 print(\"No relevant entities found. Directly generating SQL query.\")\n",
    "#         #                 sql_query = generate_sql_query(user_query, processed_schema_df)\n",
    "#         #                 print(sql_query)      \n",
    "                \n",
    "#         # Execute the SQL query and print the results\n",
    "#         sql_query = generate_sql_query(user_query, processed_schema_df)\n",
    "#         print(sql_query)\n",
    "#         results = execute_sql_query(conn, sql_query)\n",
    "#         print(results)\n",
    "#         conn.close()\n",
    "\n",
    "#         if results is not None:\n",
    "#             print(\"Query Results:\")\n",
    "#             for row in results:\n",
    "#                 print(row)\n",
    "#         else:\n",
    "#             print(\"No results returned or error occurred during query execution.\")\n",
    "        \n",
    "#         return f\"Generated SQL Query: {sql_query}\"\n",
    "    \n",
    "#     else:\n",
    "#         # For non-database related queries, respond using the chatbot\n",
    "#         database_schema = fetch_schema(conn)  # Fetching schema again if needed\n",
    "#         database_schema_df = process_schema(database_schema)\n",
    "#         return get_answer_from_chatbot(user_query, database_schema_df.to_dict(orient='records'))\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         user_input = input(\"Enter your query: \")\n",
    "#         if user_input.lower() in ['exit', 'quit']:\n",
    "#             break\n",
    "#         response = process_user_query(user_input)\n",
    "#         print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74888a1a-3c53-4fbb-bf5a-6cfa1ba98d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Smart Desk Code\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import psycopg2\n",
    "# import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# import pinecone\n",
    "# import openai\n",
    "# import os\n",
    "\n",
    "# # OpenAI API key\n",
    "# OPENAI_API_KEY = 'sk-proj-UnzdWuWBs7ZQRbRPiRCoT3BlbkFJhPM1p7DdZUMklcpnWK1S'\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"python_test_poc\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Constants\n",
    "# PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "# INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "# NAMESPACE = \"projects\"  # Replace with your namespace\n",
    "# MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# # Initialize Pinecone client\n",
    "# def initialize_pinecone():\n",
    "#     from pinecone import Pinecone, ServerlessSpec\n",
    "#     pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "#     if INDEX_NAME not in pc.list_indexes().names():\n",
    "#         pc.create_index(\n",
    "#             name=INDEX_NAME,\n",
    "#             dimension=768,\n",
    "#             metric='cosine',\n",
    "#             spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "#         )\n",
    "#     return pc.Index(INDEX_NAME)\n",
    "\n",
    "# # Load Hugging Face model for embeddings\n",
    "# def load_huggingface_model():\n",
    "#     return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "        \n",
    "# # Function to fetch schema from PostgreSQL database\n",
    "# def fetch_schema(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name, data_type\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # Function to process schema: remove special characters and convert to lowercase\n",
    "# def process_schema(schema_df):\n",
    "#     def clean_column_name(name):\n",
    "#         return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "#     schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "#     return schema_df\n",
    "\n",
    "# # Extract relevant entities based on regex and column names\n",
    "# def extract_entities(user_query, schema):\n",
    "#     entities = {\n",
    "#         'project_name': None,\n",
    "#         'owner': None\n",
    "#     }\n",
    "#     project_pattern = re.compile(r'project\\s+([a-zA-Z0-9_ ]+)', re.IGNORECASE)\n",
    "#     owner_pattern = re.compile(r'owner\\s+of\\s+project\\s+([a-zA-Z0-9_ ]+)', re.IGNORECASE)\n",
    "#     project_match = project_pattern.search(user_query)\n",
    "#     if project_match:\n",
    "#         entities['project_name'] = project_match.group(1).strip()\n",
    "#     owner_match = owner_pattern.search(user_query)\n",
    "#     if owner_match:\n",
    "#         entities['owner'] = owner_match.group(1).strip()\n",
    "#     return entities\n",
    "\n",
    "# # Query Pinecone for relevant context and augment the input\n",
    "# def query_pinecone_and_augment_input(user_input, entities, namespace):\n",
    "#     embedding_model = load_huggingface_model()\n",
    "#     pinecone_index = initialize_pinecone()\n",
    "#     augmented_input = user_input\n",
    "#     pinecone_data = {}\n",
    "#     for entity_name, entity_value in entities.items():\n",
    "#         if entity_value:\n",
    "#             query_embedding = embedding_model.encode([entity_value])[0]\n",
    "#             query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "#             try:\n",
    "#                 result = pinecone_index.query(\n",
    "#                     namespace=namespace,\n",
    "#                     vector=query_embedding.tolist(),\n",
    "#                     top_k=3,\n",
    "#                     include_values=True,\n",
    "#                     include_metadata=True\n",
    "#                 )\n",
    "#                 matches = result.get('matches', [])\n",
    "#                 if matches:\n",
    "#                     unique_values = [match['metadata'].get('unique_value') for match in matches if 'metadata' in match]\n",
    "#                     if unique_values:\n",
    "#                         pinecone_data[entity_name] = unique_values\n",
    "#                         if len(unique_values) > 1:\n",
    "#                             print(f\"Multiple matches found for '{entity_value}':\")\n",
    "#                             for idx, unique_value in enumerate(unique_values):\n",
    "#                                 print(f\"{idx + 1}: {unique_value}\")\n",
    "#                             while True:\n",
    "#                                 selection = input(f\"Please select the most relevant option for '{entity_value}' (1-{len(unique_values)}): \")\n",
    "#                                 try:\n",
    "#                                     selected_value = unique_values[int(selection) - 1]\n",
    "#                                     augmented_input = augmented_input.replace(entity_value, selected_value)\n",
    "#                                     break\n",
    "#                                 except (IndexError, ValueError):\n",
    "#                                     print(\"Invalid selection. Please choose a valid option.\")\n",
    "#                         else:\n",
    "#                             augmented_input = augmented_input.replace(entity_value, unique_values[0])\n",
    "#                 else:\n",
    "#                     print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error querying Pinecone: {str(e)}\")\n",
    "#                 return f\"Error querying Pinecone: {str(e)}\", {}\n",
    "#     return augmented_input, pinecone_data\n",
    "\n",
    "# def generate_sql_query(user_input, processed_schema_df):\n",
    "#     schema_json = processed_schema_df.to_json(orient='records')\n",
    "#     schema_with_types = processed_schema_df[['table_name', 'column_name']].to_dict(orient='records')  # Removed 'data_type'\n",
    "    \n",
    "#     context = f\"\"\"\n",
    "#     ## Database Schema Context\n",
    "#     Schema JSON: {schema_json}\n",
    "#     Detailed Schema: {schema_with_types}\n",
    "\n",
    "#     ## User Input\n",
    "#     Given the following user input: '{user_input}', generate an SQL query.\n",
    "#     Use the LIKE operator for partial matches where appropriate. Handle data type mismatches explicitly.\n",
    "\n",
    "#     ## Instructions\n",
    "#     Based on the user input and the provided schema, generate an accurate SQL query.\n",
    "#     Ensure the query maps correctly to the tables and columns in the database.\n",
    "#     Handle data type casting if necessary to match columns with different types.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response = openai.completions.create(\n",
    "#             model=\"gpt-3.5-turbo-instruct\",\n",
    "#             prompt=context,\n",
    "#             max_tokens=500,\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "#         generated_query = response.choices[0].text.strip()\n",
    "#         if generated_query.lower().startswith(\"the generated sql query is:\"):\n",
    "#             generated_query = generated_query[len(\"The generated SQL query is:\"):].strip()\n",
    "#         return generated_query\n",
    "#     except openai.OpenAIError as e:\n",
    "#         print(f\"Error generating SQL query: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# # Initialize OpenAI Chat model\n",
    "# openai_model = ChatOpenAI(\n",
    "#     openai_api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     temperature=0.7,\n",
    "#     max_tokens=150\n",
    "# )\n",
    "\n",
    "# # Create a ChatPromptTemplate with the knowledge base included\n",
    "# template = \"\"\"\n",
    "# ## Knowledge Base:\n",
    "# {knowledge_base}\n",
    "\n",
    "# ## Database Schema:\n",
    "# {database_schema}\n",
    "\n",
    "# ## Question:\n",
    "# {question}\n",
    "\n",
    "# ## Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# def get_answer_from_chatbot(question, database_schema):\n",
    "#     try:\n",
    "#         prompt = prompt_template.format(\n",
    "#             knowledge_base=\"\",\n",
    "#             database_schema=database_schema,\n",
    "#             question=question\n",
    "#         )\n",
    "#         response = openai_model.invoke(input=prompt)\n",
    "#         parsed_response = response.content.strip() if hasattr(response, 'content') else \"No response content found.\"\n",
    "#         return parsed_response\n",
    "#     except Exception as e:\n",
    "#         return f\"Error generating response from OpenAI: {str(e)}\"\n",
    "\n",
    "# # Function to execute the SQL query and print the results\n",
    "# def execute_sql_query(conn, sql_query):\n",
    "#     try:\n",
    "#         with conn.cursor() as cursor:\n",
    "#             cursor.execute(sql_query)\n",
    "#             results = cursor.fetchall()\n",
    "#             return results\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SQL query: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Determine if user query is related to database or general knowledge\n",
    "# def determine_query_type(user_query, schema_df):\n",
    "#     user_query_lower = user_query.lower()\n",
    "    \n",
    "#     if any(table.lower() in user_query_lower for table in schema_df['table_name'].unique()) or \\\n",
    "#        any(column.lower() in user_query_lower for column in schema_df['column_name'].unique()):\n",
    "#         return \"database\"\n",
    "    \n",
    "#     return \"knowledge\"\n",
    "\n",
    "\n",
    "# # Main function to handle user queries\n",
    "# def process_user_query(user_query):\n",
    "#     # Connect to the database and fetch the schema\n",
    "    \n",
    "#     conn = connect_to_db()\n",
    "    \n",
    "#     schema_df = fetch_schema(conn)\n",
    "#     processed_schema_df = process_schema(schema_df)\n",
    "#     query_type = determine_query_type(user_query,schema_df)\n",
    "    \n",
    "#     query_type = determine_query_type(user_query, schema_df)\n",
    "    \n",
    "#     if query_type == \"database\":\n",
    "#         database_schema = fetch_schema(conn)  # You can use the schema_df here if you prefer\n",
    "        \n",
    "#         entities = extract_entities(user_query, database_schema)\n",
    "#         augmented_query, pinecone_data = query_pinecone_and_augment_input(user_query, entities, NAMESPACE)\n",
    "        \n",
    "#         processed_schema_df = process_schema(database_schema)  # Process schema for better matching\n",
    "#         sql_query = generate_sql_query(augmented_query, processed_schema_df)\n",
    "        \n",
    "#         results = execute_sql_query(conn, sql_query)\n",
    "#         conn.close()\n",
    "        \n",
    "#         if results is not None:\n",
    "#             print(\"Query Results:\")\n",
    "#             for row in results:\n",
    "#                 print(row)\n",
    "#         else:\n",
    "#             print(\"No results returned or error occurred during query execution.\")\n",
    "        \n",
    "#         return f\"Generated SQL Query: {sql_query}\"\n",
    "    \n",
    "#     else:\n",
    "#         database_schema = fetch_schema(conn)  # Fetching schema again if needed\n",
    "#         database_schema_df = process_schema(database_schema)\n",
    "#         return get_answer_from_chatbot(user_query, database_schema_df.to_dict(orient='records'))\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         user_input = input(\"Enter your query: \")\n",
    "#         if user_input.lower() in ['exit', 'quit']:\n",
    "#             break\n",
    "#         response = process_user_query(user_input)\n",
    "#         print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1cfa1-970e-49fb-91c3-38c34d061f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is the status of a project IIFL Samasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1800\\62121427.py:69: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1800\\62121427.py:69: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple matches found for 'IIFL Samasta':\n",
      "1: IIFl Samasta CPL CR\n",
      "2: IIFL Samasta - CGRM\n",
      "3: IIFL SAMASTA - RPA BOT\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please select the most relevant option for 'IIFL Samasta' (1-3):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Results:\n",
      "('Completed',)\n",
      "('Completed',)\n",
      "Generated SQL Query: SELECT status FROM projects WHERE project_name LIKE '%IIFL SAMASTA - RPA BOT%';\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pinecone\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# OpenAI API key\n",
    "OPENAI_API_KEY = 'sk-proj-UnzdWuWBs7ZQRbRPiRCoT3BlbkFJhPM1p7DdZUMklcpnWK1S'\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc\"\n",
    "PORT = 5432\n",
    "\n",
    "# Constants\n",
    "PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "NAMESPACE = \"projects\"  # Replace with your namespace\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Initialize Pinecone client\n",
    "def initialize_pinecone():\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    if INDEX_NAME not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=768,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "        )\n",
    "    return pc.Index(INDEX_NAME)\n",
    "\n",
    "# Load Hugging Face model for embeddings\n",
    "def load_huggingface_model():\n",
    "    return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "        \n",
    "# Function to fetch schema from PostgreSQL database\n",
    "def fetch_schema(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Function to process schema: remove special characters and convert to lowercase\n",
    "def process_schema(schema_df):\n",
    "    def clean_column_name(name):\n",
    "        return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "    schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "    return schema_df\n",
    "\n",
    "# Extract relevant entities based on regex and column names\n",
    "def extract_entities(user_query, schema):\n",
    "    entities = {\n",
    "        'project_name': None,\n",
    "        'owner': None\n",
    "    }\n",
    "    project_pattern = re.compile(r'project\\s+([a-zA-Z0-9_ ]+)', re.IGNORECASE)\n",
    "    owner_pattern = re.compile(r'owner\\s+of\\s+project\\s+([a-zA-Z0-9_ ]+)', re.IGNORECASE)\n",
    "    project_match = project_pattern.search(user_query)\n",
    "    if project_match:\n",
    "        entities['project_name'] = project_match.group(1).strip()\n",
    "    owner_match = owner_pattern.search(user_query)\n",
    "    if owner_match:\n",
    "        entities['owner'] = owner_match.group(1).strip()\n",
    "    return entities\n",
    "\n",
    "# Query Pinecone for relevant context and augment the input\n",
    "def query_pinecone_and_augment_input(user_input, entities, namespace):\n",
    "    embedding_model = load_huggingface_model()\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    augmented_input = user_input\n",
    "    pinecone_data = {}\n",
    "    for entity_name, entity_value in entities.items():\n",
    "        if entity_value:\n",
    "            query_embedding = embedding_model.encode([entity_value])[0]\n",
    "            query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "            try:\n",
    "                result = pinecone_index.query(\n",
    "                    namespace=namespace,\n",
    "                    vector=query_embedding.tolist(),\n",
    "                    top_k=3,\n",
    "                    include_values=True,\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                matches = result.get('matches', [])\n",
    "                if matches:\n",
    "                    unique_values = [match['metadata'].get('unique_value') for match in matches if 'metadata' in match]\n",
    "                    if unique_values:\n",
    "                        pinecone_data[entity_name] = unique_values\n",
    "                        if len(unique_values) > 1:\n",
    "                            print(f\"Multiple matches found for '{entity_value}':\")\n",
    "                            for idx, unique_value in enumerate(unique_values):\n",
    "                                print(f\"{idx + 1}: {unique_value}\")\n",
    "                            while True:\n",
    "                                selection = input(f\"Please select the most relevant option for '{entity_value}' (1-{len(unique_values)}): \")\n",
    "                                try:\n",
    "                                    selected_value = unique_values[int(selection) - 1]\n",
    "                                    augmented_input = augmented_input.replace(entity_value, selected_value)\n",
    "                                    break\n",
    "                                except (IndexError, ValueError):\n",
    "                                    print(\"Invalid selection. Please choose a valid option.\")\n",
    "                        else:\n",
    "                            augmented_input = augmented_input.replace(entity_value, unique_values[0])\n",
    "                else:\n",
    "                    print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error querying Pinecone: {str(e)}\")\n",
    "                return f\"Error querying Pinecone: {str(e)}\", {}\n",
    "    return augmented_input, pinecone_data\n",
    "\n",
    "def generate_sql_query(user_input, processed_schema_df):\n",
    "    schema_json = processed_schema_df.to_json(orient='records')\n",
    "    schema_with_types = processed_schema_df[['table_name', 'column_name']].to_dict(orient='records')  # Removed 'data_type'\n",
    "    \n",
    "    context = f\"\"\"\n",
    "    ## Database Schema Context\n",
    "    Schema JSON: {schema_json}\n",
    "    Detailed Schema: {schema_with_types}\n",
    "\n",
    "    ## User Input\n",
    "    Given the following user input: '{user_input}', generate an SQL query.\n",
    "    Use the LIKE operator for partial matches where appropriate. Handle data type mismatches explicitly.\n",
    "\n",
    "    ## Instructions\n",
    "    Based on the user input and the provided schema, generate an accurate SQL query.\n",
    "    Ensure the query maps correctly to the tables and columns in the database.\n",
    "    Handle data type casting if necessary to match columns with different types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=context,\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        generated_query = response.choices[0].text.strip()\n",
    "        if generated_query.lower().startswith(\"the generated sql query is:\"):\n",
    "            generated_query = generated_query[len(\"The generated SQL query is:\"):].strip()\n",
    "        return generated_query\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error generating SQL query: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Initialize OpenAI Chat model\n",
    "openai_model = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate with the knowledge base included\n",
    "template = \"\"\"\n",
    "## Knowledge Base:\n",
    "{knowledge_base}\n",
    "\n",
    "## Database Schema:\n",
    "{database_schema}\n",
    "\n",
    "## Question:\n",
    "{question}\n",
    "\n",
    "## Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def get_answer_from_chatbot(question, database_schema):\n",
    "    try:\n",
    "        prompt = prompt_template.format(\n",
    "            knowledge_base=\"\",\n",
    "            database_schema=database_schema,\n",
    "            question=question\n",
    "        )\n",
    "        response = openai_model.invoke(input=prompt)\n",
    "        parsed_response = response.content.strip() if hasattr(response, 'content') else \"No response content found.\"\n",
    "        return parsed_response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response from OpenAI: {str(e)}\"\n",
    "\n",
    "# Function to execute the SQL query and print the results\n",
    "def execute_sql_query(conn, sql_query):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql_query)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Determine if user query is related to database or general knowledge\n",
    "def determine_query_type(user_query, schema_df):\n",
    "    user_query_lower = user_query.lower()\n",
    "    \n",
    "    if any(table.lower() in user_query_lower for table in schema_df['table_name'].unique()) or \\\n",
    "       any(column.lower() in user_query_lower for column in schema_df['column_name'].unique()):\n",
    "        return \"database\"\n",
    "    \n",
    "    return \"knowledge\"\n",
    "\n",
    "\n",
    "# Main function to handle user queries\n",
    "def process_user_query(user_query):\n",
    "    # Connect to the database and fetch the schema\n",
    "    conn = connect_to_db()\n",
    "    schema_df = fetch_schema(conn)\n",
    "    processed_schema_df = process_schema(schema_df)\n",
    "   \n",
    "    \n",
    "    query_type = determine_query_type(user_query, schema_df)\n",
    "    \n",
    "    if query_type == \"database\":\n",
    "        database_schema = fetch_schema(conn)  # You can use the schema_df here if you prefer\n",
    "        \n",
    "        entities = extract_entities(user_query, database_schema)\n",
    "        augmented_query, pinecone_data = query_pinecone_and_augment_input(user_query, entities, NAMESPACE)\n",
    "        \n",
    "        processed_schema_df = process_schema(database_schema)  # Process schema for better matching\n",
    "        sql_query = generate_sql_query(augmented_query, processed_schema_df)\n",
    "       \n",
    "        results = execute_sql_query(conn, sql_query)\n",
    "        conn.close()\n",
    "        \n",
    "        if results is not None:\n",
    "            print(\"Query Results:\")\n",
    "            for row in results:\n",
    "                print(row)\n",
    "        else:\n",
    "            print(\"No results returned or error occurred during query execution.\")\n",
    "        \n",
    "        return f\"Generated SQL Query: {sql_query}\"\n",
    "    \n",
    "    else:\n",
    "        database_schema = fetch_schema(conn)  # Fetching schema again if needed\n",
    "        database_schema_df = process_schema(database_schema)\n",
    "        return get_answer_from_chatbot(user_query, database_schema_df.to_dict(orient='records'))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        response = process_user_query(user_input)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdccc17-2c21-4fd8-94d3-6d59110e160e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da24fcf-decb-4746-bf1f-feb9cb0c1f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
