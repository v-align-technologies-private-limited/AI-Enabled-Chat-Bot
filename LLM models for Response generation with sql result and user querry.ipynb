{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0cb52f-a33a-449c-aad5-ddd0187722be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_function': 'gelu_new', 'architectures': ['GPT2LMHeadModel'], 'attn_pdrop': 0.1, 'bos_token_id': 50256, 'embd_pdrop': 0.1, 'eos_token_id': 50256, 'initializer_range': 0.02, 'layer_norm_epsilon': 1e-05, 'model_type': 'gpt2', 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'n_positions': 1024, 'resid_pdrop': 0.1, 'summary_activation': None, 'summary_first_dropout': 0.1, 'summary_proj_to_labels': True, 'summary_type': 'cls_index', 'summary_use_proj': True, 'task_specific_params': {'text-generation': {'do_sample': True, 'max_length': 50}}, 'vocab_size': 50257}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Example: Use requests to fetch a model config\n",
    "response = requests.get(\"https://huggingface.co/gpt2/resolve/main/config.json\", verify=False)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d41518-ed84-406c-be07-3b8eb52b1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response:\n",
      "SELECT * FROM gs WHERE fp > 0 ORDER BY fp AND fp < (0 <= fp), (1 <= fp),.\n",
      "In this example, we defined a query to find customers in a company, and we used Google Query (or something similar).\n",
      "SELECT * FROM gs WHERE ftp > 0 ORDER BY ftp AND ftp < (0 <= ftp), (1 < ftp), (2 < ftp),.\n",
      "Using Google Query we get SQL results which look like this:\n",
      "SELECT * FROM gs WHERE ftp > 0 ORDER BY ftp AND ftp < (0 <= ftp),.\n",
      "We also did not need to type any parameters.\n",
      "For example, we can type \"I will convert to's' and query our database instead.\n",
      "SQL result: \"All products of any type named by the database.\"\n",
      "Note that in this example we had an \"s\" in the value, and we have no parameter values: we were \"completing\" SQL.\n",
      "Let's get to a quick step up. All products of any type named by the database\n",
      "(3) SELECT * FROM gs WHERE ftp > 0 ORDER BY ftp AND ftp < (0 <= ftp),. The query was valid.\n",
      "In this example, we can type \"All products of any type named by the database.\" and we have no parameter values: if we did not specify any parameter values we would not have entered \"The number of customer names selected with's.'\" (You can use any language).\n",
      "So now we have something we can use as an example for our query. I'll show the most relevant example first:\n",
      "1.2\n",
      "The simple but useful statement I've used to describe this simple rule in the previous article, \"A customer may be listed in the database.\"\n",
      "It is the most important statement this guide will take.\n",
      "For a quick understanding of \"A customer must be in a place where a client has a database of their choosing.\"\n",
      "In this example, we set a \"name change\" clause that says \"We will change the server name\" to the name of the client.\n",
      "2.2\n",
      "Note that we don't need to type the \"name change\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_response_with_model(user_query, sql_result):\n",
    "    # Prepare the prompt for the model\n",
    "    prompt = f\"User query: \\\"{user_query}\\\"\\nSQL result: \\\"{sql_result}\\\"\\nGenerate a concise natural language response:\"\n",
    "    \n",
    "    # Load the text generation pipeline with a model\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    # Generate the response\n",
    "    response = generator(prompt, max_length=500, num_return_sequences=1)\n",
    "    \n",
    "    return response[0]['generated_text'].strip()\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What are the sales figures?\"\n",
    "sql_result = \"Sales: $1000, Expenses: $500\"\n",
    "print(generate_response_with_model(user_query, sql_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbff951-aac5-48e9-b08c-91427cd3142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response: \"What are the sales figures?\"\n",
      "SQL result: \"Sales: $1000, Expenses: $500\"\n",
      "Generate a concise natural language response: \"What are the sales\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_response_with_gpt_neo(user_query, sql_result):\n",
    "    # Prepare the prompt for the model\n",
    "    prompt = f\"User query: \\\"{user_query}\\\"\\nSQL result: \\\"{sql_result}\\\"\\nGenerate a concise natural language response:\"\n",
    "    \n",
    "    # Load the text generation pipeline with GPT-Neo\n",
    "    generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')\n",
    "    \n",
    "    # Generate the response\n",
    "    response = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "    \n",
    "    return response[0]['generated_text'].strip()\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What are the sales figures?\"\n",
    "sql_result = \"Sales: $1000, Expenses: $500\"\n",
    "print(generate_response_with_gpt_neo(user_query, sql_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f39151d-a1f7-4eb4-b961-eaf569a982e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca27ba8900a749b19bd3d3eabd2ddee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-1.3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d883898949d9424ba57790376de55d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f1f0b0ac8f4407a5b90b69a533f7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fbe96509c8420d861082149e6e07b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baa3bf7f41e42258d2b0e2676dcb65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869bba7382bd4c1583da211304905915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: What is the count of projects?\n",
      "SQL result: 10\n",
      "Generate a natural language response to this query.\n",
      "What is the best method to store or perform this query?  \n",
      "\n",
      "A:\n",
      "\n",
      "You can easily use ROW_NUMBER() to do this.\n",
      "For example, you'll want to do an UPDATE with something that you count from the last page in your results:\n",
      "UPDATE t1 \n",
      "  SET `result` = row_number() OVER(PARTITION BY p.`project` ORDER BY `result` DESC)\n",
      "WHERE \n",
      "     `result` = 1;\n",
      "\n",
      "If you aren't convinced about the benefits of using ROW_NUMBER\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def generate_response_llm(query, sql_result):\n",
    "    \"\"\"\n",
    "    Generates a natural language response based on the user query and SQL result using an LLM.\n",
    "\n",
    "    Args:\n",
    "    query (str): The user query in natural language.\n",
    "    sql_result (any): The result of the SQL query, could be a count, list, or any other type.\n",
    "\n",
    "    Returns:\n",
    "    str: A natural language response to the query using an LLM.\n",
    "    \"\"\"\n",
    "    # Convert the SQL result to string format for the LLM\n",
    "    result_str = str(sql_result)\n",
    "    \n",
    "    # Construct the prompt for the LLM\n",
    "    prompt = f\"User query: {query}\\nSQL result: {result_str}\\nGenerate a natural language response to this query.\"\n",
    "\n",
    "    # Load a free Hugging Face model (example: GPT-Neo)\n",
    "    model_name = \"EleutherAI/gpt-neo-1.3B\"  # You can switch to other models like 'flan-t5' or 'GPT-J'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize and generate a response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=150, do_sample=True)\n",
    "\n",
    "    # Decode the generated output into natural language\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "query = \"What is the count of projects?\"\n",
    "sql_result = 10\n",
    "\n",
    "response = generate_response_llm(query, sql_result)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05115b3-7353-40e7-8399-1616cc4c1ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca267591-6fac-40a4-b050-f391780ae726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_response_llm(query, sql_result):\n",
    "    \"\"\"\n",
    "    Generates a natural language response based on the user query and SQL result using GPT-J.\n",
    "\n",
    "    Args:\n",
    "    query (str): The user query in natural language.\n",
    "    sql_result (any): The result of the SQL query, could be a count, list, or any other type.\n",
    "\n",
    "    Returns:\n",
    "    str: A natural language response to the query using GPT-J.\n",
    "    \"\"\"\n",
    "    result_str = str(sql_result)\n",
    "    prompt = f\"User query: {query}\\nSQL result: {result_str}\\nGenerate a natural language response to this query.\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=5000, do_sample=True)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    # Extract the line starting with \"Result:\"\n",
    "    for line in response.splitlines():\n",
    "        if line.startswith(\"Result:\"):\n",
    "            return line\n",
    "    \n",
    "    return \"No result found in response.\"\n",
    "\n",
    "user_query = \"What are the sales figures?\"\n",
    "sql_result = \"Sales: $1000, Expenses: $500\"\n",
    "\n",
    "response = generate_response_llm(user_query, sql_result)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e14c1e-474a-4eae-821b-880f3abdca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def generate_response_llm(query, sql_result):\n",
    "    \"\"\"\n",
    "    Generates a natural language response based on the user query and SQL result using GPT-J.\n",
    "\n",
    "    Args:\n",
    "    query (str): The user query in natural language.\n",
    "    sql_result (any): The result of the SQL query, could be a count, list, or any other type.\n",
    "\n",
    "    Returns:\n",
    "    str: A natural language response to the query using GPT-J.\n",
    "    \"\"\"\n",
    "    # Convert the SQL result to string format for the LLM\n",
    "    result_str = str(sql_result)\n",
    "    \n",
    "    # Construct the prompt for the LLM\n",
    "    prompt = f\"User query: {query}\\nSQL result: {result_str}\\nGenerate a natural language response to this query.\"\n",
    "\n",
    "    # Load the GPT-J model from Hugging Face\n",
    "    model_name = \"EleutherAI/gpt-j-6B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize and generate a response\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=150, do_sample=True)\n",
    "\n",
    "    # Decode the generated output into natural language\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the count of projects?\"\n",
    "sql_result = 10\n",
    "\n",
    "response = generate_response_llm(query, sql_result)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece57e45-a522-4682-a952-3b35c66921e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
