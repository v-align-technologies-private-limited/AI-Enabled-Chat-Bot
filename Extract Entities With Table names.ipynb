{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3b956b-0c2f-4a20-a063-53b20c8f013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  give me the list of all closed tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18668\\1528503187.py:75: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_features {\"tasks\": {\"status\": \"closed\"}}\n",
      "feature_dict {'tasks': {'status': 'closed'}}\n",
      "cleaned_feature_dict {'tasks': {'status': 'closed'}}\n",
      "cleaned_feature_dict {'tasks': {'status': 'closed'}}\n",
      "tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple matches found for 'closed':\n",
      "1: Terminated/Cancelled\n",
      "2: Done\n",
      "3: In Progress\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please select the most relevant option for 'closed' (1-3):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me the list of all Done tasks\n",
      "Response: To retrieve the list of all done tasks based on the provided schema, we need to query the `tasks` table and filter for tasks that have a status indicating they are done. Assuming that the status for done tasks is represented as 'Done' (case-sensitive), the SQL query would be:\n",
      "\n",
      "```sql\n",
      "SELECT *\n",
      "FROM tasks\n",
      "WHERE status = 'Done';\n",
      "```\n",
      "\n",
      "This query selects all columns from the `tasks` table where the `status` column matches 'Done'.\n",
      "Generated SQL Query: SELECT *\n",
      "FROM tasks\n",
      "WHERE status = 'Done';\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pinecone\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "OPENAI_API_KEY = \"\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc\"\n",
    "PORT = 5432\n",
    "\n",
    "# Constants\n",
    "PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "NAMESPACE = \"\"  # Replace with your namespace\n",
    "columnnames=''\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Initialize Pinecone client\n",
    "def initialize_pinecone():\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    if INDEX_NAME not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=768,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "        )\n",
    "    return pc.Index(INDEX_NAME)\n",
    "\n",
    "# Load Hugging Face model for embeddings\n",
    "def load_huggingface_model():\n",
    "    return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Function to connect to PostgreSQL database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to fetch schema from PostgreSQL database\n",
    "def fetch_schema(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        # print(schema_df)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to process schema: remove special characters and convert to lowercase\n",
    "def process_schema(schema_df):\n",
    "    def clean_column_name(name):\n",
    "        return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "    schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "    return schema_df\n",
    "\n",
    "# Function to extract features (like project name, owner, etc.) using OpenAI's LLM\n",
    "def extract_features_with_openai(user_input, processed_schema_df):\n",
    "    schema_json = processed_schema_df.to_json(orient='records')\n",
    "    \n",
    "    # Refined prompt to ensure OpenAI extracts table names, column names, and their values\n",
    "    prompt = f\"\"\"\n",
    "    ## Database Schema Context:\n",
    "    The following represents the columns and their respective tables available in the database:\n",
    "    {schema_json}\n",
    "\n",
    "    ## User Input:\n",
    "    The user has provided the following input: \"{user_input}\"\n",
    "\n",
    "    ## Task:\n",
    "    Extract the relevant features, values, and table names from the user input based on the schema. These features might include project names, owners, dates, statuses, etc., along with their corresponding table names.\n",
    "\n",
    "    ## Instructions:\n",
    "    - Return a JSON dictionary that includes the table names as keys, and within each table, include the fields and their values extracted from the user input.\n",
    "    - Omit any fields or tables where the value is empty or null.\n",
    "    - Format the output as a JSON object with keys only for tables and fields that have values.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=500,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        extracted_features = response.choices[0].text.strip()\n",
    "        return extracted_features\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error with OpenAI: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to remove null, None, empty values from JSON and list\n",
    "def clean_extracted_features(feature_dict):\n",
    "    # Remove any keys with None or empty values\n",
    "    cleaned_feature_dict = {k: v for k, v in feature_dict.items() if v}\n",
    "    print(\"cleaned_feature_dict\", cleaned_feature_dict)\n",
    "    # Extract the non-null values into a list\n",
    "    feature_list = list(cleaned_feature_dict.values())\n",
    "    return cleaned_feature_dict, feature_list\n",
    "\n",
    "#Extract Dynamic Namespoace\n",
    "def extract_nmaespace(extracted_dict):\n",
    "    global NAMESPACE,columnnames\n",
    "    for key in extracted_dict.keys():\n",
    "        print(key)\n",
    "    NAMESPACE= key\n",
    "    # columnnames = extracted_dict[key].values()\n",
    "    # print(columnnames)\n",
    "            \n",
    "# Function to parse and process extracted features\n",
    "def process_extracted_features(extracted_features):\n",
    "    try:\n",
    "        # Remove the \"## Solution:\" part and any other non-JSON text\n",
    "        json_match = re.search(r'\\{.*\\}', extracted_features, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            # Extract the JSON part from the matched result\n",
    "            cleaned_features = json_match.group(0)\n",
    "            print(\"cleaned_features\", cleaned_features)\n",
    "            # Convert JSON string to a Python dictionary\n",
    "            feature_dict = json.loads(cleaned_features)\n",
    "            print(\"feature_dict\", feature_dict)\n",
    "\n",
    "            # Clean feature dictionary and feature list to remove nulls and empty values\n",
    "            cleaned_feature_dict, feature_list = clean_extracted_features(feature_dict)\n",
    "\n",
    "            # Return cleaned JSON and feature list\n",
    "            return json.dumps(cleaned_feature_dict, indent=4), feature_list\n",
    "        else:\n",
    "            return None, []\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Error parsing features: {e}\")\n",
    "        return None, []\n",
    "\n",
    "# Query Pinecone for relevant context and augment the input\n",
    "def query_pinecone_and_augment_input(user_input, entities, namespace):\n",
    "    embedding_model = load_huggingface_model()\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    augmented_input = user_input\n",
    "    pinecone_data = {}\n",
    "    global columnnames\n",
    "\n",
    "    # Function to flatten the nested dictionary\n",
    "    def flatten_dict(d, parent_key=''):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(flatten_dict(v, new_key).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    # Flatten the entities dictionary\n",
    "    flat_entities = flatten_dict(entities)\n",
    "\n",
    "    for entity_name, entity_value in flat_entities.items():\n",
    "        if entity_value:\n",
    "            query_embedding = embedding_model.encode([entity_value])[0]\n",
    "            query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "           \n",
    "            try:\n",
    "                result = pinecone_index.query(\n",
    "                    namespace=namespace,\n",
    "                    vector=query_embedding.tolist(),\n",
    "                                    filter={\n",
    "                        \"column_name\": {\"$eq\": \"status\" }\n",
    "                    },\n",
    "                    top_k=3,\n",
    "                    include_values=True,\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                matches = result.get('matches', [])\n",
    "                if matches:\n",
    "                    unique_values = [match['metadata'].get('unique_value') for match in matches if 'metadata' in match]\n",
    "                    if unique_values:\n",
    "                        pinecone_data[entity_name] = unique_values\n",
    "                        if len(unique_values) > 1:\n",
    "                            print(f\"Multiple matches found for '{entity_value}':\")\n",
    "                            for idx, unique_value in enumerate(unique_values):\n",
    "                                print(f\"{idx + 1}: {unique_value}\")\n",
    "                            while True:\n",
    "                                selection = input(f\"Please select the most relevant option for '{entity_value}' (1-{len(unique_values)}): \")\n",
    "                                try:\n",
    "                                    selected_value = unique_values[int(selection) - 1]\n",
    "                                    augmented_input = augmented_input.replace(entity_value, selected_value)\n",
    "                                    break\n",
    "                                except (IndexError, ValueError):\n",
    "                                    print(\"Invalid selection. Please choose a valid option.\")\n",
    "                        else:\n",
    "                            augmented_input = augmented_input.replace(entity_value, unique_values[0])\n",
    "                else:\n",
    "                    print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error querying Pinecone: {str(e)}\")\n",
    "                return f\"Error querying Pinecone: {str(e)}\", {}\n",
    "    return augmented_input, pinecone_data\n",
    "\n",
    "# Function to generate SQL query using GPT-4o-mini\n",
    "def generate_sql_query(processed_schema_df, augmented_input):\n",
    "    # Convert the schema dataframe to a string\n",
    "    schema_str = processed_schema_df.to_string(index=False)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The database contains the following schema:\n",
    "    {schema_str}\n",
    "\n",
    "    Based on this schema and the user request:\n",
    "    \"{augmented_input}\"\n",
    "\n",
    "    Generate an optimized SQL query that meets the user's intent.\n",
    "    The query should be efficient and use the correct table and column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call GPT-4o-mini-2024-07-18 model using chat completion API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in generating SQL queries, always ensuring the use of appropriate operators like LIKE or expressions in sql queries like '% %' for partial matches if needed. Accurately map user input to the relevant tables and columns in the database based on the provided schema, using the LIKE operator for partial matches where necessary. Handle data type mismatches explicitly by casting to the appropriate type when required, ensuring correct query execution. Additionally, Manage variations in user input, such as case sensitivity or small spelling differences, using flexible matching techniques to generate precise and reliable SQL queries.Note do not use ILIKE Operator\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500,  # Reduced token limit for completion\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract SQL query from the response\n",
    "    sql_response = response.choices[0].message.content\n",
    "    # Find and clean the SQL query part\n",
    "    start = sql_response.find(\"```sql\") + 6\n",
    "    end = sql_response.find(\"```\", start)\n",
    "    sql_query = sql_response\n",
    "    print(\"Response:\",sql_response)\n",
    "\n",
    "    return sql_query\n",
    "\n",
    "\n",
    "# Extract generated SQL Query\n",
    "def extract_sql_query(response):\n",
    "    start = response.find(\"```sql\") + len(\"```sql\\n\")\n",
    "    end = response.find(\"```\", start)\n",
    "    sql_query = response[start:end].strip()\n",
    "    return sql_query\n",
    "\n",
    "# Function to execute the SQL query and print the results\n",
    "def execute_sql_query(conn, sql_query):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql_query)\n",
    "            results = cursor.fetchall()\n",
    "            print(results)\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        return None\n",
    "        \n",
    "# Main function to process user input and extract entities\n",
    "def process_user_input(user_input):\n",
    "    global NAMESPACE\n",
    "    # Connect to DB and fetch schema\n",
    "    conn = connect_to_db()\n",
    "    schema_df = fetch_schema(conn)\n",
    "    processed_schema_df = process_schema(schema_df)\n",
    "\n",
    "    # Extract features from user input using OpenAI\n",
    "    extracted_features = extract_features_with_openai(user_input, processed_schema_df)\n",
    "\n",
    "    # Process the extracted features and clean them\n",
    "    cleaned_json, feature_list = process_extracted_features(extracted_features)\n",
    "\n",
    "    # Query Pinecone and augment the user input\n",
    "    if cleaned_json:\n",
    "        cleaned_feature_dict = json.loads(cleaned_json)\n",
    "        cleaned_extracted_features, feature_list = clean_extracted_features(cleaned_feature_dict)  # Rename the variable here\n",
    "        extract_namespace = extract_nmaespace(cleaned_extracted_features)\n",
    "        augmented_input, namespace_data = query_pinecone_and_augment_input(user_input, cleaned_feature_dict, NAMESPACE)  # Ensure to capture both outputs\n",
    "        print(augmented_input)\n",
    "        \n",
    "        # Generate SQL query using the augmented input\n",
    "        sql_query = generate_sql_query(processed_schema_df, augmented_input)\n",
    "        sql_query=extract_sql_query(sql_query)\n",
    "        print(\"Generated SQL Query:\", sql_query)\n",
    "\n",
    "        # Execute the SQL query\n",
    "        results = execute_sql_query(conn, sql_query)\n",
    "        print(\"Query Results:\", results)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "user_input = input(\"Enter your query: \")\n",
    "process_user_input(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76b8fbf-92a3-46aa-8eb6-8f03ce2f1f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  give me the list of all closed tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18668\\4018436545.py:74: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': {'status': 'closed'}}\n",
      "{'tasks': {'status': 'closed'}}\n",
      "tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple matches found for 'closed':\n",
      "1: Terminated/Cancelled\n",
      "2: Done\n",
      "3: In Progress\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please select the most relevant option for 'closed' (1-3):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me the list of all Done tasks\n",
      "Response: Based on the user's request to \"give me the list of all Done tasks,\" we need to query the `tasks` table and filter the results where the `status` is \"Done\". Here is the optimized SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT *\n",
      "FROM tasks\n",
      "WHERE status LIKE 'Done';\n",
      "```\n",
      "\n",
      "This query retrieves all columns from the `tasks` table where the `status` column matches \"Done\". The use of `LIKE` is appropriate for this case, allowing for potential variations in case sensitivity.\n",
      "Generated SQL Query: SELECT *\n",
      "FROM tasks\n",
      "WHERE status LIKE 'Done';\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pinecone\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "OPENAI_API_KEY = \"sk-proj-UnzdWuWBs7ZQRbRPiRCoT3BlbkFJhPM1p7DdZUMklcpnWK1S\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc\"\n",
    "PORT = 5432\n",
    "\n",
    "# Constants\n",
    "PINECONE_API_KEY = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"  # Replace with your Pinecone API key\n",
    "INDEX_NAME = \"smart-desk\"  # Replace with your Pinecone index name\n",
    "NAMESPACE = \"projects\"  # Replace with your namespace\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Initialize Pinecone client\n",
    "def initialize_pinecone():\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    if INDEX_NAME not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=768,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "        )\n",
    "    return pc.Index(INDEX_NAME)\n",
    "\n",
    "# Load Hugging Face model for embeddings\n",
    "def load_huggingface_model():\n",
    "    return SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Function to connect to PostgreSQL database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to fetch schema from PostgreSQL database\n",
    "def fetch_schema(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        # print(schema_df)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to process schema: remove special characters and convert to lowercase\n",
    "def process_schema(schema_df):\n",
    "    def clean_column_name(name):\n",
    "        return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "    schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "    return schema_df\n",
    "\n",
    "# Function to extract features (like project name, owner, etc.) using OpenAI's LLM\n",
    "def extract_features_with_openai(user_input, processed_schema_df):\n",
    "    schema_json = processed_schema_df.to_json(orient='records')\n",
    "    \n",
    "    # Refined prompt to ensure OpenAI extracts table names, column names, and their values\n",
    "    prompt = f\"\"\"\n",
    "    ## Database Schema Context:\n",
    "    The following represents the columns and their respective tables available in the database:\n",
    "    {schema_json}\n",
    "\n",
    "    ## User Input:\n",
    "    The user has provided the following input: \"{user_input}\"\n",
    "\n",
    "    ## Task:\n",
    "    Extract the relevant features, values, and table names from the user input based on the schema. These features might include project names, owners, dates, statuses, etc., along with their corresponding table names.\n",
    "\n",
    "    ## Instructions:\n",
    "    - Return a JSON dictionary that includes the table names as keys, and within each table, include the fields and their values extracted from the user input.\n",
    "    - Omit any fields or tables where the value is empty or null.\n",
    "    - Format the output as a JSON object with keys only for tables and fields that have values.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=500,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        extracted_features = response.choices[0].text.strip()\n",
    "        return extracted_features\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error with OpenAI: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to remove null, None, empty values from JSON and list\n",
    "def clean_extracted_features(feature_dict):\n",
    "    # Remove any keys with None or empty values\n",
    "    cleaned_feature_dict = {k: v for k, v in feature_dict.items() if v}\n",
    "    print(cleaned_feature_dict)\n",
    "    # Extract the non-null values into a list\n",
    "    feature_list = list(cleaned_feature_dict.values())\n",
    "    return cleaned_feature_dict, feature_list\n",
    "\n",
    "#Extract Dynamic Namespoace\n",
    "def extract_nmaespace(extracted_dict):\n",
    "    global NAMESPACE\n",
    "    for key in extracted_dict.keys():\n",
    "        print(key)\n",
    "    NAMESPACE= key\n",
    "            \n",
    "# Function to parse and process extracted features\n",
    "def process_extracted_features(extracted_features):\n",
    "    try:\n",
    "        # Remove the \"## Solution:\" part and any other non-JSON text\n",
    "        json_match = re.search(r'\\{.*\\}', extracted_features, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            # Extract the JSON part from the matched result\n",
    "            cleaned_features = json_match.group(0)\n",
    "\n",
    "            # Convert JSON string to a Python dictionary\n",
    "            feature_dict = json.loads(cleaned_features)\n",
    "\n",
    "            # Clean feature dictionary and feature list to remove nulls and empty values\n",
    "            cleaned_feature_dict, feature_list = clean_extracted_features(feature_dict)\n",
    "\n",
    "            # Return cleaned JSON and feature list\n",
    "            return json.dumps(cleaned_feature_dict, indent=4), feature_list\n",
    "        else:\n",
    "            return None, []\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Error parsing features: {e}\")\n",
    "        return None, []\n",
    "\n",
    "# Query Pinecone for relevant context and augment the input\n",
    "def query_pinecone_and_augment_input(user_input, entities, namespace):\n",
    "    embedding_model = load_huggingface_model()\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    augmented_input = user_input\n",
    "    pinecone_data = {}\n",
    "\n",
    "    # Function to flatten the nested dictionary\n",
    "    def flatten_dict(d, parent_key=''):\n",
    "        items = []\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(flatten_dict(v, new_key).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "\n",
    "    # Flatten the entities dictionary\n",
    "    flat_entities = flatten_dict(entities)\n",
    "\n",
    "    for entity_name, entity_value in flat_entities.items():\n",
    "        if entity_value:\n",
    "            query_embedding = embedding_model.encode([entity_value])[0]\n",
    "            query_embedding = np.array(query_embedding, dtype=np.float32)\n",
    "            try:\n",
    "                result = pinecone_index.query(\n",
    "                    namespace=namespace,\n",
    "                    vector=query_embedding.tolist(),\n",
    "                    filter={\n",
    "                        \"column_name\": {\"$eq\": \"status\"}\n",
    "                    },\n",
    "                    top_k=3,\n",
    "                    include_values=True,\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                matches = result.get('matches', [])\n",
    "                if matches:\n",
    "                    unique_values = [match['metadata'].get('unique_value') for match in matches if 'metadata' in match]\n",
    "                    if unique_values:\n",
    "                        pinecone_data[entity_name] = unique_values\n",
    "                        if len(unique_values) > 1:\n",
    "                            print(f\"Multiple matches found for '{entity_value}':\")\n",
    "                            for idx, unique_value in enumerate(unique_values):\n",
    "                                print(f\"{idx + 1}: {unique_value}\")\n",
    "                            while True:\n",
    "                                selection = input(f\"Please select the most relevant option for '{entity_value}' (1-{len(unique_values)}): \")\n",
    "                                try:\n",
    "                                    selected_value = unique_values[int(selection) - 1]\n",
    "                                    augmented_input = augmented_input.replace(entity_value, selected_value)\n",
    "                                    break\n",
    "                                except (IndexError, ValueError):\n",
    "                                    print(\"Invalid selection. Please choose a valid option.\")\n",
    "                        else:\n",
    "                            augmented_input = augmented_input.replace(entity_value, unique_values[0])\n",
    "                else:\n",
    "                    print(f\"No matches found for {entity_value} in Pinecone.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error querying Pinecone: {str(e)}\")\n",
    "                return f\"Error querying Pinecone: {str(e)}\", {}\n",
    "    return augmented_input, pinecone_data\n",
    "\n",
    "# Function to generate SQL query using GPT-4o-mini\n",
    "def generate_sql_query(processed_schema_df, augmented_input):\n",
    "    # Convert the schema dataframe to a string\n",
    "    schema_str = processed_schema_df.to_string(index=False)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The database contains the following schema:\n",
    "    {schema_str}\n",
    "\n",
    "    Based on this schema and the user request:\n",
    "    \"{augmented_input}\"\n",
    "\n",
    "    Generate an optimized SQL query that meets the user's intent.\n",
    "    The query should be efficient and use the correct table and column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call GPT-4o-mini-2024-07-18 model using chat completion API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in generating SQL queries, always ensuring the use of appropriate operators like LIKE or expressions in sql queries like '% %' for partial matches if needed. Accurately map user input to the relevant tables and columns in the database based on the provided schema, using the LIKE operator for partial matches where necessary. Handle data type mismatches explicitly by casting to the appropriate type when required, ensuring correct query execution. Additionally, Manage variations in user input, such as case sensitivity or small spelling differences, using flexible matching techniques to generate precise and reliable SQL queries.Note do not use ILIKE Operator\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500,  # Reduced token limit for completion\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract SQL query from the response\n",
    "    sql_response = response.choices[0].message.content\n",
    "    # Find and clean the SQL query part\n",
    "    start = sql_response.find(\"```sql\") + 6\n",
    "    end = sql_response.find(\"```\", start)\n",
    "    sql_query = sql_response\n",
    "    print(\"Response:\",sql_response)\n",
    "\n",
    "    return sql_query\n",
    "\n",
    "\n",
    "# Extract generated SQL Query\n",
    "def extract_sql_query(response):\n",
    "    start = response.find(\"```sql\") + len(\"```sql\\n\")\n",
    "    end = response.find(\"```\", start)\n",
    "    sql_query = response[start:end].strip()\n",
    "    return sql_query\n",
    "\n",
    "# Function to execute the SQL query and print the results\n",
    "def execute_sql_query(conn, sql_query):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(sql_query)\n",
    "            results = cursor.fetchall()\n",
    "            print(results)\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        return None\n",
    "        \n",
    "# Main function to process user input and extract entities\n",
    "def process_user_input(user_input):\n",
    "    global NAMESPACE\n",
    "    # Connect to DB and fetch schema\n",
    "    conn = connect_to_db()\n",
    "    schema_df = fetch_schema(conn)\n",
    "    processed_schema_df = process_schema(schema_df)\n",
    "\n",
    "    # Extract features from user input using OpenAI\n",
    "    extracted_features = extract_features_with_openai(user_input, processed_schema_df)\n",
    "\n",
    "    # Process the extracted features and clean them\n",
    "    cleaned_json, feature_list = process_extracted_features(extracted_features)\n",
    "\n",
    "    # Query Pinecone and augment the user input\n",
    "    if cleaned_json:\n",
    "        cleaned_feature_dict = json.loads(cleaned_json)\n",
    "        cleaned_extracted_features, feature_list = clean_extracted_features(cleaned_feature_dict)  # Rename the variable here\n",
    "        extract_namespace = extract_nmaespace(cleaned_extracted_features)\n",
    "        augmented_input, namespace_data = query_pinecone_and_augment_input(user_input, cleaned_feature_dict, NAMESPACE)  # Ensure to capture both outputs\n",
    "        print(augmented_input)\n",
    "        \n",
    "        # Generate SQL query using the augmented input\n",
    "        sql_query = generate_sql_query(processed_schema_df, augmented_input)\n",
    "        sql_query=extract_sql_query(sql_query)\n",
    "        print(\"Generated SQL Query:\", sql_query)\n",
    "\n",
    "        # Execute the SQL query\n",
    "        results = execute_sql_query(conn, sql_query)\n",
    "        print(\"Query Results:\", results)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "user_input = input(\"Enter your query: \")\n",
    "process_user_input(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21b8243-4021-430b-9c35-363af13ecc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import psycopg2\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import json\n",
    "\n",
    "# # Initialize OpenAI API key\n",
    "# OPENAI_API_KEY = \"sk-proj-UnzdWuWBs7ZQRbRPiRCoT3BlbkFJhPM1p7DdZUMklcpnWK1S\"\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# # Database connection details\n",
    "# DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "# DATABASE_USERNAME = \"postgres\"\n",
    "# DATABASE_PASSWORD = \"valign#123\"\n",
    "# DATABASE_DB = \"python_test_poc\"\n",
    "# PORT = 5432\n",
    "\n",
    "# # Function to connect to PostgreSQL database\n",
    "# def connect_to_db():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=DATABASE_DB,\n",
    "#             user=DATABASE_USERNAME,\n",
    "#             password=DATABASE_PASSWORD,\n",
    "#             host=DATABASE_HOST,\n",
    "#             port=PORT\n",
    "#         )\n",
    "#         return conn\n",
    "#     except psycopg2.Error as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Function to fetch schema from PostgreSQL database\n",
    "# def fetch_schema(conn):\n",
    "#     try:\n",
    "#         query = \"\"\"\n",
    "#         SELECT table_name, column_name\n",
    "#         FROM information_schema.columns\n",
    "#         WHERE table_schema = 'public'\n",
    "#         \"\"\"\n",
    "#         schema_df = pd.read_sql(query, conn)\n",
    "#         # print(schema_df)\n",
    "#         return schema_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching schema: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Function to process schema: remove special characters and convert to lowercase\n",
    "# def process_schema(schema_df):\n",
    "#     def clean_column_name(name):\n",
    "#         return re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "\n",
    "#     schema_df['processed_column_name'] = schema_df['column_name'].apply(clean_column_name)\n",
    "#     return schema_df\n",
    "\n",
    "# # Function to extract features (like project name, owner, etc.) using OpenAI's LLM\n",
    "# def extract_features_with_openai(user_input, processed_schema_df):\n",
    "#     schema_json = processed_schema_df.to_json(orient='records')\n",
    "    \n",
    "#     # Refined prompt to ensure OpenAI extracts table names, column names, and their values\n",
    "#     prompt = f\"\"\"\n",
    "#     ## Database Schema Context:\n",
    "#     The following represents the columns and their respective tables available in the database:\n",
    "#     {schema_json}\n",
    "\n",
    "#     ## User Input:\n",
    "#     The user has provided the following input: \"{user_input}\"\n",
    "\n",
    "#     ## Task:\n",
    "#     Extract the relevant features, values, and table names from the user input based on the schema. These features might include project names, owners, dates, statuses, etc., along with their corresponding table names.\n",
    "\n",
    "#     ## Instructions:\n",
    "#     - Return a JSON dictionary that includes the table names as keys, and within each table, include the fields and their values extracted from the user input.\n",
    "#     - Omit any fields or tables where the value is empty or null.\n",
    "#     - Format the output as a JSON object with keys only for tables and fields that have values.\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         response = openai.completions.create(\n",
    "#             model=\"gpt-3.5-turbo-instruct\",\n",
    "#             prompt=prompt,\n",
    "#             max_tokens=500,\n",
    "#             temperature=0.5\n",
    "#         )\n",
    "#         extracted_features = response.choices[0].text.strip()\n",
    "#         return extracted_features\n",
    "#     except openai.OpenAIError as e:\n",
    "#         print(f\"Error with OpenAI: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Function to remove null, None, empty values from JSON and list\n",
    "# def clean_extracted_features(feature_dict):\n",
    "#     # Remove any keys with None or empty values\n",
    "#     cleaned_feature_dict = {k: v for k, v in feature_dict.items() if v}\n",
    "#     # Extract the non-null values into a list\n",
    "#     feature_list = list(cleaned_feature_dict.values())\n",
    "#     return cleaned_feature_dict, feature_list\n",
    "\n",
    "# # Function to parse and process extracted features\n",
    "# def process_extracted_features(extracted_features):\n",
    "#     try:\n",
    "#         # Remove the \"## Solution:\" part and any other non-JSON text\n",
    "#         json_match = re.search(r'\\{.*\\}', extracted_features, re.DOTALL)\n",
    "        \n",
    "#         if json_match:\n",
    "#             # Extract the JSON part from the matched result\n",
    "#             cleaned_features = json_match.group(0)\n",
    "\n",
    "#             # Convert JSON string to a Python dictionary\n",
    "#             feature_dict = json.loads(cleaned_features)\n",
    "\n",
    "#             # Clean feature dictionary and feature list to remove nulls and empty values\n",
    "#             cleaned_feature_dict, feature_list = clean_extracted_features(feature_dict)\n",
    "\n",
    "#             # Return cleaned JSON and feature list\n",
    "#             return json.dumps(cleaned_feature_dict, indent=4), feature_list\n",
    "#         else:\n",
    "#             return None, []\n",
    "#     except (json.JSONDecodeError, ValueError) as e:\n",
    "#         print(f\"Error parsing features: {e}\")\n",
    "#         return None, []\n",
    "\n",
    "# # Main function to process user input and extract entities\n",
    "# def process_user_input(user_input):\n",
    "#     # Connect to DB and fetch schema\n",
    "#     conn = connect_to_db()\n",
    "#     schema_df = fetch_schema(conn)\n",
    "#     processed_schema_df = process_schema(schema_df)\n",
    "\n",
    "#     # Extract features from user input using OpenAI's LLM\n",
    "#     extracted_features = extract_features_with_openai(user_input, processed_schema_df)\n",
    "\n",
    "#     # Process the extracted features and clean them\n",
    "#     cleaned_json, feature_list = process_extracted_features(extracted_features)\n",
    "\n",
    "#     # Output cleaned JSON and feature list\n",
    "#     if cleaned_json:\n",
    "#         print(\"Cleaned Extracted JSON Features:\")\n",
    "#         print(cleaned_json)\n",
    "    \n",
    "#     print(\"Feature List:\")\n",
    "#     print(feature_list)\n",
    "\n",
    "# # Get user input\n",
    "# user_input = input(\"Enter your query: \")\n",
    "\n",
    "# # Process user input and extract entities\n",
    "# process_user_input(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd55984-7a65-4d32-8a79-58a4382cd8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
