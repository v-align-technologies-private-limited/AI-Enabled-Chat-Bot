{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f140ff5-eaf7-47d7-9ef0-d8f74e9f4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protoc-gen-openapiv2\n",
      "  Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: googleapis-common-protos in c:\\users\\admin\\appdata\\local\\anaconda3\\envs\\py310\\lib\\site-packages (from protoc-gen-openapiv2) (1.65.0)\n",
      "Requirement already satisfied: protobuf>=4.21.0 in c:\\users\\admin\\appdata\\local\\anaconda3\\envs\\py310\\lib\\site-packages (from protoc-gen-openapiv2) (4.25.3)\n",
      "Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: protoc-gen-openapiv2\n",
      "Successfully installed protoc-gen-openapiv2-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)'))': /simple/protoc-gen-openapiv2/\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install protoc-gen-openapiv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d3dfab-ca52-4272-a520-bb1107d15f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    table_name                column_name          data_type  \\\n",
      "0   milestones  milestone_completion_mode  character varying   \n",
      "1   milestones               milestone_id  character varying   \n",
      "2   milestones                 owner_name  character varying   \n",
      "3   milestones           milestone_status  character varying   \n",
      "4   milestones                 project_id  character varying   \n",
      "5   milestones                application  character varying   \n",
      "6   milestones                     status  character varying   \n",
      "7   milestones               project_name  character varying   \n",
      "8   milestones             milestone_name  character varying   \n",
      "9     projects                  projectid  character varying   \n",
      "10    projects                projectname  character varying   \n",
      "11    projects                      owner  character varying   \n",
      "12    projects                     status  character varying   \n",
      "13    projects              delivery_team  character varying   \n",
      "14       tasks     actual_time_taken_temp               text   \n",
      "15       tasks                 sprint_new  character varying   \n",
      "16       tasks                   qc_owner  character varying   \n",
      "17       tasks          actual_time_taken               text   \n",
      "18       tasks                   duration               text   \n",
      "19       tasks          time_spent_so_far               text   \n",
      "20       tasks                    task_id  character varying   \n",
      "21       tasks                  task_name  character varying   \n",
      "22       tasks                      owner  character varying   \n",
      "23       tasks                   priority  character varying   \n",
      "24       tasks                     status  character varying   \n",
      "25       tasks                 is_overdue  character varying   \n",
      "26       tasks               project_name  character varying   \n",
      "27       tasks             milestone_name  character varying   \n",
      "28       tasks                 project_id  character varying   \n",
      "29       tasks              milestone_id1  character varying   \n",
      "30       tasks               milestone_id  character varying   \n",
      "31       tasks            task_delay_time               text   \n",
      "32       tasks       task_completion_mode  character varying   \n",
      "33       tasks              duration_unit  character varying   \n",
      "34       tasks              clarity_level  character varying   \n",
      "35       tasks                     sprint  character varying   \n",
      "36       tasks               billing_type  character varying   \n",
      "37       tasks              product_skill  character varying   \n",
      "38       tasks               sprint_ff_sf  character varying   \n",
      "39       tasks                open_closed  character varying   \n",
      "40       tasks      allocated_unallocated  character varying   \n",
      "41       tasks          days_completed_on  character varying   \n",
      "42       users                 user_email  character varying   \n",
      "43       users                       role  character varying   \n",
      "44       users                     status  character varying   \n",
      "45       users                  user_name  character varying   \n",
      "46       users               profile_name  character varying   \n",
      "\n",
      "    character_maximum_length  \n",
      "0                        NaN  \n",
      "1                        NaN  \n",
      "2                        NaN  \n",
      "3                        NaN  \n",
      "4                        NaN  \n",
      "5                        NaN  \n",
      "6                        NaN  \n",
      "7                        NaN  \n",
      "8                        NaN  \n",
      "9                        NaN  \n",
      "10                       NaN  \n",
      "11                       NaN  \n",
      "12                       NaN  \n",
      "13                       NaN  \n",
      "14                       NaN  \n",
      "15                       NaN  \n",
      "16                       NaN  \n",
      "17                       NaN  \n",
      "18                       NaN  \n",
      "19                       NaN  \n",
      "20                       NaN  \n",
      "21                       NaN  \n",
      "22                       NaN  \n",
      "23                       NaN  \n",
      "24                       NaN  \n",
      "25                       NaN  \n",
      "26                       NaN  \n",
      "27                       NaN  \n",
      "28                       NaN  \n",
      "29                       NaN  \n",
      "30                       NaN  \n",
      "31                       NaN  \n",
      "32                       NaN  \n",
      "33                       NaN  \n",
      "34                       NaN  \n",
      "35                       NaN  \n",
      "36                       NaN  \n",
      "37                       NaN  \n",
      "38                       NaN  \n",
      "39                       NaN  \n",
      "40                       NaN  \n",
      "41                       NaN  \n",
      "42                     255.0  \n",
      "43                      50.0  \n",
      "44                      20.0  \n",
      "45                     255.0  \n",
      "46                     100.0  \n",
      "Schema with string data types fetched successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2472565274.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for string columns fetched successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model loaded successfully.\n",
      "No unique values found for actual_time_taken_temp in tasks. Skipping embeddings.\n",
      "No unique values found for qc_owner in tasks. Skipping embeddings.\n",
      "No unique values found for milestone_id1 in tasks. Skipping embeddings.\n",
      "Embeddings for string columns generated successfully.\n",
      "Upserted batch for milestone_completion_mode in milestones\n",
      "Upserted batch for milestone_id in milestones\n",
      "Upserted batch for milestone_id in milestones\n",
      "Upserted batch for milestone_id in milestones\n",
      "Upserted batch for milestone_id in milestones\n",
      "Upserted batch for milestone_id in milestones\n",
      "Upserted batch for milestone_id in milestones\n",
      "Upserted batch for owner_name in milestones\n",
      "Upserted batch for milestone_status in milestones\n",
      "Upserted batch for project_id in milestones\n",
      "Upserted batch for project_id in milestones\n",
      "Upserted batch for application in milestones\n",
      "Upserted batch for status in milestones\n",
      "Upserted batch for project_name in milestones\n",
      "Upserted batch for project_name in milestones\n",
      "Upserted batch for milestone_name in milestones\n",
      "Upserted batch for milestone_name in milestones\n",
      "Upserted batch for milestone_name in milestones\n",
      "Upserted batch for milestone_name in milestones\n",
      "Upserted batch for milestone_name in milestones\n",
      "Upserted batch for projectid in projects\n",
      "Upserted batch for projectid in projects\n",
      "Upserted batch for projectid in projects\n",
      "Upserted batch for projectid in projects\n",
      "Upserted batch for projectname in projects\n",
      "Upserted batch for projectname in projects\n",
      "Upserted batch for projectname in projects\n",
      "Upserted batch for projectname in projects\n",
      "Upserted batch for owner in projects\n",
      "Upserted batch for status in projects\n",
      "Upserted batch for delivery_team in projects\n",
      "Upserted batch for sprint_new in tasks\n",
      "Upserted batch for actual_time_taken in tasks\n",
      "Upserted batch for actual_time_taken in tasks\n",
      "Upserted batch for duration in tasks\n",
      "Upserted batch for time_spent_so_far in tasks\n",
      "Upserted batch for time_spent_so_far in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_id in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for task_name in tasks\n",
      "Upserted batch for owner in tasks\n",
      "Upserted batch for priority in tasks\n",
      "Upserted batch for status in tasks\n",
      "Upserted batch for is_overdue in tasks\n",
      "Upserted batch for project_name in tasks\n",
      "Upserted batch for milestone_name in tasks\n",
      "Upserted batch for project_id in tasks\n",
      "Upserted batch for milestone_id in tasks\n",
      "Upserted batch for task_delay_time in tasks\n",
      "Upserted batch for task_delay_time in tasks\n",
      "Upserted batch for task_completion_mode in tasks\n",
      "Upserted batch for duration_unit in tasks\n",
      "Upserted batch for clarity_level in tasks\n",
      "Upserted batch for sprint in tasks\n",
      "Upserted batch for billing_type in tasks\n",
      "Upserted batch for product_skill in tasks\n",
      "Upserted batch for sprint_ff_sf in tasks\n",
      "Upserted batch for open_closed in tasks\n",
      "Upserted batch for allocated_unallocated in tasks\n",
      "Upserted batch for days_completed_on in tasks\n",
      "Upserted batch for user_email in users\n",
      "Upserted batch for role in users\n",
      "Upserted batch for status in users\n",
      "Upserted batch for user_name in users\n",
      "Upserted batch for profile_name in users\n",
      "Embeddings upserted into Pinecone successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc_two\"\n",
    "PORT = 5432\n",
    "\n",
    "# Pinecone details\n",
    "pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "index_name = \"smart-desk\"\n",
    "BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# Function to connect to PostgreSQL database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch schema with column names and data types, only including string types\n",
    "def fetch_schema_with_data_types(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type, character_maximum_length\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'public'\n",
    "AND data_type = 'character varying'\n",
    "OR data_type IN ('text', 'varchar')\n",
    "AND table_name in('projects','milestones','tasks','users')\n",
    "ORDER BY table_name;\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        print(schema_df)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema with data types: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch unique values from each column along with table details\n",
    "def fetch_unique_values(conn, table_name, column_name):\n",
    "    try:\n",
    "        query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df[column_name].dropna().astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching unique values for {column_name} in {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch all unique values for each column and map them to table details\n",
    "def fetch_all_unique_values_with_table(conn, schema_df):\n",
    "    unique_values_dict = {}\n",
    "    for table_name in schema_df['table_name'].unique():\n",
    "        unique_values_dict[table_name] = {}\n",
    "        table_columns = schema_df[schema_df['table_name'] == table_name]\n",
    "        for column_name in table_columns['column_name']:\n",
    "            unique_values = fetch_unique_values(conn, table_name, column_name)\n",
    "            unique_values_dict[table_name][column_name] = unique_values\n",
    "    return unique_values_dict\n",
    "\n",
    "# Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "def load_huggingface_model():\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    return embedding_model\n",
    "\n",
    "# Generate embeddings for each unique value and store them\n",
    "# Generate embeddings for each unique value and store them\n",
    "def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "    embeddings_dict = {}\n",
    "    for table_name, columns in unique_values_dict.items():\n",
    "        embeddings_dict[table_name] = {}\n",
    "        for column_name, unique_values in columns.items():\n",
    "            if unique_values:  # Check if there are any unique values\n",
    "                try:\n",
    "                    embeddings = embedding_model.encode(unique_values)\n",
    "                    embeddings_dict[table_name][column_name] = {\n",
    "                        \"unique_values\": unique_values,\n",
    "                        \"embeddings\": embeddings\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating embeddings for {column_name} in {table_name}: {e}\")\n",
    "                    embeddings_dict[table_name][column_name] = {\n",
    "                        \"unique_values\": unique_values,\n",
    "                        \"embeddings\": []  # Store an empty list if encoding fails\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"No unique values found for {column_name} in {table_name}. Skipping embeddings.\")\n",
    "                embeddings_dict[table_name][column_name] = {\n",
    "                    \"unique_values\": [],\n",
    "                    \"embeddings\": []\n",
    "                }\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "# Initialize Pinecone\n",
    "def initialize_pinecone():\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "# Batch the embeddings for upserts\n",
    "def batch_embeddings(upsert_data, batch_size):\n",
    "    for i in range(0, len(upsert_data), batch_size):\n",
    "        yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "    for table_name, columns in embeddings_dict.items():\n",
    "        for column_name, data in columns.items():\n",
    "            upsert_data = []\n",
    "            for i, embedding in enumerate(data['embeddings']):\n",
    "                unique_value = data['unique_values'][i]\n",
    "                vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "                metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "                upsert_data.append({\n",
    "                    \"id\": vector_id,\n",
    "                    \"values\": embedding.tolist(),\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "\n",
    "            # Batch the upsert to avoid exceeding size limits\n",
    "            for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "                index.upsert(vectors=batch, namespace=table_name)\n",
    "                print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# Main function to execute the process\n",
    "def main():\n",
    "    # Step 1: Connect to the database\n",
    "    conn = connect_to_db()\n",
    "\n",
    "    # Step 2: Fetch the schema with metadata and data types, only for string columns\n",
    "    schema_df = fetch_schema_with_data_types(conn)\n",
    "    print(\"Schema with string data types fetched successfully.\")\n",
    "\n",
    "    # Step 3: Fetch all unique values along with table details\n",
    "    unique_values_dict = fetch_all_unique_values_with_table(conn, schema_df)\n",
    "    print(\"Unique values for string columns fetched successfully.\")\n",
    "\n",
    "    # Step 4: Load the Hugging Face model for embeddings\n",
    "    embedding_model = load_huggingface_model()\n",
    "    print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "    # Step 5: Generate embeddings for all unique values\n",
    "    embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "    print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "    # Step 6: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "    print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cebda072-afa1-4dd8-ad86-679c17d912f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15812\\2198155610.py:42: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [table_name, column_name, data_type]\n",
      "Index: []\n",
      "Schema with string data types fetched successfully.\n",
      "Unique values for string columns fetched successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model loaded successfully.\n",
      "Embeddings for string columns generated successfully.\n",
      "Embeddings upserted into Pinecone successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# Database connection details\n",
    "DATABASE_HOST = \"database-test-postgress-instance.cpk2uyae6iza.ap-south-1.rds.amazonaws.com\"\n",
    "DATABASE_USERNAME = \"postgres\"\n",
    "DATABASE_PASSWORD = \"valign#123\"\n",
    "DATABASE_DB = \"python_test_poc_two\"\n",
    "PORT = 5432\n",
    "\n",
    "# Pinecone details\n",
    "pinecone_api_key = \"9fbe58e4-9e72-4023-90eb-ba8d022916b5\"\n",
    "index_name = \"smart-desk\"\n",
    "BATCH_SIZE = 200  # Adjust the batch size to avoid exceeding the size limit\n",
    "\n",
    "# Function to connect to PostgreSQL database\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DATABASE_DB,\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch schema with column names and data types, only including string types\n",
    "def fetch_schema_with_data_types(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        AND data_type IN ('character varying', 'text', 'varchar') AND table_name = 'contacts'\n",
    "        \"\"\"\n",
    "        schema_df = pd.read_sql(query, conn)\n",
    "        print(schema_df)\n",
    "        return schema_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema with data types: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch unique values from each column along with table details\n",
    "def fetch_unique_values(conn, table_name, column_name):\n",
    "    try:\n",
    "        query = f\"SELECT DISTINCT {column_name} FROM {table_name}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df[column_name].dropna().astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching unique values for {column_name} in {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch all unique values for each column and map them to table details\n",
    "def fetch_all_unique_values_with_table(conn, schema_df):\n",
    "    unique_values_dict = {}\n",
    "    for table_name in schema_df['table_name'].unique():\n",
    "        unique_values_dict[table_name] = {}\n",
    "        table_columns = schema_df[schema_df['table_name'] == table_name]\n",
    "        for column_name in table_columns['column_name']:\n",
    "            unique_values = fetch_unique_values(conn, table_name, column_name)\n",
    "            unique_values_dict[table_name][column_name] = unique_values\n",
    "    return unique_values_dict\n",
    "\n",
    "# Initialize SentenceTransformer model for Hugging Face embeddings\n",
    "def load_huggingface_model():\n",
    "    model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    embedding_model = SentenceTransformer(model_name)\n",
    "    return embedding_model\n",
    "\n",
    "# Generate embeddings for each unique value and store them\n",
    "# Generate embeddings for each unique value and store them\n",
    "def generate_and_store_embeddings(embedding_model, unique_values_dict):\n",
    "    embeddings_dict = {}\n",
    "    for table_name, columns in unique_values_dict.items():\n",
    "        embeddings_dict[table_name] = {}\n",
    "        for column_name, unique_values in columns.items():\n",
    "            if unique_values:  # Check if there are any unique values\n",
    "                try:\n",
    "                    embeddings = embedding_model.encode(unique_values)\n",
    "                    embeddings_dict[table_name][column_name] = {\n",
    "                        \"unique_values\": unique_values,\n",
    "                        \"embeddings\": embeddings\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating embeddings for {column_name} in {table_name}: {e}\")\n",
    "                    embeddings_dict[table_name][column_name] = {\n",
    "                        \"unique_values\": unique_values,\n",
    "                        \"embeddings\": []  # Store an empty list if encoding fails\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"No unique values found for {column_name} in {table_name}. Skipping embeddings.\")\n",
    "                embeddings_dict[table_name][column_name] = {\n",
    "                    \"unique_values\": [],\n",
    "                    \"embeddings\": []\n",
    "                }\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "# Initialize Pinecone\n",
    "def initialize_pinecone():\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "# Batch the embeddings for upserts\n",
    "def batch_embeddings(upsert_data, batch_size):\n",
    "    for i in range(0, len(upsert_data), batch_size):\n",
    "        yield upsert_data[i:i + batch_size]\n",
    "\n",
    "# Upsert embeddings into Pinecone with metadata for each table (namespace)\n",
    "def upsert_embeddings_into_pinecone(index, embeddings_dict):\n",
    "    for table_name, columns in embeddings_dict.items():\n",
    "        for column_name, data in columns.items():\n",
    "            upsert_data = []\n",
    "            for i, embedding in enumerate(data['embeddings']):\n",
    "                unique_value = data['unique_values'][i]\n",
    "                vector_id = f\"{table_name}_{column_name}_{i}\"\n",
    "                metadata = {\"column_name\": column_name, \"unique_value\": unique_value}\n",
    "\n",
    "                upsert_data.append({\n",
    "                    \"id\": vector_id,\n",
    "                    \"values\": embedding.tolist(),\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "\n",
    "            # Batch the upsert to avoid exceeding size limits\n",
    "            for batch in batch_embeddings(upsert_data, BATCH_SIZE):\n",
    "                index.upsert(vectors=batch, namespace=table_name)\n",
    "                print(f\"Upserted batch for {column_name} in {table_name}\")\n",
    "\n",
    "# Main function to execute the process\n",
    "def main():\n",
    "    # Step 1: Connect to the database\n",
    "    conn = connect_to_db()\n",
    "\n",
    "    # Step 2: Fetch the schema with metadata and data types, only for string columns\n",
    "    schema_df = fetch_schema_with_data_types(conn)\n",
    "    print(\"Schema with string data types fetched successfully.\")\n",
    "\n",
    "    # Step 3: Fetch all unique values along with table details\n",
    "    unique_values_dict = fetch_all_unique_values_with_table(conn, schema_df)\n",
    "    print(\"Unique values for string columns fetched successfully.\")\n",
    "\n",
    "    # Step 4: Load the Hugging Face model for embeddings\n",
    "    embedding_model = load_huggingface_model()\n",
    "    print(\"Hugging Face model loaded successfully.\")\n",
    "\n",
    "    # Step 5: Generate embeddings for all unique values\n",
    "    embeddings_dict = generate_and_store_embeddings(embedding_model, unique_values_dict)\n",
    "    print(\"Embeddings for string columns generated successfully.\")\n",
    "\n",
    "    # Step 6: Initialize Pinecone and upsert embeddings under each table's namespace\n",
    "    pinecone_index = initialize_pinecone()\n",
    "    upsert_embeddings_into_pinecone(pinecone_index, embeddings_dict)\n",
    "    print(\"Embeddings upserted into Pinecone successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f759028-2859-4c15-81f6-7e37c0a39e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
